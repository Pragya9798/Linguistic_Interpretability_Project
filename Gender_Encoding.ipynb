{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bccc305-2bd6-4615-879e-a7bff1976cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample stimuli created:\\n\\nOriginal UD sentence: \"Der Mann geht nach Hause.\"\\nTemplate: \"Der [NOUN] geht nach Hause.\"\\n\\nGenerated stimuli:\\n1. \"Der Mann geht nach Hause.\" (original, grammatical)\\n2. \"Der Frau geht nach Hause.\" (substituted, ungrammatical - should be \"Die Frau\")  \\n3. \"Der Kind geht nach Hause.\" (substituted, ungrammatical - should be \"Das Kind\")\\n\\nThis tests whether mBERT\\'s representation of \"Der\" encodes that it should agree with masculine nouns.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import conllu\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def load_ud_german(filepath: str):\n",
    "    \"\"\"Load UD German dataset\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        sentences = conllu.parse(f.read())\n",
    "    return sentences\n",
    "\n",
    "def extract_article_noun_pairs(sentences):\n",
    "    \"\"\"Extract all article-noun pairs with their contexts\"\"\"\n",
    "    pairs = []\n",
    "    debug_info = {'total_sentences': 0, 'articles_found': 0, 'with_gender': 0, 'sample_tokens': []}\n",
    "    \n",
    "    for sent_idx, sent in enumerate(sentences):\n",
    "        debug_info['total_sentences'] += 1\n",
    "        tokens = [token for token in sent if isinstance(token['id'], int)]\n",
    "        \n",
    "        if sent_idx < 3:\n",
    "            debug_info['sample_tokens'].extend([\n",
    "                {\n",
    "                    'form': token['form'],\n",
    "                    'upos': token['upos'],\n",
    "                    'lemma': token.get('lemma'),\n",
    "                    'feats': str(token.get('feats')) if token.get('feats') else None\n",
    "                } for token in tokens[:10]  # First 10 tokens\n",
    "            ])\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            # Finding definite articles - be more flexible\n",
    "            if token['upos'] == 'DET':\n",
    "                debug_info['articles_found'] += 1\n",
    "                \n",
    "                # Debug print for first few articles\n",
    "                if debug_info['articles_found'] <= 5:\n",
    "                    print(f\"Article found: form='{token['form']}', lemma='{token.get('lemma')}', feats='{token.get('feats')}'\")\n",
    "                \n",
    "                # More flexible article detection\n",
    "                article_forms = ['der', 'die', 'das', 'dem', 'den', 'des']\n",
    "                if (token['form'].lower() in article_forms or \n",
    "                    (token.get('lemma') and token['lemma'].lower() in ['der', 'die', 'das'])):\n",
    "                    \n",
    "                    # Finding the noun this article modifies\n",
    "                    noun_idx = None\n",
    "                    \n",
    "                    # Check dependency relation\n",
    "                    if token.get('head') and isinstance(token['head'], int):\n",
    "                        potential_noun_idx = token['head'] - 1  # Convert to 0-indexed\n",
    "                        if (0 <= potential_noun_idx < len(tokens) and \n",
    "                            tokens[potential_noun_idx]['upos'] == 'NOUN'):\n",
    "                            noun_idx = potential_noun_idx\n",
    "                    \n",
    "                    # Checking immediate next token\n",
    "                    if noun_idx is None and i + 1 < len(tokens):\n",
    "                        if tokens[i + 1]['upos'] == 'NOUN':\n",
    "                            noun_idx = i + 1\n",
    "                    \n",
    "                    # Looking ahead a few tokens for noun\n",
    "                    if noun_idx is None:\n",
    "                        for j in range(i + 1, min(i + 4, len(tokens))):\n",
    "                            if tokens[j]['upos'] == 'NOUN':\n",
    "                                noun_idx = j\n",
    "                                break\n",
    "                    \n",
    "                    if noun_idx is not None and noun_idx < len(tokens):\n",
    "                        noun = tokens[noun_idx]\n",
    "                        \n",
    "                        # Extracts gender \n",
    "                        gender = None\n",
    "                        if noun.get('feats'):\n",
    "                            gender = extract_gender(noun['feats'])\n",
    "                        \n",
    "                        if not gender and token.get('feats'):\n",
    "                            gender = extract_gender(token['feats'])\n",
    "                        \n",
    "                        case = None\n",
    "                        if token.get('feats'):\n",
    "                            case = extract_case(token['feats'])\n",
    "                        \n",
    "                        if gender:  # Only add if we found gender\n",
    "                            debug_info['with_gender'] += 1\n",
    "                            pairs.append({\n",
    "                                'sentence': ' '.join([t['form'] for t in tokens]),\n",
    "                                'article': token['form'].lower(),\n",
    "                                'article_lemma': token.get('lemma', '').lower(),\n",
    "                                'article_pos': i,\n",
    "                                'noun': noun['form'],\n",
    "                                'noun_lemma': noun.get('lemma', ''),\n",
    "                                'noun_pos': noun_idx,\n",
    "                                'gender': gender,\n",
    "                                'case': case,\n",
    "                                'sentence_length': len(tokens),\n",
    "                                'distance': noun_idx - i\n",
    "                            })\n",
    "    \n",
    "    print(f\"Debug info: {debug_info['total_sentences']} sentences, {debug_info['articles_found']} articles found, {debug_info['with_gender']} with gender\")\n",
    "    \n",
    "    if debug_info['sample_tokens']:\n",
    "        print(\"\\nSample tokens from first sentences:\")\n",
    "        for token in debug_info['sample_tokens'][:15]:\n",
    "            print(f\"  {token}\")\n",
    "    \n",
    "    return pd.DataFrame(pairs)\n",
    "\n",
    "def extract_gender(feats):\n",
    "    \"\"\"Extract gender from morphological features\"\"\"\n",
    "    if not feats:\n",
    "        return None\n",
    "    \n",
    "    if isinstance(feats, dict):\n",
    "        gender = feats.get('Gender')\n",
    "    else:\n",
    "        feat_str = str(feats)\n",
    "        if 'Gender=Masc' in feat_str or \"'Gender': 'Masc'\" in feat_str:\n",
    "            gender = 'Masc'\n",
    "        elif 'Gender=Fem' in feat_str or \"'Gender': 'Fem'\" in feat_str:\n",
    "            gender = 'Fem'\n",
    "        elif 'Gender=Neut' in feat_str or \"'Gender': 'Neut'\" in feat_str:\n",
    "            gender = 'Neut'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # Convert to single letter format\n",
    "    if gender == 'Masc':\n",
    "        return 'm'\n",
    "    elif gender == 'Fem':\n",
    "        return 'f'\n",
    "    elif gender == 'Neut':\n",
    "        return 'n'\n",
    "    return None\n",
    "\n",
    "def extract_case(feats):\n",
    "    \"\"\"Extract case from morphological features\"\"\"\n",
    "    if not feats:\n",
    "        return None\n",
    "    \n",
    "    if isinstance(feats, dict):\n",
    "        case = feats.get('Case')\n",
    "    else:\n",
    "        # Parse string format\n",
    "        feat_str = str(feats)\n",
    "        if 'Case=Nom' in feat_str or \"'Case': 'Nom'\" in feat_str:\n",
    "            case = 'Nom'\n",
    "        elif 'Case=Acc' in feat_str or \"'Case': 'Acc'\" in feat_str:\n",
    "            case = 'Acc'\n",
    "        elif 'Case=Dat' in feat_str or \"'Case': 'Dat'\" in feat_str:\n",
    "            case = 'Dat'\n",
    "        elif 'Case=Gen' in feat_str or \"'Case': 'Gen'\" in feat_str:\n",
    "            case = 'Gen'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    if case:\n",
    "        return case.lower()\n",
    "    return None\n",
    "\n",
    "def extract_valid_article_gender_pairs(pairs_df):\n",
    "    \"\"\"Extract valid examples for each article-gender combination\"\"\"\n",
    "    \n",
    "    valid_combinations = {\n",
    "        ('der', 'nom', 'm'), ('der', 'dat', 'f'), ('der', 'gen', 'f'),\n",
    "        ('die', 'nom', 'f'), ('die', 'acc', 'f'), ('die', 'nom', 'n'), ('die', 'acc', 'n'),\n",
    "        ('das', 'nom', 'n'), ('das', 'acc', 'n'),\n",
    "        ('dem', 'dat', 'm'), ('dem', 'dat', 'n'),\n",
    "        ('den', 'acc', 'm'),\n",
    "        ('des', 'gen', 'm'), ('des', 'gen', 'n')\n",
    "    }\n",
    "    \n",
    "    stimuli = []\n",
    "    for article, case, gender in valid_combinations:\n",
    "        examples = pairs_df[\n",
    "            (pairs_df['article'] == article) & \n",
    "            (pairs_df['case'] == case) & \n",
    "            (pairs_df['gender'] == gender) &\n",
    "            (pairs_df['distance'] <= 2) &  # Article close to noun\n",
    "            (pairs_df['sentence_length'] <= 20)  # Reasonable length\n",
    "        ]\n",
    "        \n",
    "        # Sample up to N examples per combination\n",
    "        if len(examples) >= 10:\n",
    "            sampled = examples.sample(n=min(50, len(examples)), random_state=42)\n",
    "            stimuli.extend(sampled.to_dict('records'))\n",
    "    \n",
    "    return stimuli\n",
    "\n",
    "\n",
    "def create_ud_based_substitution_stimuli(pairs_df):\n",
    "    \"\"\"Create substitution stimuli using UD sentence templates\"\"\"\n",
    "    \n",
    "    valid_examples = extract_valid_article_gender_pairs(pairs_df)\n",
    "    \n",
    "    templates_by_article_case = defaultdict(list)\n",
    "    \n",
    "    for example in valid_examples:\n",
    "        # Creating template by replacing the noun with [NOUN]\n",
    "        sentence = example['sentence']\n",
    "        noun = example['noun']\n",
    "        template = sentence.replace(noun, '[NOUN]', 1)\n",
    "        \n",
    "        # Grouping templates by article and case\n",
    "        key = (example['article'], example['case'])\n",
    "        templates_by_article_case[key].append({\n",
    "            'template': template,\n",
    "            'article': example['article'],\n",
    "            'case': example['case'],\n",
    "            'article_pos': example['article_pos'],\n",
    "            'original_noun': noun,\n",
    "            'original_gender': example['gender']\n",
    "        })\n",
    "    \n",
    "    noun_sets = {\n",
    "        'm': {\n",
    "            'nom': ['Mann', 'Hund', 'Tisch', 'Baum', 'Stuhl'],\n",
    "            'acc': ['Mann', 'Hund', 'Tisch', 'Baum', 'Stuhl'], \n",
    "            'dat': ['Mann', 'Hund', 'Tisch', 'Baum', 'Stuhl'],\n",
    "            'gen': ['Mannes', 'Hundes', 'Tisches', 'Baumes', 'Stuhles']  # Genitive forms\n",
    "        },\n",
    "        'f': {\n",
    "            'nom': ['Frau', 'Katze', 'Lampe', 'Blume', 'Uhr'],\n",
    "            'acc': ['Frau', 'Katze', 'Lampe', 'Blume', 'Uhr'],\n",
    "            'dat': ['Frau', 'Katze', 'Lampe', 'Blume', 'Uhr'],\n",
    "            'gen': ['Frau', 'Katze', 'Lampe', 'Blume', 'Uhr']  # No change for feminine\n",
    "        },\n",
    "        'n': {\n",
    "            'nom': ['Kind', 'Haus', 'Buch', 'Auto', 'Fenster'],\n",
    "            'acc': ['Kind', 'Haus', 'Buch', 'Auto', 'Fenster'],\n",
    "            'dat': ['Kind', 'Haus', 'Buch', 'Auto', 'Fenster'], \n",
    "            'gen': ['Kindes', 'Hauses', 'Buches', 'Autos', 'Fensters']  # Genitive forms\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    stimuli = []\n",
    "    \n",
    "    for (article, case), template_list in templates_by_article_case.items():\n",
    "        sampled_templates = random.sample(template_list, min(10, len(template_list)))\n",
    "        \n",
    "        for template_info in sampled_templates:\n",
    "            template = template_info['template']\n",
    "            \n",
    "            # Testing each gender with this template\n",
    "            for gender in ['m', 'f', 'n']:\n",
    "                # Skip if this is the original correct combination (for comparison)\n",
    "                if gender == template_info['original_gender']:\n",
    "                    continue\n",
    "                \n",
    "                if case in noun_sets[gender]:\n",
    "                    available_nouns = noun_sets[gender][case]\n",
    "                    \n",
    "                    for noun in available_nouns[:3]:  # Use first 3 nouns per gender\n",
    "                        substituted_sentence = template.replace('[NOUN]', noun)\n",
    "                        \n",
    "                        stimuli.append({\n",
    "                            'sentence': substituted_sentence,\n",
    "                            'template': template,\n",
    "                            'article': article,\n",
    "                            'article_pos': template_info['article_pos'],\n",
    "                            'substituted_noun': noun,\n",
    "                            'substituted_gender': gender,\n",
    "                            'case': case,\n",
    "                            'original_noun': template_info['original_noun'],\n",
    "                            'original_gender': template_info['original_gender'],\n",
    "                            'is_grammatical': is_grammatically_correct(article, case, gender),\n",
    "                            'is_substitution': True\n",
    "                        })\n",
    "                \n",
    "                # Also add the original (grammatical) version for comparison\n",
    "                original_sentence = template.replace('[NOUN]', template_info['original_noun'])\n",
    "                stimuli.append({\n",
    "                    'sentence': original_sentence,\n",
    "                    'template': template,\n",
    "                    'article': article,\n",
    "                    'article_pos': template_info['article_pos'],\n",
    "                    'substituted_noun': template_info['original_noun'],\n",
    "                    'substituted_gender': template_info['original_gender'],\n",
    "                    'case': case,\n",
    "                    'original_noun': template_info['original_noun'],\n",
    "                    'original_gender': template_info['original_gender'],\n",
    "                    'is_grammatical': True,\n",
    "                    'is_substitution': False\n",
    "                })\n",
    "    \n",
    "    return stimuli\n",
    "\n",
    "def is_grammatically_correct(article, case, gender):\n",
    "    \"\"\"Check if article-case-gender combination is grammatically correct\"\"\"\n",
    "    correct_combinations = {\n",
    "        ('der', 'nom', 'm'), ('der', 'dat', 'f'), ('der', 'gen', 'f'),\n",
    "        ('die', 'nom', 'f'), ('die', 'acc', 'f'), \n",
    "        ('das', 'nom', 'n'), ('das', 'acc', 'n'),\n",
    "        ('dem', 'dat', 'm'), ('dem', 'dat', 'n'),\n",
    "        ('den', 'acc', 'm'),\n",
    "        ('des', 'gen', 'm'), ('des', 'gen', 'n')\n",
    "    }\n",
    "    return (article, case, gender) in correct_combinations\n",
    "\n",
    "def analyze_substitution_stimuli(stimuli):\n",
    "    \"\"\"Analyze the created substitution stimuli\"\"\"\n",
    "    df = pd.DataFrame(stimuli)\n",
    "    \n",
    "    print(\"Substitution Stimuli Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total stimuli: {len(df)}\")\n",
    "    print(f\"Grammatical: {len(df[df['is_grammatical']])}\")\n",
    "    print(f\"Ungrammatical: {len(df[~df['is_grammatical']])}\")\n",
    "    print(f\"Original sentences: {len(df[~df['is_substitution']])}\")\n",
    "    print(f\"Substituted sentences: {len(df[df['is_substitution']])}\")\n",
    "    \n",
    "    print(\"\\nBy article:\")\n",
    "    print(df['article'].value_counts())\n",
    "    \n",
    "    print(\"\\nBy case:\")\n",
    "    print(df['case'].value_counts())\n",
    "    \n",
    "    print(\"\\nBy substituted gender:\")\n",
    "    print(df['substituted_gender'].value_counts())\n",
    "    \n",
    "    print(\"\\nGrammaticality by article-gender:\")\n",
    "    for article in df['article'].unique():\n",
    "        article_data = df[df['article'] == article]\n",
    "        print(f\"\\n{article}:\")\n",
    "        for gender in ['m', 'f', 'n']:\n",
    "            gender_data = article_data[article_data['substituted_gender'] == gender]\n",
    "            if len(gender_data) > 0:\n",
    "                grammatical_pct = (gender_data['is_grammatical'].sum() / len(gender_data)) * 100\n",
    "                print(f\"  {gender}: {grammatical_pct:.1f}% grammatical ({len(gender_data)} examples)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "def create_comprehensive_stimuli_dataset(ud_sentences):\n",
    "    \"\"\"Create comprehensive dataset combining UD extraction and substitution\"\"\"\n",
    "    \n",
    "    pairs_df = extract_article_noun_pairs(ud_sentences)\n",
    "\n",
    "    print(\"Extracting valid UD examples...\")\n",
    "    valid_examples = extract_valid_article_gender_pairs(pairs_df)\n",
    "    print(f\"Found {len(valid_examples)} valid examples\")\n",
    "    \n",
    "    print(\"\\nCreating substitution stimuli...\")\n",
    "    substitution_stimuli = create_ud_based_substitution_stimuli(pairs_df)\n",
    "    print(f\"Created {len(substitution_stimuli)} substitution stimuli\")\n",
    "    \n",
    "    # Analyze\n",
    "    stimuli_df = analyze_substitution_stimuli(substitution_stimuli)\n",
    "    \n",
    "    return {\n",
    "        'valid_examples': valid_examples,\n",
    "        'substitution_stimuli': substitution_stimuli,\n",
    "        'analysis_df': stimuli_df\n",
    "    }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b8a3a2ec-ee20-400b-ad16-f1df6a4e7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article found: form='der', lemma='der', feats='{'Case': 'Gen', 'Definite': 'Def', 'Number': 'Plur', 'PronType': 'Art'}'\n",
      "Article found: form='Die', lemma='der', feats='{'Case': 'Nom', 'Definite': 'Def', 'Number': 'Plur', 'PronType': 'Art'}'\n",
      "Article found: form='dem', lemma='der', feats='{'Case': 'Dat', 'Definite': 'Def', 'Gender': 'Masc', 'Number': 'Sing', 'PronType': 'Art'}'\n",
      "Article found: form='der', lemma='der', feats='{'Case': 'Dat', 'Definite': 'Def', 'Gender': 'Fem', 'Number': 'Sing', 'PronType': 'Art'}'\n",
      "Article found: form='der', lemma='der', feats='{'Case': 'Dat', 'Definite': 'Def', 'Gender': 'Fem', 'Number': 'Sing', 'PronType': 'Art'}'\n",
      "Debug info: 13814 sentences, 37305 articles found, 21734 with gender\n",
      "\n",
      "Sample tokens from first sentences:\n",
      "  {'form': 'Sehr', 'upos': 'ADV', 'lemma': 'sehr', 'feats': None}\n",
      "  {'form': 'gute', 'upos': 'ADJ', 'lemma': 'gut', 'feats': \"{'Case': 'Nom', 'Degree': 'Pos', 'Gender': 'Fem', 'Number': 'Sing'}\"}\n",
      "  {'form': 'Beratung', 'upos': 'NOUN', 'lemma': 'Beratung', 'feats': \"{'Case': 'Nom', 'Gender': 'Fem', 'Number': 'Sing'}\"}\n",
      "  {'form': ',', 'upos': 'PUNCT', 'lemma': ',', 'feats': None}\n",
      "  {'form': 'schnelle', 'upos': 'ADJ', 'lemma': 'schnell', 'feats': \"{'Case': 'Nom', 'Degree': 'Pos', 'Gender': 'Fem', 'Number': 'Sing'}\"}\n",
      "  {'form': 'Behebung', 'upos': 'NOUN', 'lemma': 'Behebung', 'feats': \"{'Case': 'Nom', 'Gender': 'Fem', 'Number': 'Sing'}\"}\n",
      "  {'form': 'der', 'upos': 'DET', 'lemma': 'der', 'feats': \"{'Case': 'Gen', 'Definite': 'Def', 'Number': 'Plur', 'PronType': 'Art'}\"}\n",
      "  {'form': 'Probleme', 'upos': 'NOUN', 'lemma': 'Problem', 'feats': \"{'Case': 'Gen', 'Gender': 'Neut', 'Number': 'Plur'}\"}\n",
      "  {'form': ',', 'upos': 'PUNCT', 'lemma': ',', 'feats': None}\n",
      "  {'form': 'so', 'upos': 'ADV', 'lemma': 'so', 'feats': None}\n",
      "  {'form': 'Die', 'upos': 'DET', 'lemma': 'der', 'feats': \"{'Case': 'Nom', 'Definite': 'Def', 'Number': 'Plur', 'PronType': 'Art'}\"}\n",
      "  {'form': 'Kosten', 'upos': 'NOUN', 'lemma': 'Kosten', 'feats': \"{'Case': 'Nom', 'Number': 'Plur'}\"}\n",
      "  {'form': 'sind', 'upos': 'VERB', 'lemma': 'sein', 'feats': \"{'Mood': 'Ind', 'Number': 'Plur', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\"}\n",
      "  {'form': 'definitiv', 'upos': 'ADJ', 'lemma': 'definitiv', 'feats': \"{'Degree': 'Pos'}\"}\n",
      "  {'form': 'auch', 'upos': 'ADV', 'lemma': 'auch', 'feats': None}\n",
      "Extracting valid UD examples...\n",
      "Found 692 valid examples\n",
      "\n",
      "Creating substitution stimuli...\n",
      "Created 800 substitution stimuli\n",
      "Substitution Stimuli Analysis:\n",
      "==================================================\n",
      "Total stimuli: 800\n",
      "Grammatical: 290\n",
      "Ungrammatical: 510\n",
      "Original sentences: 200\n",
      "Substituted sentences: 600\n",
      "\n",
      "By article:\n",
      "article\n",
      "der    240\n",
      "das    160\n",
      "die    160\n",
      "dem     80\n",
      "des     80\n",
      "den     80\n",
      "Name: count, dtype: int64\n",
      "\n",
      "By case:\n",
      "case\n",
      "nom    240\n",
      "acc    240\n",
      "dat    160\n",
      "gen    160\n",
      "Name: count, dtype: int64\n",
      "\n",
      "By substituted gender:\n",
      "substituted_gender\n",
      "m    271\n",
      "f    270\n",
      "n    259\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Grammaticality by article-gender:\n",
      "\n",
      "dem:\n",
      "  m: 100.0% grammatical (27 examples)\n",
      "  f: 0.0% grammatical (30 examples)\n",
      "  n: 100.0% grammatical (23 examples)\n",
      "\n",
      "der:\n",
      "  m: 25.0% grammatical (80 examples)\n",
      "  f: 57.1% grammatical (70 examples)\n",
      "  n: 0.0% grammatical (90 examples)\n",
      "\n",
      "das:\n",
      "  m: 0.0% grammatical (60 examples)\n",
      "  f: 0.0% grammatical (60 examples)\n",
      "  n: 100.0% grammatical (40 examples)\n",
      "\n",
      "die:\n",
      "  m: 0.0% grammatical (60 examples)\n",
      "  f: 100.0% grammatical (50 examples)\n",
      "  n: 40.0% grammatical (50 examples)\n",
      "\n",
      "des:\n",
      "  m: 100.0% grammatical (24 examples)\n",
      "  f: 0.0% grammatical (30 examples)\n",
      "  n: 100.0% grammatical (26 examples)\n",
      "\n",
      "den:\n",
      "  m: 100.0% grammatical (20 examples)\n",
      "  f: 0.0% grammatical (30 examples)\n",
      "  n: 0.0% grammatical (30 examples)\n"
     ]
    }
   ],
   "source": [
    "sentences = load_ud_german(\"UD_German-GSD/de_gsd-ud-train.conllu\")\n",
    "    \n",
    "    # Creating comprehensive dataset for stimulus\n",
    "dataset = create_comprehensive_stimuli_dataset(sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "83386b31-82d6-4749-8e86-c2c56001c133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'Der Sage nach wurden die Nelken 1270 von dem Mann des französischen Königs Ludwig IX .',\n",
       "  'template': 'Der Sage nach wurden die Nelken 1270 von dem [NOUN] des französischen Königs Ludwig IX .',\n",
       "  'article': 'dem',\n",
       "  'article_pos': 8,\n",
       "  'substituted_noun': 'Mann',\n",
       "  'substituted_gender': 'm',\n",
       "  'case': 'dat',\n",
       "  'original_noun': 'Heer',\n",
       "  'original_gender': 'n',\n",
       "  'is_grammatical': True,\n",
       "  'is_substitution': True},\n",
       " {'sentence': 'Der Sage nach wurden die Nelken 1270 von dem Hund des französischen Königs Ludwig IX .',\n",
       "  'template': 'Der Sage nach wurden die Nelken 1270 von dem [NOUN] des französischen Königs Ludwig IX .',\n",
       "  'article': 'dem',\n",
       "  'article_pos': 8,\n",
       "  'substituted_noun': 'Hund',\n",
       "  'substituted_gender': 'm',\n",
       "  'case': 'dat',\n",
       "  'original_noun': 'Heer',\n",
       "  'original_gender': 'n',\n",
       "  'is_grammatical': True,\n",
       "  'is_substitution': True},\n",
       " {'sentence': 'Der Sage nach wurden die Nelken 1270 von dem Tisch des französischen Königs Ludwig IX .',\n",
       "  'template': 'Der Sage nach wurden die Nelken 1270 von dem [NOUN] des französischen Königs Ludwig IX .',\n",
       "  'article': 'dem',\n",
       "  'article_pos': 8,\n",
       "  'substituted_noun': 'Tisch',\n",
       "  'substituted_gender': 'm',\n",
       "  'case': 'dat',\n",
       "  'original_noun': 'Heer',\n",
       "  'original_gender': 'n',\n",
       "  'is_grammatical': True,\n",
       "  'is_substitution': True},\n",
       " {'sentence': 'Der Sage nach wurden die Nelken 1270 von dem Heer des französischen Königs Ludwig IX .',\n",
       "  'template': 'Der Sage nach wurden die Nelken 1270 von dem [NOUN] des französischen Königs Ludwig IX .',\n",
       "  'article': 'dem',\n",
       "  'article_pos': 8,\n",
       "  'substituted_noun': 'Heer',\n",
       "  'substituted_gender': 'n',\n",
       "  'case': 'dat',\n",
       "  'original_noun': 'Heer',\n",
       "  'original_gender': 'n',\n",
       "  'is_grammatical': True,\n",
       "  'is_substitution': False},\n",
       " {'sentence': 'Der Sage nach wurden die Nelken 1270 von dem Frau des französischen Königs Ludwig IX .',\n",
       "  'template': 'Der Sage nach wurden die Nelken 1270 von dem [NOUN] des französischen Königs Ludwig IX .',\n",
       "  'article': 'dem',\n",
       "  'article_pos': 8,\n",
       "  'substituted_noun': 'Frau',\n",
       "  'substituted_gender': 'f',\n",
       "  'case': 'dat',\n",
       "  'original_noun': 'Heer',\n",
       "  'original_gender': 'n',\n",
       "  'is_grammatical': False,\n",
       "  'is_substitution': True},\n",
       " {'sentence': 'Der Sage nach wurden die Nelken 1270 von dem Katze des französischen Königs Ludwig IX .',\n",
       "  'template': 'Der Sage nach wurden die Nelken 1270 von dem [NOUN] des französischen Königs Ludwig IX .',\n",
       "  'article': 'dem',\n",
       "  'article_pos': 8,\n",
       "  'substituted_noun': 'Katze',\n",
       "  'substituted_gender': 'f',\n",
       "  'case': 'dat',\n",
       "  'original_noun': 'Heer',\n",
       "  'original_gender': 'n',\n",
       "  'is_grammatical': False,\n",
       "  'is_substitution': True},\n",
       " {'sentence': 'Der Sage nach wurden die Nelken 1270 von dem Lampe des französischen Königs Ludwig IX .',\n",
       "  'template': 'Der Sage nach wurden die Nelken 1270 von dem [NOUN] des französischen Königs Ludwig IX .',\n",
       "  'article': 'dem',\n",
       "  'article_pos': 8,\n",
       "  'substituted_noun': 'Lampe',\n",
       "  'substituted_gender': 'f',\n",
       "  'case': 'dat',\n",
       "  'original_noun': 'Heer',\n",
       "  'original_gender': 'n',\n",
       "  'is_grammatical': False,\n",
       "  'is_substitution': True},\n",
       " {'sentence': 'Der Sage nach wurden die Nelken 1270 von dem Heer des französischen Königs Ludwig IX .',\n",
       "  'template': 'Der Sage nach wurden die Nelken 1270 von dem [NOUN] des französischen Königs Ludwig IX .',\n",
       "  'article': 'dem',\n",
       "  'article_pos': 8,\n",
       "  'substituted_noun': 'Heer',\n",
       "  'substituted_gender': 'n',\n",
       "  'case': 'dat',\n",
       "  'original_noun': 'Heer',\n",
       "  'original_gender': 'n',\n",
       "  'is_grammatical': True,\n",
       "  'is_substitution': False},\n",
       " {'sentence': 'Dem Bonner Mann kann dies eigentlich auch nicht recht sein .',\n",
       "  'template': 'Dem Bonner [NOUN] kann dies eigentlich auch nicht recht sein .',\n",
       "  'article': 'dem',\n",
       "  'article_pos': 0,\n",
       "  'substituted_noun': 'Mann',\n",
       "  'substituted_gender': 'm',\n",
       "  'case': 'dat',\n",
       "  'original_noun': 'Bauministerium',\n",
       "  'original_gender': 'n',\n",
       "  'is_grammatical': True,\n",
       "  'is_substitution': True},\n",
       " {'sentence': 'Dem Bonner Hund kann dies eigentlich auch nicht recht sein .',\n",
       "  'template': 'Dem Bonner [NOUN] kann dies eigentlich auch nicht recht sein .',\n",
       "  'article': 'dem',\n",
       "  'article_pos': 0,\n",
       "  'substituted_noun': 'Hund',\n",
       "  'substituted_gender': 'm',\n",
       "  'case': 'dat',\n",
       "  'original_noun': 'Bauministerium',\n",
       "  'original_gender': 'n',\n",
       "  'is_grammatical': True,\n",
       "  'is_substitution': True}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['substitution_stimuli'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "324f6b82-c9b3-4374-ba2b-eaaa454d72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def extract_article_representations(model, tokenizer, stimuli_list):\n",
    "    \"\"\"Extracts article representations from UD-based substitution stimuli\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for stimulus in stimuli_list:\n",
    "        # Extracts information from stimulus dictionary\n",
    "        sentence = stimulus['sentence']\n",
    "        article = stimulus['article']\n",
    "        article_pos = stimulus['article_pos']  # Already provided in your data\n",
    "        substituted_noun = stimulus['substituted_noun']\n",
    "        substituted_gender = stimulus['substituted_gender']\n",
    "        case = stimulus['case']\n",
    "        is_grammatical = stimulus['is_grammatical']\n",
    "        is_substitution = stimulus['is_substitution']\n",
    "        \n",
    "        # Tokenized sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        \n",
    "        # Finding actual article position in tokenized sequence\n",
    "        try:\n",
    "            art_token_pos = None\n",
    "            \n",
    "            search_range = range(max(0, article_pos - 2), min(len(tokens), article_pos + 3))\n",
    "            for pos in search_range:\n",
    "                if tokens[pos].lower().replace('##', '') == article.lower():\n",
    "                    art_token_pos = pos\n",
    "                    break\n",
    "            \n",
    "            # If not found, search the entire sequence\n",
    "            if art_token_pos is None:\n",
    "                for pos, token in enumerate(tokens):\n",
    "                    if token.lower().replace('##', '') == article.lower():\n",
    "                        art_token_pos = pos\n",
    "                        break\n",
    "            \n",
    "            if art_token_pos is None:\n",
    "                print(f\"Warning: Could not find article '{article}' in tokens: {tokens}\")\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence: {sentence}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        \n",
    "        for transformer_layer in range(model.config.num_hidden_layers):  # 0 to 11 for mBERT\n",
    "            # Getting the corresponding hidden state (add 1 to skip embeddings)\n",
    "            hidden_state_idx = transformer_layer + 1\n",
    "            layer_repr = outputs.hidden_states[hidden_state_idx][0].numpy()\n",
    "            \n",
    "            # Processing each attention head\n",
    "            for head in range(model.config.num_attention_heads):\n",
    "                head_dim = layer_repr.shape[-1] // model.config.num_attention_heads\n",
    "                start = head * head_dim\n",
    "                end = (head + 1) * head_dim\n",
    "                head_repr = layer_repr[art_token_pos, start:end]\n",
    "                \n",
    "                # Finds noun position (should be close to article)\n",
    "                noun_token_pos = None\n",
    "                noun_token = None\n",
    "                \n",
    "                # Looks for noun around article position\n",
    "                for offset in [1, 2, -1, 0]:  # Check positions relative to article\n",
    "                    pos = art_token_pos + offset\n",
    "                    if 0 <= pos < len(tokens):\n",
    "                        token = tokens[pos].replace('##', '')\n",
    "                        if token.lower() == substituted_noun.lower():\n",
    "                            noun_token_pos = pos\n",
    "                            noun_token = tokens[pos]\n",
    "                            break\n",
    "                \n",
    "                data.append({\n",
    "                    \"sentence\": sentence,\n",
    "                    \"article\": article,\n",
    "                    \"article_position\": art_token_pos,\n",
    "                    \"noun\": substituted_noun,\n",
    "                    \"noun_token\": noun_token,\n",
    "                    \"noun_position\": noun_token_pos,\n",
    "                    \"noun_gender\": substituted_gender,\n",
    "                    \"case\": case,\n",
    "                    \"is_grammatical\": is_grammatical,\n",
    "                    \"is_substitution\": is_substitution,\n",
    "                    \"original_gender\": stimulus.get('original_gender'),\n",
    "                    \"layer\": transformer_layer,  # ✅ FIXED: Now correctly 0-11\n",
    "                    \"head\": head,\n",
    "                    \"representation\": head_repr,\n",
    "                    \"template\": stimulus.get('template', '')\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def analyze_article_sensitivity(df):\n",
    "    \"\"\"Analyzes article sensitivity to noun gender from representation data\"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler  \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for (layer, head, article, case), group in df.groupby([\"layer\", \"head\", \"article\", \"case\"]):\n",
    "        if len(group) < 20:  \n",
    "            continue\n",
    "        \n",
    "        # Prepare data\n",
    "        X = np.stack(group[\"representation\"])\n",
    "        y_gender = group[\"noun_gender\"]\n",
    "        y_grammatical = group[\"is_grammatical\"]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Gender prediction accuracy\n",
    "        try:\n",
    "            gender_scores = cross_val_score(\n",
    "                LogisticRegression(max_iter=2000, solver='liblinear', random_state=42), \n",
    "                X_scaled, y_gender, cv=min(3, len(set(y_gender)))  # ✅ FIXED: CV can't exceed classes\n",
    "            )\n",
    "            gender_accuracy = np.mean(gender_scores)\n",
    "        except Exception as e:\n",
    "            print(f\"Gender classification error for layer {layer}, head {head}: {e}\")\n",
    "            gender_accuracy = 0.0\n",
    "        \n",
    "        # Grammaticality prediction accuracy\n",
    "        try:\n",
    "            grammar_scores = cross_val_score(\n",
    "                LogisticRegression(max_iter=2000, solver='liblinear', random_state=42), \n",
    "                X_scaled, y_grammatical, cv=min(3, len(set(y_grammatical)))  # ✅ FIXED: CV can't exceed classes\n",
    "            )\n",
    "            grammar_accuracy = np.mean(grammar_scores)\n",
    "        except Exception as e:\n",
    "            print(f\"Grammar classification error for layer {layer}, head {head}: {e}\")\n",
    "            grammar_accuracy = 0.0\n",
    "        \n",
    "        # Clustering analysis\n",
    "        try:\n",
    "            if len(set(y_gender)) >= 2:  # Need at least 2 genders\n",
    "                n_clusters = min(len(set(y_gender)), len(X_scaled))  # ✅ FIXED: Can't have more clusters than samples\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "                silhouette = silhouette_score(X_scaled, cluster_labels)\n",
    "            else:\n",
    "                silhouette = 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Clustering error for layer {layer}, head {head}: {e}\")\n",
    "            silhouette = 0.0\n",
    "        \n",
    "        # Variance analysis\n",
    "        try:\n",
    "            total_var = np.trace(np.cov(X_scaled.T))\n",
    "            gender_vars = []\n",
    "            for gender in set(y_gender):\n",
    "                gender_data = X_scaled[y_gender == gender]\n",
    "                if len(gender_data) > 1:\n",
    "                    gender_vars.append(np.trace(np.cov(gender_data.T)))\n",
    "            \n",
    "            if gender_vars:\n",
    "                within_var = np.mean(gender_vars)\n",
    "                variance_ratio = total_var / within_var if within_var > 0 else 0\n",
    "            else:\n",
    "                variance_ratio = 0\n",
    "        except Exception as e:\n",
    "            print(f\"Variance analysis error for layer {layer}, head {head}: {e}\")\n",
    "            variance_ratio = 0\n",
    "        \n",
    "        results.append({\n",
    "            \"layer\": layer,\n",
    "            \"head\": head,\n",
    "            \"article\": article,\n",
    "            \"case\": case,\n",
    "            \"n_examples\": len(group),\n",
    "            \"gender_accuracy\": gender_accuracy,\n",
    "            \"grammar_accuracy\": grammar_accuracy,\n",
    "            \"silhouette_score\": silhouette,\n",
    "            \"variance_ratio\": variance_ratio,\n",
    "            \"agreement_score\": (gender_accuracy + grammar_accuracy + silhouette) / 3\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values(\"agreement_score\", ascending=False)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def run_full_analysis():\n",
    "    \"\"\"Running complete analysis pipeline\"\"\"\n",
    "    model = AutoModel.from_pretrained('bert-base-multilingual-cased', output_attentions=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    print(\"Extracting representations...\")\n",
    "    repr_df = extract_article_representations(model, tokenizer, dataset['substitution_stimuli'])\n",
    "    print(f\"Extracted representations for {len(repr_df)} examples\")\n",
    "    \n",
    "    print(\"\\nAnalyzing gender sensitivity...\")\n",
    "    analysis_df = analyze_article_sensitivity(repr_df)\n",
    "    print(f\"Analyzed {len(analysis_df)} layer-head combinations\")\n",
    "    \n",
    "    print(\"\\nTop 10 gender-sensitive heads:\")\n",
    "    print(analysis_df.head(10)[['layer', 'head', 'article', 'case', 'gender_accuracy', 'agreement_score']])\n",
    "    \n",
    "    return repr_df, analysis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b52659bb-0e77-459c-a800-1234a7a99614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting representations...\n",
      "Extracted representations for 115200 examples\n",
      "\n",
      "Analyzing gender sensitivity...\n",
      "Analyzed 1440 layer-head combinations\n",
      "\n",
      "Top 10 gender-sensitive heads:\n",
      "     layer  head article case  gender_accuracy  agreement_score\n",
      "186      1     6     der  nom         0.825261         0.683244\n",
      "546      4     6     der  nom         0.923552         0.666211\n",
      "181      1     6     das  nom         0.838557         0.662879\n",
      "311      2     7     das  nom         0.887464         0.653940\n",
      "286      2     4     der  nom         0.788224         0.653716\n",
      "211      1     9     das  nom         0.825261         0.653599\n",
      "510      4     3     das  acc         0.912156         0.653019\n",
      "111      0    11     das  nom         0.750712         0.652995\n",
      "246      2     0     der  nom         0.824786         0.652186\n",
      "176      1     5     der  nom         0.787749         0.652101\n"
     ]
    }
   ],
   "source": [
    "df1, df2 = run_full_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7329e267-ba2d-43c6-a5f2-1eded14e6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def probe_fixed_article_case_context(repr_df, min_examples=20):\n",
    "    \"\"\"\n",
    "    H1 (Article Gender): Representations constant → Low accuracy (~33%), only morphological gender is attended to\n",
    "    H2 (Noun Gender): Representations vary by noun → High accuracy (>70%), context determines the gender of the article, specifically the noun\n",
    "    \n",
    "    For each (layer, head, article, case) context, probe whether representations\n",
    "    encode article's gender (H1) or noun's gender (H2).\n",
    "    \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"Probing fixed (article, case) contexts...\")\n",
    "    print(\"H1: Article's morphological gender encoding:: Low accuracy (~33%)\")  \n",
    "    print(\"H2: Noun affects gender encoding:: High accuracy (>70%)\")\n",
    "    \n",
    "    # Group by (layer, head, article, case) - the fixed context\n",
    "    for (layer, head, article, case), group in repr_df.groupby(['layer', 'head', 'article', 'case']):\n",
    "        \n",
    "        if len(group) < min_examples:\n",
    "            continue\n",
    "            \n",
    "        gender_counts = group['noun_gender'].value_counts()\n",
    "        if len(gender_counts) < 2:  # At least 2 different genders are needed from the stimuli dataset, with varying noun genders\n",
    "            continue\n",
    "            \n",
    "       \n",
    "        X = np.stack(group['representation'])  # Extracting article representations\n",
    "        y = group['noun_gender']               # Noun genders (target)\n",
    "        \n",
    "        # Training a simple Logistic Regression classifier to predict noun gender from article representation\n",
    "        try:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            \n",
    "            # Scaling the representations for convergence\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            classifier = LogisticRegression(\n",
    "                max_iter=2000,          \n",
    "                solver='liblinear',     \n",
    "                random_state=42         \n",
    "            )\n",
    "            \n",
    "            scores = cross_val_score(\n",
    "                classifier, \n",
    "                X_scaled,  # Using scaled data\n",
    "                y, \n",
    "                cv=min(5, len(set(y))),  \n",
    "                scoring='accuracy'\n",
    "            )\n",
    "            accuracy = np.mean(scores)\n",
    "            \n",
    "            # Accuracy determines hypothesis \n",
    "            if accuracy > 0.9:\n",
    "                hypothesis = \"H2 (Noun Gender)\"\n",
    "                evidence_strength = \"Very strong\"\n",
    "            if accuracy > 0.7:\n",
    "                hypothesis = \"H2 (Noun Gender)\"\n",
    "                evidence_strength = \"Strong\"\n",
    "            elif accuracy > 0.5:\n",
    "                hypothesis = \"H2 (Noun Gender)\" \n",
    "                evidence_strength = \"Moderate\"\n",
    "            elif accuracy < 0.4:\n",
    "                hypothesis = \"H1 (Article Gender)\"\n",
    "                evidence_strength = \"Strong\" if accuracy < 0.35 else \"Moderate\"\n",
    "            else:\n",
    "                hypothesis = \"Unclear\"\n",
    "                evidence_strength = \"Weak\"\n",
    "            \n",
    "            results.append({\n",
    "                'layer': layer,\n",
    "                'head': head,\n",
    "                'article': article,\n",
    "                'case': case,\n",
    "                'n_examples': len(group),\n",
    "                'n_genders': len(gender_counts),\n",
    "                'gender_distribution': dict(gender_counts),\n",
    "                'accuracy': accuracy,\n",
    "                'hypothesis': hypothesis,\n",
    "                'evidence_strength': evidence_strength,\n",
    "                'grammatical_count': len(group[group['is_grammatical'] == True]),\n",
    "                'ungrammatical_count': len(group[group['is_grammatical'] == False])\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {layer}, {head}, {article}, {case}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_hypothesis_results(results_df):\n",
    "    \"\"\"Analyze the results of the hypothesis testing\"\"\"\n",
    "    \n",
    "    print(\"\\nHypothesis Testing Results:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    hypothesis_counts = results_df['hypothesis'].value_counts()\n",
    "    print(f\"\\nOverall Distribution:\")\n",
    "    for hyp, count in hypothesis_counts.items():\n",
    "        pct = (count / len(results_df)) * 100\n",
    "        print(f\"  {hyp}: {count} contexts ({pct:.1f}%)\")\n",
    "\n",
    "    strong_evidence = results_df[results_df['evidence_strength'] == 'Strong']\n",
    "    print(f\"\\nStrong Evidence Cases ({len(strong_evidence)} contexts):\")\n",
    "    \n",
    "    strong_h1 = strong_evidence[strong_evidence['hypothesis'] == 'H1 (Article Gender)']\n",
    "    strong_h2 = strong_evidence[strong_evidence['hypothesis'] == 'H2 (Noun Gender)']\n",
    "    \n",
    "    print(f\"  Strong H1 (Article Gender) evidence: {len(strong_h1)} contexts\")\n",
    "    print(f\"  Strong H2 (Noun Gender) evidence: {len(strong_h2)} contexts\")\n",
    "    \n",
    "    # Grouping by layer, hypothesis combination for layer-wise analysis\n",
    "    print(f\"\\nHypothesis by Layer:\")\n",
    "    layer_analysis = results_df.groupby(['layer', 'hypothesis']).size().unstack(fill_value=0)\n",
    "    print(layer_analysis)\n",
    "    \n",
    "    print(f\"\\nTop 10 Noun Gender Encoding Contexts (of H2 context):\")\n",
    "    top_h2 = results_df[results_df['hypothesis'] == 'H2 (Noun Gender)'].sort_values('accuracy', ascending=False).head(10)\n",
    "    \n",
    "    print(\"Layer Head Article Case  Accuracy  N_Examples  Evidence\")\n",
    "    for _, row in top_h2.iterrows():\n",
    "        print(f\"{row['layer']:5d} {row['head']:4d} {row['article']:7s} {row['case']:4s}  \"\n",
    "              f\"{row['accuracy']:8.3f}  {row['n_examples']:10d}  {row['evidence_strength']}\")\n",
    "    \n",
    "    return {\n",
    "        'hypothesis_counts': hypothesis_counts,\n",
    "        'strong_h1': strong_h1,\n",
    "        'strong_h2': strong_h2,\n",
    "        'layer_analysis': layer_analysis\n",
    "    }\n",
    "\n",
    "def find_consistent_noun_gender_heads(results_df, min_contexts=3):\n",
    "    \"\"\"Grouping for heads that consistently encode noun gender across multiple contexts\"\"\"\n",
    "    \n",
    "    # Group by (layer, head) and counting for H2 contexts\n",
    "    head_summary = results_df.groupby(['layer', 'head']).agg({\n",
    "        'hypothesis': lambda x: (x == 'H2 (Noun Gender)').sum(),  # Count H2\n",
    "        'accuracy': 'mean',\n",
    "        'article': 'count'  # Total contexts\n",
    "    }).rename(columns={'hypothesis': 'h2_count', 'article': 'total_contexts'})\n",
    "    \n",
    "    # H2 percentage calculated\n",
    "    head_summary['h2_percentage'] = (head_summary['h2_count'] / head_summary['total_contexts']) * 100\n",
    "    \n",
    "    consistent_heads = head_summary[\n",
    "        (head_summary['total_contexts'] >= min_contexts) &\n",
    "        (head_summary['h2_percentage'] >= 90)  # At least 70% H2 contexts\n",
    "    ].sort_values('h2_percentage', ascending=False)\n",
    "    \n",
    "    print(f\"\\nConsistent Noun Gender Encoding Heads (≥{min_contexts} contexts, ≥70% H2):\")\n",
    "    print(\"Layer Head  Total_Contexts  H2_Count  H2_Percentage  Avg_Accuracy\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for (layer, head), row in consistent_heads.iterrows():\n",
    "        print(f\"{layer:5d} {head:4d}  {row['total_contexts']:13.0f}  \"\n",
    "              f\"{row['h2_count']:8.0f}  {row['h2_percentage']:12.1f}%  \"\n",
    "              f\"{row['accuracy']:12.3f}\")\n",
    "    \n",
    "    return consistent_heads\n",
    "\n",
    "def run_fixed_context_probe(repr_df):\n",
    "    \"\"\"Running the complete fixed context probing analysis\"\"\"\n",
    "    \n",
    "    print(\"Running Fixed Article-Case Context Gender Probe\")\n",
    "    \n",
    "    results = probe_fixed_article_case_context(repr_df)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(\"No valid contexts found!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nAnalyzed {len(results)} (layer, head, article, case) contexts\")\n",
    "    \n",
    "    # Analyzing results\n",
    "    analysis = analyze_hypothesis_results(results)\n",
    "    \n",
    "    # Finding consistent heads\n",
    "    consistent_heads = find_consistent_noun_gender_heads(results)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'analysis': analysis,\n",
    "        'consistent_heads': consistent_heads\n",
    "    }\n",
    "\n",
    "# Usage example:\n",
    "# probe_results = run_fixed_context_probe(repr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "67ccc6ae-266f-47e5-94bc-38482cd83245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fixed Article-Case Context Gender Probe\n",
      "============================================================\n",
      "Probing fixed (article, case) contexts...\n",
      "H1: Article gender encoding → Low accuracy (~33%)\n",
      "H2: Noun gender encoding → High accuracy (>70%)\n",
      "============================================================\n",
      "\n",
      "Analyzed 1440 (layer, head, article, case) contexts\n",
      "\n",
      "Hypothesis Testing Results:\n",
      "================================================================================\n",
      "\n",
      "Overall Distribution:\n",
      "  H2 (Noun Gender): 1304 contexts (90.6%)\n",
      "  Unclear: 123 contexts (8.5%)\n",
      "  H1 (Article Gender): 13 contexts (0.9%)\n",
      "\n",
      "Strong Evidence Cases (613 contexts):\n",
      "  Strong H1 (Article Gender) evidence: 0 contexts\n",
      "  Strong H2 (Noun Gender) evidence: 613 contexts\n",
      "\n",
      "Hypothesis by Layer:\n",
      "hypothesis  H1 (Article Gender)  H2 (Noun Gender)  Unclear\n",
      "layer                                                     \n",
      "0                             1                91       28\n",
      "1                             2               108       10\n",
      "2                             4               108        8\n",
      "3                             3               108        9\n",
      "4                             0               115        5\n",
      "5                             1               112        7\n",
      "6                             0               113        7\n",
      "7                             1               109       10\n",
      "8                             0               105       15\n",
      "9                             1               108       11\n",
      "10                            0               113        7\n",
      "11                            0               114        6\n",
      "\n",
      "Top 10 Noun Gender Encoding Contexts (H2):\n",
      "Layer Head Article Case  Accuracy  N_Examples  Evidence\n",
      "-------------------------------------------------------\n",
      "    8    5 das     acc      0.988          80  Strong\n",
      "    6    4 das     nom      0.962          80  Strong\n",
      "    6    5 das     acc      0.951          80  Strong\n",
      "    6    9 das     acc      0.951          80  Strong\n",
      "    6   11 das     acc      0.951          80  Strong\n",
      "    6   10 der     nom      0.950          80  Strong\n",
      "    6    4 der     nom      0.938          80  Strong\n",
      "    9    1 das     acc      0.938          80  Strong\n",
      "    6    1 das     nom      0.938          80  Strong\n",
      "    9    0 der     nom      0.938          80  Strong\n",
      "\n",
      "Consistent Noun Gender Encoding Heads (≥3 contexts, ≥70% H2):\n",
      "Layer Head  Total_Contexts  H2_Count  H2_Percentage  Avg_Accuracy\n",
      "-----------------------------------------------------------------\n",
      "   11   11             10        10         100.0%         0.702\n",
      "    5    1             10        10         100.0%         0.682\n",
      "    3    6             10        10         100.0%         0.677\n",
      "    4    0             10        10         100.0%         0.706\n",
      "    7    2             10        10         100.0%         0.701\n",
      "    4    2             10        10         100.0%         0.691\n",
      "    9    1             10        10         100.0%         0.706\n",
      "    4    5             10        10         100.0%         0.682\n",
      "    4    6             10        10         100.0%         0.725\n",
      "    4    7             10        10         100.0%         0.710\n",
      "    4    8             10        10         100.0%         0.705\n",
      "    4   11             10        10         100.0%         0.705\n",
      "    5    2             10        10         100.0%         0.698\n",
      "   10    0             10        10         100.0%         0.707\n",
      "    5    4             10        10         100.0%         0.691\n",
      "    5    5             10        10         100.0%         0.710\n",
      "    5    9             10        10         100.0%         0.714\n",
      "    6    0             10        10         100.0%         0.749\n",
      "    6    1             10        10         100.0%         0.731\n",
      "    8    0             10        10         100.0%         0.732\n",
      "    6    4             10        10         100.0%         0.744\n",
      "    6    5             10        10         100.0%         0.719\n",
      "    7   10             10        10         100.0%         0.719\n",
      "    6   10             10        10         100.0%         0.729\n",
      "    9   10             10        10         100.0%         0.700\n",
      "    9    3             10        10         100.0%         0.699\n",
      "    1   10             10        10         100.0%         0.641\n",
      "   10    3             10        10         100.0%         0.686\n",
      "   10    2             10        10         100.0%         0.705\n",
      "   11    4             10        10         100.0%         0.703\n",
      "   11    0             10        10         100.0%         0.705\n",
      "    1    0             10        10         100.0%         0.649\n",
      "   11    5             10        10         100.0%         0.696\n",
      "    2    8             10        10         100.0%         0.647\n",
      "   11    8             10        10         100.0%         0.717\n",
      "   10   11             10        10         100.0%         0.694\n",
      "   11    7             10        10         100.0%         0.734\n",
      "   10    9             10        10         100.0%         0.704\n",
      "   11    6             10         9          90.0%         0.680\n",
      "    8    3             10         9          90.0%         0.677\n",
      "    8    2             10         9          90.0%         0.694\n",
      "    8    1             10         9          90.0%         0.684\n",
      "    7   11             10         9          90.0%         0.666\n",
      "   11    3             10         9          90.0%         0.681\n",
      "   11    9             10         9          90.0%         0.700\n",
      "    7    9             10         9          90.0%         0.680\n",
      "    7    7             10         9          90.0%         0.704\n",
      "    7    6             10         9          90.0%         0.690\n",
      "   11   10             10         9          90.0%         0.672\n",
      "    7    5             10         9          90.0%         0.707\n",
      "    7    4             10         9          90.0%         0.716\n",
      "    8    5             10         9          90.0%         0.750\n",
      "   10    1             10         9          90.0%         0.711\n",
      "   11    2             10         9          90.0%         0.668\n",
      "   10    6             10         9          90.0%         0.688\n",
      "    9    9             10         9          90.0%         0.690\n",
      "   10    4             10         9          90.0%         0.712\n",
      "    9    7             10         9          90.0%         0.693\n",
      "    9    6             10         9          90.0%         0.663\n",
      "    9    5             10         9          90.0%         0.720\n",
      "    9    4             10         9          90.0%         0.707\n",
      "   10    5             10         9          90.0%         0.724\n",
      "    9    2             10         9          90.0%         0.680\n",
      "    8    7             10         9          90.0%         0.711\n",
      "   10    7             10         9          90.0%         0.727\n",
      "   10    8             10         9          90.0%         0.707\n",
      "   10   10             10         9          90.0%         0.686\n",
      "    9    0             10         9          90.0%         0.716\n",
      "    8   10             10         9          90.0%         0.681\n",
      "    8    9             10         9          90.0%         0.693\n",
      "   11    1             10         9          90.0%         0.696\n",
      "    7    3             10         9          90.0%         0.695\n",
      "    0    3             10         9          90.0%         0.576\n",
      "    7    1             10         9          90.0%         0.676\n",
      "    2    3             10         9          90.0%         0.642\n",
      "    3    2             10         9          90.0%         0.658\n",
      "    3    1             10         9          90.0%         0.677\n",
      "    3    0             10         9          90.0%         0.656\n",
      "    2   11             10         9          90.0%         0.641\n",
      "    2   10             10         9          90.0%         0.652\n",
      "    2    9             10         9          90.0%         0.630\n",
      "    2    7             10         9          90.0%         0.641\n",
      "    2    6             10         9          90.0%         0.663\n",
      "    2    5             10         9          90.0%         0.663\n",
      "    2    4             10         9          90.0%         0.655\n",
      "    2    2             10         9          90.0%         0.663\n",
      "    7    0             10         9          90.0%         0.701\n",
      "    2    0             10         9          90.0%         0.672\n",
      "    1    9             10         9          90.0%         0.638\n",
      "    1    8             10         9          90.0%         0.640\n",
      "    1    7             10         9          90.0%         0.612\n",
      "    1    6             10         9          90.0%         0.640\n",
      "    1    4             10         9          90.0%         0.642\n",
      "    1    3             10         9          90.0%         0.646\n",
      "    1    2             10         9          90.0%         0.666\n",
      "    1    1             10         9          90.0%         0.662\n",
      "    0   10             10         9          90.0%         0.585\n",
      "    3    3             10         9          90.0%         0.683\n",
      "    3    4             10         9          90.0%         0.658\n",
      "    3    5             10         9          90.0%         0.670\n",
      "    3    7             10         9          90.0%         0.646\n",
      "    6   11             10         9          90.0%         0.719\n",
      "    6    9             10         9          90.0%         0.731\n",
      "    6    8             10         9          90.0%         0.706\n",
      "    6    7             10         9          90.0%         0.707\n",
      "    6    6             10         9          90.0%         0.730\n",
      "    0    5             10         9          90.0%         0.575\n",
      "    6    2             10         9          90.0%         0.706\n",
      "    5   11             10         9          90.0%         0.698\n",
      "    5    8             10         9          90.0%         0.692\n",
      "    5    7             10         9          90.0%         0.667\n",
      "    5    6             10         9          90.0%         0.677\n",
      "    5    3             10         9          90.0%         0.700\n",
      "    5    0             10         9          90.0%         0.695\n",
      "    4   10             10         9          90.0%         0.665\n",
      "    4    9             10         9          90.0%         0.713\n",
      "    4    4             10         9          90.0%         0.684\n",
      "    4    3             10         9          90.0%         0.710\n",
      "    4    1             10         9          90.0%         0.661\n",
      "    3   10             10         9          90.0%         0.645\n",
      "    3    9             10         9          90.0%         0.681\n",
      "    3    8             10         9          90.0%         0.676\n",
      "    6    3             10         9          90.0%         0.692\n"
     ]
    }
   ],
   "source": [
    "probe_results = run_fixed_context_probe(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bbb17a69-1326-410b-9c00-88267a7a6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_data = sorted(probe_results[\"consistent_heads\"], key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "df_sorted = probe_results[\"consistent_heads\"].sort_values(by='accuracy', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a20b7677-e318-4b63-9731-5092da431777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>h2_count</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>total_contexts</th>\n",
       "      <th>h2_percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer</th>\n",
       "      <th>head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.749573</td>\n",
       "      <td>10</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.748528</td>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.743732</td>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.734046</td>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.732146</td>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">6</th>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.731102</td>\n",
       "      <td>10</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>0.729820</td>\n",
       "      <td>10</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.728965</td>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.727208</td>\n",
       "      <td>10</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            h2_count  accuracy  total_contexts  h2_percentage\n",
       "layer head                                                   \n",
       "8     5            9  0.749573              10           90.0\n",
       "6     0           10  0.748528              10          100.0\n",
       "      4           10  0.743732              10          100.0\n",
       "11    7           10  0.734046              10          100.0\n",
       "8     0           10  0.732146              10          100.0\n",
       "6     9            9  0.731102              10           90.0\n",
       "      1           10  0.730769              10          100.0\n",
       "      6            9  0.729820              10           90.0\n",
       "      10          10  0.728965              10          100.0\n",
       "10    7            9  0.727208              10           90.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdced16-6df6-4469-8d90-aa965846d286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4adbc-7c8c-4ab3-9d30-dc140f648155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
