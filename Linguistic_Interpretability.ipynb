{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUmxlsmuLiDz",
    "outputId": "59bc22c3-bec1-4fd0-a7d1-439df2734f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting conllu\n",
      "  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: conllu\n",
      "Successfully installed conllu-6.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FzGSePk_LI9z"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import conllu\n",
    "\n",
    "def extract_minimal_pairs(conllu_file):\n",
    "    \"\"\"\n",
    "    Creating the minimal pairs dataset where the articles are varied according to the same case, but with incorrect gender\n",
    "    \"\"\"\n",
    "\n",
    "    GERMAN_CONTRACTIONS = {\n",
    "    'am': ['an', 'dem'],\n",
    "    'ans': ['an', 'das'],\n",
    "    'aufs': ['auf', 'das'],\n",
    "    'beim': ['bei', 'dem'],\n",
    "    'durchs': ['durch', 'das'],\n",
    "    'fürs': ['für', 'das'],\n",
    "    'im': ['in', 'dem'],\n",
    "    'ins': ['in', 'das'],\n",
    "    'unterm': ['unter', 'dem'],\n",
    "    'unters': ['unter', 'das'],\n",
    "    'vom': ['von', 'dem'],\n",
    "    'vorm': ['vor', 'dem'],\n",
    "    'zum': ['zu', 'dem'],\n",
    "    'zur': ['zu', 'der'],\n",
    "    'überm': ['über', 'dem'],\n",
    "    'übers': ['über', 'das'],\n",
    "    'hinterm': ['hinter', 'dem'],\n",
    "    'hinters': ['hinter', 'das'],\n",
    "    }\n",
    "\n",
    "    # Extracting gender/case minimal pairs from CoNLL-U file, while skipping contracted articles.\n",
    "    with open(conllu_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    sentences = conllu.parse(data)\n",
    "    pairs = defaultdict(list)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = [token[\"form\"] for token in sentence] # no expansion of contractions done\n",
    "\n",
    "        # Skipping the entire sentence if it contains ANY contractions\n",
    "        has_contractions = any(token.lower() in GERMAN_CONTRACTIONS for token in tokens)\n",
    "        if has_contractions:\n",
    "            continue\n",
    "\n",
    "        # Only processing sentences with no contractions at all\n",
    "        for i, token in enumerate(sentence):\n",
    "            feats = token.get(\"feats\") or {}\n",
    "\n",
    "            if (token[\"upos\"] in [\"NOUN\", \"PRON\"] and\n",
    "                \"Case\" in feats and\n",
    "                \"Gender\" in feats and\n",
    "                feats.get(\"Number\") == \"Sing\"):  # working with only singular nouns\n",
    "\n",
    "                # Defining the case mappings\n",
    "                case_mappings = {\n",
    "                    # Masculine to Feminine\n",
    "                    (\"Masc\", \"Nom\", \"der\"): \"Die\",\n",
    "                    (\"Masc\", \"Acc\", \"den\"): \"die\",\n",
    "                    (\"Masc\", \"Dat\", \"dem\"): \"der\",\n",
    "\n",
    "                    # Feminine to Masculine\n",
    "                    (\"Fem\", \"Nom\", \"die\"): \"Der\",\n",
    "                    (\"Fem\", \"Acc\", \"die\"): \"den\",\n",
    "                    (\"Fem\", \"Dat\", \"der\"): \"dem\",\n",
    "\n",
    "                    # Masculine to Neuter\n",
    "                    (\"Masc\", \"Nom\", \"der\"): \"Das\",\n",
    "                    (\"Masc\", \"Acc\", \"den\"): \"das\",\n",
    "                    (\"Masc\", \"Dat\", \"dem\"): \"dem\",\n",
    "\n",
    "                    # Neuter to Masculine\n",
    "                    (\"Neut\", \"Nom\", \"das\"): \"Der\",\n",
    "                    (\"Neut\", \"Acc\", \"das\"): \"den\",\n",
    "                    (\"Neut\", \"Dat\", \"dem\"): \"dem\",\n",
    "\n",
    "                    # Feminine to Neuter\n",
    "                    (\"Fem\", \"Nom\", \"die\"): \"Das\",\n",
    "                    (\"Fem\", \"Acc\", \"die\"): \"das\",\n",
    "                    (\"Fem\", \"Dat\", \"der\"): \"dem\",\n",
    "\n",
    "                    # Neuter to Feminine\n",
    "                    (\"Neut\", \"Nom\", \"das\"): \"Die\",\n",
    "                    (\"Neut\", \"Acc\", \"das\"): \"die\",\n",
    "                    (\"Neut\", \"Dat\", \"dem\"): \"der\"\n",
    "                }\n",
    "\n",
    "                gender = feats[\"Gender\"]\n",
    "                case = feats[\"Case\"]\n",
    "\n",
    "                if gender in [\"Masc\", \"Fem\", \"Neut\"] and case in [\"Nom\", \"Acc\", \"Dat\"]:\n",
    "                    idx = i - 1  # Looking for the determiner before the noun\n",
    "                    if idx >= 0:\n",
    "                        det = sentence[idx]\n",
    "                        det_feats = det.get(\"feats\") or {}\n",
    "                        if det[\"upos\"] == \"DET\" and det_feats.get(\"Case\") == case:\n",
    "                            det_form = det[\"form\"]  # Keeping original capitalization for sentence starts\n",
    "                            det_form_lower = det_form.lower()\n",
    "\n",
    "                            # Only processing standard uncontracted articles\n",
    "                            if det_form_lower not in ['der', 'die', 'das', 'dem', 'den']:\n",
    "                                continue  # else, skipping\n",
    "\n",
    "                            # Checks for the mapping for this combination\n",
    "                            mapping_key = (gender, case, det_form_lower)\n",
    "                            if mapping_key in case_mappings:\n",
    "                                replacement = tokens.copy()\n",
    "                                new_article = case_mappings[mapping_key]\n",
    "\n",
    "                                if det_form[0].isupper():\n",
    "                                    new_article = new_article.capitalize()\n",
    "                                else:\n",
    "                                    new_article = new_article.lower()\n",
    "\n",
    "                                replacement[idx] = new_article\n",
    "\n",
    "                                original_sent = \" \".join(tokens)\n",
    "                                modified_sent = \" \".join(replacement)\n",
    "\n",
    "                                if original_sent != modified_sent:\n",
    "                                    # Stored as tuple: (original_sentence, modified_sentence, original_gender)\n",
    "                                    pairs[\"case\"].append((original_sent, modified_sent, gender))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3bsgjzjWMhv2"
   },
   "outputs": [],
   "source": [
    "pairs = extract_minimal_pairs(\"de_gsd-ud-train.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "msRPW1GvoYsu"
   },
   "outputs": [],
   "source": [
    "def patch_attention_head_activations(model, tokenizer, clean_sent, corrupted_sent,\n",
    "                                         target_layer, target_head, article_positions):\n",
    "    \"\"\"\n",
    "    Attention head patching for all heads of mBERT\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(article_positions, int):\n",
    "        article_positions = [article_positions]\n",
    "    elif not isinstance(article_positions, list):\n",
    "        raise ValueError(f\"article_positions must be int or list, got {type(article_positions)}\")\n",
    "\n",
    "    # Tokenizing sentences\n",
    "    clean_tokens = tokenizer(clean_sent, return_tensors=\"pt\")\n",
    "    corrupted_tokens = tokenizer(corrupted_sent, return_tensors=\"pt\")\n",
    "\n",
    "    def get_hidden_states(outputs):\n",
    "        if hasattr(outputs, 'last_hidden_state'):\n",
    "            return outputs.last_hidden_state\n",
    "        elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "            return outputs.hidden_states[-1]\n",
    "        else:\n",
    "            raise AttributeError(f\"Cannot find hidden states in {type(outputs)}\")\n",
    "\n",
    "    # Get baseline hidden states\n",
    "    with torch.no_grad():\n",
    "        orig_outputs = model(**clean_tokens, output_attentions=True)\n",
    "        baseline_hidden = get_hidden_states(orig_outputs)[0, article_positions[0]]  # Now works!\n",
    "\n",
    "\n",
    "    # Get intervention attention pattern\n",
    "    with torch.no_grad():\n",
    "        corrupted_outputs = model(**corrupted_tokens, output_attentions=True)\n",
    "\n",
    "        # Check if sequences are compatible\n",
    "        clean_seq_len = orig_outputs.attentions[target_layer].shape[-1]\n",
    "        corrupted_seq_len = corrupted_outputs.attentions[target_layer].shape[-1]\n",
    "\n",
    "        if abs(clean_seq_len - corrupted_seq_len) > 3:\n",
    "            print(f\"Sequence length mismatch too large: {clean_seq_len} vs {corrupted_seq_len}\")\n",
    "            return 0.0\n",
    "\n",
    "        intervention_attention = corrupted_outputs.attentions[target_layer][0, target_head]\n",
    "\n",
    "    # Safe intervention: modify model temporarily\n",
    "    hook_handle = None\n",
    "    try:\n",
    "        # Access the attention module for mBERT\n",
    "        if hasattr(model, 'encoder'):\n",
    "            attention_layer = model.encoder.layer[target_layer].attention\n",
    "        elif hasattr(model, 'bert'):\n",
    "            attention_layer = model.bert.encoder.layer[target_layer].attention\n",
    "        else:\n",
    "            raise AttributeError(\"Cannot find encoder layers\")\n",
    "\n",
    "        # Store original attention dropout for restoration\n",
    "        original_dropout = attention_layer.self.dropout\n",
    "\n",
    "        # Create custom dropout that replaces attention for our specific head\n",
    "        class InterventionDropout(torch.nn.Module):\n",
    "            def __init__(self, original_dropout, intervention_attn, target_head, min_seq_len):\n",
    "                super().__init__()\n",
    "                self.original_dropout = original_dropout\n",
    "                self.intervention_attn = intervention_attn\n",
    "                self.target_head = target_head\n",
    "                self.training = original_dropout.training\n",
    "                self.min_seq_len = min_seq_len\n",
    "\n",
    "            def forward(self, attention_probs):\n",
    "                # Replace the target head's attention safely\n",
    "                if attention_probs.dim() == 4:  # [batch, heads, seq, seq]\n",
    "                    modified_probs = attention_probs.clone()\n",
    "\n",
    "                    # Only patch if dimensions are compatible\n",
    "                    current_seq_len = attention_probs.shape[-1]\n",
    "                    patch_len = min(current_seq_len, self.min_seq_len)\n",
    "\n",
    "                    if patch_len > 0:\n",
    "                        modified_probs[0, self.target_head, :patch_len, :patch_len] = \\\n",
    "                            self.intervention_attn[:patch_len, :patch_len]\n",
    "\n",
    "                    return self.original_dropout(modified_probs)\n",
    "                else:\n",
    "                    return self.original_dropout(attention_probs)\n",
    "\n",
    "        # Calculate safe sequence length\n",
    "        min_seq_len = min(clean_seq_len, corrupted_seq_len)\n",
    "\n",
    "        # Replace dropout temporarily\n",
    "        attention_layer.self.dropout = InterventionDropout(\n",
    "            original_dropout, intervention_attention, target_head, min_seq_len\n",
    "        )\n",
    "\n",
    "        # Forward pass with intervention\n",
    "        intervened_outputs = model(**clean_tokens, output_attentions=True)\n",
    "        intervened_hidden = get_hidden_states(intervened_outputs)[0, article_positions[0]]\n",
    "\n",
    "        # Calculate effect\n",
    "        effect = torch.norm(intervened_hidden - baseline_hidden).item()\n",
    "\n",
    "        # Restore original dropout\n",
    "        attention_layer.self.dropout = original_dropout\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with head {target_layer}.{target_head}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    return effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NeRaH0A8L7bN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def get_attention_head_effect(model, tokenizer, orig_sent, pert_sent):\n",
    "    \"\"\"\n",
    "    Get the effect of each attention head using safe attention patching\n",
    "    Optimized for mBERT\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize sentences\n",
    "    orig_tokens = tokenizer(orig_sent, return_tensors=\"pt\")\n",
    "    pert_tokens = tokenizer(pert_sent, return_tensors=\"pt\")\n",
    "\n",
    "    # Checking if lengths are compatible\n",
    "    orig_len = orig_tokens.input_ids.shape[1]\n",
    "    pert_len = pert_tokens.input_ids.shape[1]\n",
    "\n",
    "    if abs(orig_len - pert_len) > 3:\n",
    "        print(f\"Skipping pair due to length mismatch: {orig_len} vs {pert_len}\")\n",
    "        return {}\n",
    "\n",
    "    # Getting article position\n",
    "    orig_ids = orig_tokens['input_ids'][0]\n",
    "    pert_ids = pert_tokens['input_ids'][0]\n",
    "\n",
    "    diff_positions = []\n",
    "    min_len = min(len(orig_ids), len(pert_ids))\n",
    "\n",
    "    for i in range(min_len):\n",
    "        if orig_ids[i] != pert_ids[i]:\n",
    "            diff_positions.append(i)\n",
    "\n",
    "    if not diff_positions:\n",
    "        print(\"No differences found between sentences\")\n",
    "        return {}\n",
    "\n",
    "    article_pos = diff_positions[0]\n",
    "\n",
    "    if article_pos >= min(orig_len, pert_len):\n",
    "        print(f\"Article position {article_pos} out of bounds\")\n",
    "        return {}\n",
    "\n",
    "    # Articles obtained\n",
    "    correct_article = tokenizer.decode(orig_ids[article_pos]).strip()\n",
    "    incorrect_article = tokenizer.decode(pert_ids[article_pos]).strip()\n",
    "\n",
    "\n",
    "    # Testing each attention head (mBERT has 12 layers, 12 heads each)\n",
    "    head_effects = {}\n",
    "\n",
    "    for layer in range(model.config.num_hidden_layers):  # Should be 12 for mBERT\n",
    "        for head in range(model.config.num_attention_heads):  # Should be 12 for mBERT\n",
    "            try:\n",
    "                effect = patch_attention_head_activations(\n",
    "                    model, tokenizer, orig_sent, pert_sent, layer, head,\n",
    "                    [article_pos]\n",
    "                )\n",
    "                head_effects[(layer, head)] = effect\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with head {layer}.{head}: {e}\")\n",
    "                head_effects[(layer, head)] = 0.0\n",
    "\n",
    "    return head_effects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LV-zmnYKMJin"
   },
   "outputs": [],
   "source": [
    "def find_token_position(tokenizer, sentence, target_word):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.replace(\"##\", \"\") == target_word:\n",
    "            return i\n",
    "    raise ValueError(f\"Target word '{target_word}' not found in sentence: {sentence}\")\n",
    "\n",
    "def get_article_probabilities(logits, correct_article, incorrect_article):\n",
    "    \"\"\"To extract probabilities for correct/incorrect articles\"\"\"\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    correct_id = tokenizer.convert_tokens_to_ids(correct_article)\n",
    "    incorrect_id = tokenizer.convert_tokens_to_ids(incorrect_article)\n",
    "\n",
    "    p_correct = probs[correct_id]\n",
    "    p_incorrect = probs[incorrect_id]\n",
    "\n",
    "    return {\n",
    "        'correct': p_correct.item(),\n",
    "        'incorrect': p_incorrect.item(),\n",
    "        'ratio': p_incorrect / p_correct\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3UBi85iwMMOe"
   },
   "outputs": [],
   "source": [
    "def store_attention_head_results(model, tokenizer, sentence_pairs, max_sentences):\n",
    "    \"\"\"\n",
    "    Storing results of attention analysis\n",
    "\n",
    "    Args:\n",
    "        model: BERT model with output_attentions=True\n",
    "        tokenizer: Corresponding tokenizer\n",
    "        sentence_pairs: List of (original_sentence, perturbed_sentence) tuples\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries with structured results\n",
    "    \"\"\"\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, (orig_sent, pert_sent, gender) in enumerate(sentence_pairs[:max_sentences]):\n",
    "        #print(f\"\\n Analyzing: {orig_sent[:50]}...\")\n",
    "\n",
    "        try:\n",
    "            # Getting attention head effects\n",
    "            head_effects = get_attention_head_effect(model, tokenizer, orig_sent, pert_sent)\n",
    "\n",
    "            # Finding article position and the perturbed difference\n",
    "            orig_tokens = tokenizer(orig_sent, return_tensors=\"pt\")\n",
    "            pert_tokens = tokenizer(pert_sent, return_tensors=\"pt\")\n",
    "\n",
    "            orig_ids = orig_tokens['input_ids'][0]\n",
    "            pert_ids = pert_tokens['input_ids'][0]\n",
    "\n",
    "            # Find the position where tokens differ\n",
    "            article_pos = None\n",
    "            correct_article = None\n",
    "            incorrect_article = None\n",
    "\n",
    "            for pos in range(min(len(orig_ids), len(pert_ids))):\n",
    "                if orig_ids[pos] != pert_ids[pos]:\n",
    "                    article_pos = pos\n",
    "                    correct_article = tokenizer.decode(orig_ids[pos]).strip()\n",
    "                    incorrect_article = tokenizer.decode(pert_ids[pos]).strip()\n",
    "                    break\n",
    "\n",
    "            if article_pos is None:\n",
    "                print(f\"  Warning: No difference found between sentences, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Store structured result\n",
    "            result = {\n",
    "                'sentence': orig_sent,\n",
    "                'article_pos': article_pos,\n",
    "                'correct_article': correct_article,\n",
    "                'incorrect_article': incorrect_article,\n",
    "                'head_effects': head_effects,  # Full dictionary of all heads\n",
    "            }\n",
    "\n",
    "            all_results.append(result)\n",
    "\n",
    "            sorted_heads = sorted(head_effects.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing sentence {i+1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n Successfully processed {len(all_results)} sentences\")\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305,
     "referenced_widgets": [
      "6729c88515574dad87d1378e2c0acc51",
      "662f62f068444844b138a3b64370be51",
      "634ca89b087949e78c2659258bbcaecb",
      "f78347e9c3d4496ebdf670577b2f5ff1",
      "c4e8402252cd48daa5a8c9407b184f38",
      "7c8e788dd3a447fba760f320966c3f3d",
      "3e597a8544064b63bb035e1dcd293b0a",
      "95e9d60420214e1eb36185b1d4f858cf",
      "561c8ce40e9a4a75856317ba57f90f59",
      "735869ec37204e41bd5bbd1c57333bf9",
      "3d862992bb384669a74f14e785227b02",
      "8b2372e4391c4cda9c22c52dbcf74bd5",
      "c5b70b61f09d415faf91aab2d3425af2",
      "945e044d736f41bf85b73e0d0642b87f",
      "641395dc25e44feb9827cbf4ea7e2489",
      "d242e5ef29044edfbdcd10d2b55e4b4e",
      "9526d45d31014131a2224e772d5a2952",
      "dc7c32d20f0c4289a130d0ff1fee58a4",
      "b4d96c9bd7b94357becc678da3fd7311",
      "78c05bfa29784c5dad221c638d03adec",
      "72e1ee1e260d4f79a517c6b5f33f20f5",
      "c8557c58b83d4eb2a26c8e9f410e1e0a",
      "e495526ff6f64c5682f822d324d452a8",
      "fe44266a6b994bb1bb23180f5a46348e",
      "ee0445ef2f1c455784f9b9f03518149c",
      "59a65b0ef781442f8f93e06ae9e2ed66",
      "91cfa857920641938badb3c4859ff6de",
      "0673588584f7487f9274e7aeee3bec7f",
      "4cda7b8a7aa741b2a7132427dd86503b",
      "0e551e6fc294432f830a4fb818d7ef48",
      "ff11389886e7416cb14c7ef508d14f4b",
      "02ce8086ce0c4aed9868b4076dd0ca92",
      "30e31501bb324843a806060d02764812",
      "df70851135d94f51b3a31b45514c0f1d",
      "d616f8577eaa4f0cb3200ebc05c50be9",
      "a41e4a295a6f4d11811f432cf5456356",
      "6358a96fa7cd489192ed96ab0e004916",
      "074298d534a040efb9f3fee7f5af616a",
      "b2b6b1c8b70240a684f269e923cd10cc",
      "a93224c63388419fbbe028d57f90efa8",
      "7bffcac8e4904b868b1963ea358d3fef",
      "44a8705097f84ea4b74769347301328b",
      "7006b599412f4050892d748cd6cb7f7b",
      "b75df36b062546259f505165c159ca2c",
      "8c36ab9bb6cc480e845ce11e08b96307",
      "be90b8bbf40c4685866304f64581bd09",
      "586f032e1ec84c64bb7ad7ccd8403647",
      "05f501d4cbd8407e9c56bb97206783fe",
      "f7b22adac63141589796039a6c79aca0",
      "cdabbe55e04c4ec1921d74d6f2d76aeb",
      "ef16083930bd4773a64d4fa59060e8fd",
      "ae2dd07167304b9aaf531e3506ae8841",
      "c21e45a4ebdb400cbbcb4d6ade3ec324",
      "f0702d2ea0594f44b32726ea6bf7b756",
      "76014b9da26e4b3c8236b4c77b3121a3"
     ]
    },
    "id": "s0cse3WYnvKD",
    "outputId": "5c8ed36f-2229-4f38-85d5-873e5c3fbf34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6729c88515574dad87d1378e2c0acc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2372e4391c4cda9c22c52dbcf74bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e495526ff6f64c5682f822d324d452a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df70851135d94f51b3a31b45514c0f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c36ab9bb6cc480e845ce11e08b96307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('bert-base-multilingual-cased', output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "g39gRqNsMOll",
    "outputId": "93ec1aa0-2f91-411e-a0f7-35c3af0c7f0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-288053732.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run analysis and store results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_attention_head_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"case\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-748248148.py\u001b[0m in \u001b[0;36mstore_attention_head_results\u001b[0;34m(model, tokenizer, sentence_pairs, max_sentences)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Getting attention head effects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mhead_effects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_attention_head_effect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpert_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Finding article position and the perturbed difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3317018295.py\u001b[0m in \u001b[0;36mget_attention_head_effect\u001b[0;34m(model, tokenizer, orig_sent, pert_sent)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Should be 12 for mBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 effect = patch_attention_head_activations(\n\u001b[0m\u001b[1;32m     56\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpert_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0marticle_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1849689180.py\u001b[0m in \u001b[0;36mpatch_attention_head_activations\u001b[0;34m(model, tokenizer, clean_sent, corrupted_sent, target_layer, target_head, article_positions)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Get baseline hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0morig_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclean_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mbaseline_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hidden_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now works!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    676\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    615\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run analysis and store results\n",
    "results = store_attention_head_results(model, tokenizer, pairs[\"case\"], max_sentences=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOXEvNzNMTH8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def analyze_head_consistency(all_results):\n",
    "    \"\"\"\n",
    "    Analyzes how consistently attention heads contribute across different sentences\n",
    "\n",
    "    all_results contains:\n",
    "    {\n",
    "        'sentence': str,\n",
    "        'article_pos': int,\n",
    "        'correct_article': str,\n",
    "        'incorrect_article': str,\n",
    "        'head_effects': dict\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Consistency Analysis\n",
    "    head_appearances = defaultdict(list)\n",
    "    head_sentence_count = defaultdict(int)\n",
    "\n",
    "    sentence_data = []\n",
    "\n",
    "    for result in all_results:\n",
    "        sentence = result['sentence']\n",
    "        article_pos = result['article_pos']\n",
    "        head_effects = result['head_effects']\n",
    "\n",
    "        # Get top 10 heads for this sentence\n",
    "        top_10_heads = sorted(head_effects.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        for (layer, head), effect in head_effects.items():\n",
    "            head_appearances[(layer, head)].append(effect)\n",
    "\n",
    "            # Count if this head is in top 10 for this sentence\n",
    "            if (layer, head) in [h[0] for h in top_10_heads]:\n",
    "                head_sentence_count[(layer, head)] += 1\n",
    "\n",
    "        # Storing sentence metadata\n",
    "        sentence_data.append({\n",
    "            'sentence': sentence[:50] + \"...\",  # Truncate for display\n",
    "            'article_pos': article_pos,\n",
    "            'correct_article': result['correct_article'],\n",
    "            'incorrect_article': result['incorrect_article'],\n",
    "            'top_head': top_10_heads[0][0],  # (layer, head) of most important\n",
    "            'top_effect': top_10_heads[0][1],  # effect of most important\n",
    "        })\n",
    "\n",
    "    # Finding most consistent heads\n",
    "    total_sentences = len(all_results)\n",
    "    consistency_scores = {}\n",
    "\n",
    "    for head, appearances in head_appearances.items():\n",
    "        if len(appearances) >= 3:  # Only consider heads that appear in multiple sentences\n",
    "            consistency_scores[head] = {\n",
    "                'mean_effect': np.mean(appearances),\n",
    "                'std_effect': np.std(appearances),\n",
    "                'appearance_rate': head_sentence_count[head] / total_sentences,\n",
    "                'total_appearances': head_sentence_count[head],\n",
    "                'coefficient_of_variation': np.std(appearances) / np.mean(appearances) if np.mean(appearances) > 0 else float('inf')\n",
    "            }\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ATTENTION HEAD CONSISTENCY ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\nAnalyzed {total_sentences} sentences with gender agreement\")\n",
    "\n",
    "    # Most consistent heads (appear frequently with stable effects)\n",
    "    print(\"\\n MOST CONSISTENT HEADS (appear in many sentences):\")\n",
    "    print(\"-\" * 60)\n",
    "    consistent_heads = sorted(\n",
    "        [(head, stats) for head, stats in consistency_scores.items()],\n",
    "        key=lambda x: x[1]['appearance_rate'],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    for i, ((layer, head), stats) in enumerate(consistent_heads[:15]):\n",
    "        print(f\"{i+1:2d}. Layer {layer:2d}, Head {head:2d}: \"\n",
    "              f\"appears {stats['total_appearances']:2d}/{total_sentences} sentences \"\n",
    "              f\"({stats['appearance_rate']:.1%}) | \"\n",
    "              f\"avg effect: {stats['mean_effect']:.3f} ± {stats['std_effect']:.3f}\")\n",
    "\n",
    "    # Most variable heads (context-dependent)\n",
    "    print(\"\\nMOST CONTEXT-DEPENDENT HEADS (high variability):\")\n",
    "    print(\"-\" * 60)\n",
    "    variable_heads = sorted(\n",
    "        [(head, stats) for head, stats in consistency_scores.items()\n",
    "         if stats['total_appearances'] >= 3],\n",
    "        key=lambda x: x[1]['coefficient_of_variation'],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    for i, ((layer, head), stats) in enumerate(variable_heads[:10]):\n",
    "        print(f\"{i+1:2d}. Layer {layer:2d}, Head {head:2d}: \"\n",
    "              f\"CV: {stats['coefficient_of_variation']:.2f} | \"\n",
    "              f\"range: {min(head_appearances[(layer, head)]):.3f} - \"\n",
    "              f\"{max(head_appearances[(layer, head)]):.3f}\")\n",
    "\n",
    "    # Sentence-specific analysis\n",
    "    print(\"\\n SENTENCE-SPECIFIC PATTERNS:\")\n",
    "    print(\"-\" * 60)\n",
    "    sentence_df = pd.DataFrame(sentence_data)\n",
    "\n",
    "    # Group by article position\n",
    "    pos_groups = sentence_df.groupby('article_pos')['top_head'].apply(list)\n",
    "    print(\"\\nTop heads by article position:\")\n",
    "    for pos in sorted(pos_groups.index):\n",
    "        heads = pos_groups[pos]\n",
    "        head_counts = Counter(heads)\n",
    "        print(f\"  Position {pos:2d}: {dict(head_counts.most_common(3))}\")\n",
    "\n",
    "    # Group by article type\n",
    "    article_groups = sentence_df.groupby(['correct_article', 'incorrect_article'])['top_head'].apply(list)\n",
    "    print(f\"\\nTop heads by article transition:\")\n",
    "    for (correct, incorrect), heads in article_groups.items():\n",
    "        head_counts = Counter(heads)\n",
    "        print(f\"  '{correct}' → '{incorrect}': {dict(head_counts.most_common(2))}\")\n",
    "\n",
    "    return {\n",
    "        'consistency_scores': consistency_scores,\n",
    "        'sentence_data': sentence_df,\n",
    "        'head_appearances': dict(head_appearances)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def compare_sentence_types(all_results):\n",
    "    \"\"\"Compare head usage across different types of sentences\"\"\"\n",
    "\n",
    "    print(\"\\nDETAILED SENTENCE TYPE ANALYSIS:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Group by article position ranges\n",
    "    early_pos = [r for r in all_results if r['article_pos'] <= 5]\n",
    "    mid_pos = [r for r in all_results if 6 <= r['article_pos'] <= 15]\n",
    "    late_pos = [r for r in all_results if r['article_pos'] > 15]\n",
    "\n",
    "    position_groups = {\n",
    "        'Early (pos 1-5)': early_pos,\n",
    "        'Middle (pos 6-15)': mid_pos,\n",
    "        'Late (pos 16+)': late_pos\n",
    "    }\n",
    "\n",
    "    for group_name, group_results in position_groups.items():\n",
    "        if not group_results:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{group_name}: {len(group_results)} sentences\")\n",
    "\n",
    "        # Get top heads for this group\n",
    "        all_head_effects = defaultdict(list)\n",
    "        for result in group_results:\n",
    "            for head, effect in result['head_effects'].items():\n",
    "                all_head_effects[head].append(effect)\n",
    "\n",
    "        # Calculate average effects and find top heads\n",
    "        avg_effects = {\n",
    "            head: np.mean(effects)\n",
    "            for head, effects in all_head_effects.items()\n",
    "            if len(effects) >= len(group_results) * 0.3  # Appear in at least 30% of sentences\n",
    "        }\n",
    "\n",
    "        top_heads = sorted(avg_effects.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "        for i, ((layer, head), avg_effect) in enumerate(top_heads):\n",
    "            appearance_count = len(all_head_effects[(layer, head)])\n",
    "            print(f\"  {i+1}. Layer {layer:2d}, Head {head:2d}: \"\n",
    "                  f\"avg {avg_effect:.3f} \"\n",
    "                  f\"(appears {appearance_count}/{len(group_results)} times)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZK2FsHxZZCx",
    "outputId": "ccc91b1f-876b-4af7-902e-1dad5f14f73c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ATTENTION HEAD CONSISTENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Analyzed 200 sentences with gender agreement\n",
      "\n",
      " MOST CONSISTENT HEADS (appear in many sentences):\n",
      "------------------------------------------------------------\n",
      " 1. Layer  4, Head  4: appears 156/200 sentences (78.0%) | avg effect: 0.649 ± 0.340\n",
      " 2. Layer  6, Head  2: appears 79/200 sentences (39.5%) | avg effect: 0.392 ± 0.195\n",
      " 3. Layer  7, Head  4: appears 77/200 sentences (38.5%) | avg effect: 0.371 ± 0.198\n",
      " 4. Layer  5, Head  6: appears 75/200 sentences (37.5%) | avg effect: 0.375 ± 0.183\n",
      " 5. Layer  8, Head 11: appears 75/200 sentences (37.5%) | avg effect: 0.391 ± 0.205\n",
      " 6. Layer  6, Head  0: appears 67/200 sentences (33.5%) | avg effect: 0.363 ± 0.221\n",
      " 7. Layer  6, Head  1: appears 64/200 sentences (32.0%) | avg effect: 0.385 ± 0.225\n",
      " 8. Layer  4, Head  2: appears 59/200 sentences (29.5%) | avg effect: 0.348 ± 0.182\n",
      " 9. Layer  9, Head  4: appears 59/200 sentences (29.5%) | avg effect: 0.389 ± 0.259\n",
      "10. Layer  5, Head  0: appears 57/200 sentences (28.5%) | avg effect: 0.338 ± 0.183\n",
      "11. Layer  9, Head  0: appears 55/200 sentences (27.5%) | avg effect: 0.335 ± 0.200\n",
      "12. Layer 10, Head 10: appears 50/200 sentences (25.0%) | avg effect: 0.315 ± 0.237\n",
      "13. Layer  8, Head  5: appears 48/200 sentences (24.0%) | avg effect: 0.311 ± 0.220\n",
      "14. Layer 11, Head  3: appears 47/200 sentences (23.5%) | avg effect: 0.315 ± 0.219\n",
      "15. Layer  4, Head 10: appears 43/200 sentences (21.5%) | avg effect: 0.325 ± 0.153\n",
      "\n",
      "MOST CONTEXT-DEPENDENT HEADS (high variability):\n",
      "------------------------------------------------------------\n",
      " 1. Layer  0, Head 10: CV: 1.60 | range: 0.013 - 1.886\n",
      " 2. Layer  3, Head  0: CV: 1.49 | range: 0.006 - 0.909\n",
      " 3. Layer  4, Head  5: CV: 1.41 | range: 0.012 - 0.965\n",
      " 4. Layer  0, Head  9: CV: 1.28 | range: 0.010 - 1.725\n",
      " 5. Layer  0, Head  1: CV: 1.23 | range: 0.022 - 1.916\n",
      " 6. Layer  0, Head  8: CV: 1.12 | range: 0.012 - 1.285\n",
      " 7. Layer 11, Head  1: CV: 1.00 | range: 0.004 - 0.923\n",
      " 8. Layer 11, Head  8: CV: 0.98 | range: 0.001 - 0.556\n",
      " 9. Layer 11, Head 11: CV: 0.96 | range: 0.001 - 0.657\n",
      "10. Layer  0, Head  2: CV: 0.91 | range: 0.020 - 0.939\n",
      "\n",
      " SENTENCE-SPECIFIC PATTERNS:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Top heads by article position:\n",
      "  Position  1: {(4, 4): 32, (7, 4): 5, (5, 0): 4}\n",
      "  Position  2: {(4, 4): 3, (8, 11): 1, (6, 2): 1}\n",
      "  Position  3: {(4, 4): 6, (6, 1): 1, (6, 0): 1}\n",
      "  Position  4: {(4, 4): 2, (9, 9): 2, (7, 4): 1}\n",
      "  Position  5: {(4, 4): 6, (9, 4): 2, (7, 4): 1}\n",
      "  Position  6: {(4, 4): 2, (11, 3): 1, (5, 8): 1}\n",
      "  Position  7: {(4, 4): 3, (9, 4): 1, (4, 5): 1}\n",
      "  Position  8: {(4, 4): 3, (11, 3): 2, (6, 2): 1}\n",
      "  Position  9: {(6, 2): 1, (11, 10): 1, (9, 7): 1}\n",
      "  Position 10: {(4, 4): 2, (10, 5): 1, (6, 1): 1}\n",
      "  Position 11: {(9, 4): 2, (4, 4): 2, (5, 8): 1}\n",
      "  Position 12: {(6, 2): 1, (9, 2): 1, (9, 4): 1}\n",
      "  Position 13: {(0, 9): 1, (4, 4): 1, (9, 0): 1}\n",
      "  Position 14: {(10, 0): 1}\n",
      "  Position 15: {(4, 4): 3, (8, 11): 1, (6, 2): 1}\n",
      "  Position 16: {(11, 3): 2, (4, 4): 1, (11, 2): 1}\n",
      "  Position 17: {(11, 3): 1}\n",
      "  Position 18: {(4, 4): 2, (7, 4): 1, (11, 3): 1}\n",
      "  Position 19: {(5, 0): 1, (10, 10): 1, (6, 1): 1}\n",
      "  Position 20: {(4, 4): 1}\n",
      "  Position 21: {(11, 2): 1, (9, 2): 1, (6, 1): 1}\n",
      "  Position 22: {(4, 4): 1}\n",
      "  Position 27: {(7, 6): 1, (6, 1): 1}\n",
      "  Position 28: {(4, 4): 2}\n",
      "  Position 32: {(4, 4): 1}\n",
      "  Position 33: {(4, 2): 1}\n",
      "  Position 38: {(11, 3): 1}\n",
      "\n",
      "Top heads by article transition:\n",
      "  'DI' → 'Das': {(2, 9): 1}\n",
      "  'Das' → 'Die': {(4, 4): 16, (8, 4): 2}\n",
      "  'Den' → 'Das': {(11, 10): 1}\n",
      "  'Der' → 'Das': {(5, 0): 3, (7, 4): 3}\n",
      "  'Die' → 'Das': {(4, 4): 15, (5, 6): 1}\n",
      "  'das' → 'die': {(4, 4): 11, (11, 3): 3}\n",
      "  'dem' → 'der': {(4, 4): 3, (11, 3): 1}\n",
      "  'den' → 'das': {(6, 2): 3, (4, 4): 2}\n",
      "  'der' → 'das': {(6, 1): 3, (4, 4): 2}\n",
      "  'der' → 'dem': {(4, 4): 11, (6, 1): 2}\n",
      "  'die' → 'das': {(4, 4): 13, (11, 3): 5}\n",
      "\n",
      "DETAILED SENTENCE TYPE ANALYSIS:\n",
      "============================================================\n",
      "\n",
      "Early (pos 1-5): 111 sentences\n",
      "  1. Layer  4, Head  4: avg 0.595 (appears 111/111 times)\n",
      "  2. Layer  6, Head  1: avg 0.367 (appears 111/111 times)\n",
      "  3. Layer  7, Head  4: avg 0.364 (appears 111/111 times)\n",
      "  4. Layer  8, Head 11: avg 0.360 (appears 111/111 times)\n",
      "  5. Layer  6, Head  2: avg 0.356 (appears 111/111 times)\n",
      "\n",
      "Middle (pos 6-15): 61 sentences\n",
      "  1. Layer  4, Head  4: avg 0.699 (appears 61/61 times)\n",
      "  2. Layer  9, Head  4: avg 0.480 (appears 61/61 times)\n",
      "  3. Layer  6, Head  2: avg 0.462 (appears 61/61 times)\n",
      "  4. Layer  8, Head 11: avg 0.426 (appears 61/61 times)\n",
      "  5. Layer  6, Head  1: avg 0.406 (appears 61/61 times)\n",
      "\n",
      "Late (pos 16+): 28 sentences\n",
      "  1. Layer  4, Head  4: avg 0.751 (appears 28/28 times)\n",
      "  2. Layer 10, Head 10: avg 0.445 (appears 28/28 times)\n",
      "  3. Layer  7, Head  4: avg 0.438 (appears 28/28 times)\n",
      "  4. Layer  8, Head 11: avg 0.436 (appears 28/28 times)\n",
      "  5. Layer 11, Head  3: avg 0.433 (appears 28/28 times)\n"
     ]
    }
   ],
   "source": [
    "analysis = analyze_head_consistency(results)\n",
    "# visualize_head_patterns(analysis)\n",
    "compare_sentence_types(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Awazv83qZdnX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_article_position_from_pairs(orig_sent, pert_sent, tokenizer):\n",
    "    \"\"\"\n",
    "    Getting article position by comparing original and perturbed sentences\n",
    "\n",
    "    Args:\n",
    "        orig_sent: Original sentence\n",
    "        pert_sent: Perturbed sentence (with different article)\n",
    "        tokenizer: The tokenizer\n",
    "\n",
    "    Returns:\n",
    "        int or None: Position where sentences differ (article position)\n",
    "    \"\"\"\n",
    "    orig_tokens = tokenizer(orig_sent, return_tensors=\"pt\")\n",
    "    pert_tokens = tokenizer(pert_sent, return_tensors=\"pt\")\n",
    "\n",
    "    # Finding the differing article position\n",
    "    orig_ids = orig_tokens['input_ids'][0]\n",
    "    pert_ids = pert_tokens['input_ids'][0]\n",
    "\n",
    "    diff_positions = []\n",
    "    min_len = min(len(orig_ids), len(pert_ids))\n",
    "\n",
    "    for i in range(min_len):\n",
    "        if orig_ids[i] != pert_ids[i]:\n",
    "            diff_positions.append(i)\n",
    "\n",
    "    if not diff_positions:\n",
    "        print(\"No differences found between sentences\")\n",
    "        return None\n",
    "\n",
    "    # The first difference is taken as article position\n",
    "    return diff_positions[0]\n",
    "\n",
    "def identify_gender_mapping_heads(model, tokenizer, minimal_pairs):\n",
    "    \"\"\"\n",
    "    Finding heads that consistently encode gender information using minimal pairs\n",
    "\n",
    "    Args:\n",
    "        minimal_pairs: List of tuples like [\n",
    "            (\"Leider war das Schnitzel aus der Fritöse .\",\n",
    "             \"Leider war das Schnitzel aus dem Fritöse .\",\n",
    "             \"Fem\"),\n",
    "            (\"Das Restaurant schließt schon Um 21 Uhr .\",\n",
    "             \"Die Restaurant schließt schon Um 21 Uhr .\",\n",
    "             \"Neut\"),\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    gender_mapping_data = []\n",
    "\n",
    "    for orig_sent, pert_sent, gender in minimal_pairs:\n",
    "\n",
    "        # Find article position by comparing the two sentences\n",
    "        article_pos = find_article_position_from_pairs(orig_sent, pert_sent, tokenizer)\n",
    "\n",
    "        if article_pos is None:\n",
    "            continue\n",
    "\n",
    "        # Process only the original sentence\n",
    "        tokens = tokenizer(orig_sent, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "        # Extract representations for each head\n",
    "        for layer in range(model.config.num_hidden_layers):\n",
    "            for head in range(model.config.num_attention_heads):\n",
    "\n",
    "                hidden_states = outputs.hidden_states[layer][0]\n",
    "                head_dim = hidden_states.size(-1) // model.config.num_attention_heads\n",
    "                start_idx = head * head_dim\n",
    "                end_idx = (head + 1) * head_dim\n",
    "\n",
    "                # Get head representation at article position\n",
    "                head_repr = hidden_states[article_pos, start_idx:end_idx]\n",
    "\n",
    "                # Also get attention pattern from article\n",
    "                attention = outputs.attentions[layer][0, head]\n",
    "                article_attention = attention[article_pos, :]\n",
    "\n",
    "                gender_mapping_data.append({\n",
    "                    'sentence': orig_sent,\n",
    "                    'gender': gender,\n",
    "                    'layer': layer,\n",
    "                    'head': head,\n",
    "                    'article_position': article_pos,\n",
    "                    'head_representation': head_repr.numpy(),\n",
    "                    'attention_pattern': article_attention.numpy(),\n",
    "                    # Store some key stats\n",
    "                    'repr_norm': torch.norm(head_repr).item(),\n",
    "                    'attention_entropy': -torch.sum(article_attention * torch.log(article_attention + 1e-10)).item()\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(gender_mapping_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpuG7v0GZz0Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def simple_gender_analysis(gender_mapping_df):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Group by each head (layer, head combination)\n",
    "    for (layer, head), head_data in gender_mapping_df.groupby(['layer', 'head']):\n",
    "\n",
    "        # Skip if we don't have enough data\n",
    "        if len(head_data) < 10:\n",
    "            continue\n",
    "\n",
    "        if head_data['gender'].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        print(f\"Analyzing Layer {layer}, Head {head} - {len(head_data)} samples\")\n",
    "\n",
    "        # X\n",
    "        representations = np.stack(head_data['head_representation'].values)\n",
    "\n",
    "        # Getting the gender labels and converting to labels, y\n",
    "        genders = head_data['gender'].values\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        gender_numbers = le.fit_transform(genders)\n",
    "\n",
    "        # Training a simple classifier\n",
    "        classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "        # Using cross-validation\n",
    "        cv_scores = cross_val_score(classifier, representations, gender_numbers,\n",
    "                                  cv=min(5, len(head_data)), scoring='accuracy')\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'layer': layer,\n",
    "            'head': head,\n",
    "            'accuracy': cv_scores.mean(),\n",
    "            'accuracy_std': cv_scores.std(),\n",
    "            'sample_count': len(head_data),\n",
    "            'gender_count': len(np.unique(genders))\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlUNlU4qdYaP",
    "outputId": "da36b04a-47f4-42b9-f077-564305bd1095"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('bert-base-multilingual-cased', output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "gender_mapping_df = identify_gender_mapping_heads(model, tokenizer, pairs[\"case\"][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1eZhB9wdagP",
    "outputId": "f9cef3e7-d01c-452b-8fda-2eca0f965a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Layer 0, Head 0 - 200 samples\n",
      "Analyzing Layer 0, Head 1 - 200 samples\n",
      "Analyzing Layer 0, Head 2 - 200 samples\n",
      "Analyzing Layer 0, Head 3 - 200 samples\n",
      "Analyzing Layer 0, Head 4 - 200 samples\n",
      "Analyzing Layer 0, Head 5 - 200 samples\n",
      "Analyzing Layer 0, Head 6 - 200 samples\n",
      "Analyzing Layer 0, Head 7 - 200 samples\n",
      "Analyzing Layer 0, Head 8 - 200 samples\n",
      "Analyzing Layer 0, Head 9 - 200 samples\n",
      "Analyzing Layer 0, Head 10 - 200 samples\n",
      "Analyzing Layer 0, Head 11 - 200 samples\n",
      "Analyzing Layer 1, Head 0 - 200 samples\n",
      "Analyzing Layer 1, Head 1 - 200 samples\n",
      "Analyzing Layer 1, Head 2 - 200 samples\n",
      "Analyzing Layer 1, Head 3 - 200 samples\n",
      "Analyzing Layer 1, Head 4 - 200 samples\n",
      "Analyzing Layer 1, Head 5 - 200 samples\n",
      "Analyzing Layer 1, Head 6 - 200 samples\n",
      "Analyzing Layer 1, Head 7 - 200 samples\n",
      "Analyzing Layer 1, Head 8 - 200 samples\n",
      "Analyzing Layer 1, Head 9 - 200 samples\n",
      "Analyzing Layer 1, Head 10 - 200 samples\n",
      "Analyzing Layer 1, Head 11 - 200 samples\n",
      "Analyzing Layer 2, Head 0 - 200 samples\n",
      "Analyzing Layer 2, Head 1 - 200 samples\n",
      "Analyzing Layer 2, Head 2 - 200 samples\n",
      "Analyzing Layer 2, Head 3 - 200 samples\n",
      "Analyzing Layer 2, Head 4 - 200 samples\n",
      "Analyzing Layer 2, Head 5 - 200 samples\n",
      "Analyzing Layer 2, Head 6 - 200 samples\n",
      "Analyzing Layer 2, Head 7 - 200 samples\n",
      "Analyzing Layer 2, Head 8 - 200 samples\n",
      "Analyzing Layer 2, Head 9 - 200 samples\n",
      "Analyzing Layer 2, Head 10 - 200 samples\n",
      "Analyzing Layer 2, Head 11 - 200 samples\n",
      "Analyzing Layer 3, Head 0 - 200 samples\n",
      "Analyzing Layer 3, Head 1 - 200 samples\n",
      "Analyzing Layer 3, Head 2 - 200 samples\n",
      "Analyzing Layer 3, Head 3 - 200 samples\n",
      "Analyzing Layer 3, Head 4 - 200 samples\n",
      "Analyzing Layer 3, Head 5 - 200 samples\n",
      "Analyzing Layer 3, Head 6 - 200 samples\n",
      "Analyzing Layer 3, Head 7 - 200 samples\n",
      "Analyzing Layer 3, Head 8 - 200 samples\n",
      "Analyzing Layer 3, Head 9 - 200 samples\n",
      "Analyzing Layer 3, Head 10 - 200 samples\n",
      "Analyzing Layer 3, Head 11 - 200 samples\n",
      "Analyzing Layer 4, Head 0 - 200 samples\n",
      "Analyzing Layer 4, Head 1 - 200 samples\n",
      "Analyzing Layer 4, Head 2 - 200 samples\n",
      "Analyzing Layer 4, Head 3 - 200 samples\n",
      "Analyzing Layer 4, Head 4 - 200 samples\n",
      "Analyzing Layer 4, Head 5 - 200 samples\n",
      "Analyzing Layer 4, Head 6 - 200 samples\n",
      "Analyzing Layer 4, Head 7 - 200 samples\n",
      "Analyzing Layer 4, Head 8 - 200 samples\n",
      "Analyzing Layer 4, Head 9 - 200 samples\n",
      "Analyzing Layer 4, Head 10 - 200 samples\n",
      "Analyzing Layer 4, Head 11 - 200 samples\n",
      "Analyzing Layer 5, Head 0 - 200 samples\n",
      "Analyzing Layer 5, Head 1 - 200 samples\n",
      "Analyzing Layer 5, Head 2 - 200 samples\n",
      "Analyzing Layer 5, Head 3 - 200 samples\n",
      "Analyzing Layer 5, Head 4 - 200 samples\n",
      "Analyzing Layer 5, Head 5 - 200 samples\n",
      "Analyzing Layer 5, Head 6 - 200 samples\n",
      "Analyzing Layer 5, Head 7 - 200 samples\n",
      "Analyzing Layer 5, Head 8 - 200 samples\n",
      "Analyzing Layer 5, Head 9 - 200 samples\n",
      "Analyzing Layer 5, Head 10 - 200 samples\n",
      "Analyzing Layer 5, Head 11 - 200 samples\n",
      "Analyzing Layer 6, Head 0 - 200 samples\n",
      "Analyzing Layer 6, Head 1 - 200 samples\n",
      "Analyzing Layer 6, Head 2 - 200 samples\n",
      "Analyzing Layer 6, Head 3 - 200 samples\n",
      "Analyzing Layer 6, Head 4 - 200 samples\n",
      "Analyzing Layer 6, Head 5 - 200 samples\n",
      "Analyzing Layer 6, Head 6 - 200 samples\n",
      "Analyzing Layer 6, Head 7 - 200 samples\n",
      "Analyzing Layer 6, Head 8 - 200 samples\n",
      "Analyzing Layer 6, Head 9 - 200 samples\n",
      "Analyzing Layer 6, Head 10 - 200 samples\n",
      "Analyzing Layer 6, Head 11 - 200 samples\n",
      "Analyzing Layer 7, Head 0 - 200 samples\n",
      "Analyzing Layer 7, Head 1 - 200 samples\n",
      "Analyzing Layer 7, Head 2 - 200 samples\n",
      "Analyzing Layer 7, Head 3 - 200 samples\n",
      "Analyzing Layer 7, Head 4 - 200 samples\n",
      "Analyzing Layer 7, Head 5 - 200 samples\n",
      "Analyzing Layer 7, Head 6 - 200 samples\n",
      "Analyzing Layer 7, Head 7 - 200 samples\n",
      "Analyzing Layer 7, Head 8 - 200 samples\n",
      "Analyzing Layer 7, Head 9 - 200 samples\n",
      "Analyzing Layer 7, Head 10 - 200 samples\n",
      "Analyzing Layer 7, Head 11 - 200 samples\n",
      "Analyzing Layer 8, Head 0 - 200 samples\n",
      "Analyzing Layer 8, Head 1 - 200 samples\n",
      "Analyzing Layer 8, Head 2 - 200 samples\n",
      "Analyzing Layer 8, Head 3 - 200 samples\n",
      "Analyzing Layer 8, Head 4 - 200 samples\n",
      "Analyzing Layer 8, Head 5 - 200 samples\n",
      "Analyzing Layer 8, Head 6 - 200 samples\n",
      "Analyzing Layer 8, Head 7 - 200 samples\n",
      "Analyzing Layer 8, Head 8 - 200 samples\n",
      "Analyzing Layer 8, Head 9 - 200 samples\n",
      "Analyzing Layer 8, Head 10 - 200 samples\n",
      "Analyzing Layer 8, Head 11 - 200 samples\n",
      "Analyzing Layer 9, Head 0 - 200 samples\n",
      "Analyzing Layer 9, Head 1 - 200 samples\n",
      "Analyzing Layer 9, Head 2 - 200 samples\n",
      "Analyzing Layer 9, Head 3 - 200 samples\n",
      "Analyzing Layer 9, Head 4 - 200 samples\n",
      "Analyzing Layer 9, Head 5 - 200 samples\n",
      "Analyzing Layer 9, Head 6 - 200 samples\n",
      "Analyzing Layer 9, Head 7 - 200 samples\n",
      "Analyzing Layer 9, Head 8 - 200 samples\n",
      "Analyzing Layer 9, Head 9 - 200 samples\n",
      "Analyzing Layer 9, Head 10 - 200 samples\n",
      "Analyzing Layer 9, Head 11 - 200 samples\n",
      "Analyzing Layer 10, Head 0 - 200 samples\n",
      "Analyzing Layer 10, Head 1 - 200 samples\n",
      "Analyzing Layer 10, Head 2 - 200 samples\n",
      "Analyzing Layer 10, Head 3 - 200 samples\n",
      "Analyzing Layer 10, Head 4 - 200 samples\n",
      "Analyzing Layer 10, Head 5 - 200 samples\n",
      "Analyzing Layer 10, Head 6 - 200 samples\n",
      "Analyzing Layer 10, Head 7 - 200 samples\n",
      "Analyzing Layer 10, Head 8 - 200 samples\n",
      "Analyzing Layer 10, Head 9 - 200 samples\n",
      "Analyzing Layer 10, Head 10 - 200 samples\n",
      "Analyzing Layer 10, Head 11 - 200 samples\n",
      "Analyzing Layer 11, Head 0 - 200 samples\n",
      "Analyzing Layer 11, Head 1 - 200 samples\n",
      "Analyzing Layer 11, Head 2 - 200 samples\n",
      "Analyzing Layer 11, Head 3 - 200 samples\n",
      "Analyzing Layer 11, Head 4 - 200 samples\n",
      "Analyzing Layer 11, Head 5 - 200 samples\n",
      "Analyzing Layer 11, Head 6 - 200 samples\n",
      "Analyzing Layer 11, Head 7 - 200 samples\n",
      "Analyzing Layer 11, Head 8 - 200 samples\n",
      "Analyzing Layer 11, Head 9 - 200 samples\n",
      "Analyzing Layer 11, Head 10 - 200 samples\n",
      "Analyzing Layer 11, Head 11 - 200 samples\n"
     ]
    }
   ],
   "source": [
    "results_df = simple_gender_analysis(gender_mapping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAu37U-ddeNm"
   },
   "outputs": [],
   "source": [
    "# Ultimately, not using these results as a more thorough design is done in Gender_Encoding.ipynb\n",
    "\n",
    "def analyze_results(results_df, top_n=10):\n",
    "\n",
    "\n",
    "    print(\"=== GENDER ENCODING ANALYSIS RESULTS ===\")\n",
    "\n",
    "    print(\"- Random guessing would be ~33% for 3 genders (Masc/Fem/Neut), so good heads should have >60% accuracy\")\n",
    "\n",
    "\n",
    "    print(f\"\\n=== TOP {top_n} GENDER-ENCODING HEADS ===\")\n",
    "\n",
    "    top_heads = results_df.head(top_n)\n",
    "\n",
    "    for idx, row in top_heads.iterrows():\n",
    "        accuracy_pct = row['accuracy'] * 100\n",
    "        print(f\"Layer {row['layer']}, Head {row['head']}: {accuracy_pct:.1f}% accuracy \"\n",
    "              f\"({row['sample_count']} samples, {row['gender_count']} genders)\")\n",
    "\n",
    "    print(f\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Total heads analyzed: {len(results_df)}\")\n",
    "    good_heads = results_df[results_df['accuracy'] > 0.6]\n",
    "    print(f\"Heads with >60% accuracy: {len(good_heads)}\")\n",
    "    best_accuracy = results_df['accuracy'].max() * 100\n",
    "    print(f\"Best accuracy achieved: {best_accuracy:.1f}%\")\n",
    "\n",
    "    return top_heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVlITBBsd52Z",
    "outputId": "80bf4565-0d60-4991-e8b3-e2730e5a18e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENDER ENCODING ANALYSIS RESULTS ===\n",
      "\n",
      "What this means:\n",
      "- Accuracy = How well we can predict gender from this head's representations\n",
      "- Higher accuracy = This head encodes gender information better\n",
      "- Random guessing would be ~33% for 3 genders (Masc/Fem/Neut)\n",
      "- Good heads should have >60% accuracy\n",
      "\n",
      "=== TOP 10 GENDER-ENCODING HEADS ===\n",
      "Layer 4.0, Head 3.0: 100.0% accuracy (200.0 samples, 3.0 genders)\n",
      "Layer 5.0, Head 5.0: 99.5% accuracy (200.0 samples, 3.0 genders)\n",
      "Layer 3.0, Head 10.0: 99.5% accuracy (200.0 samples, 3.0 genders)\n",
      "Layer 4.0, Head 10.0: 99.5% accuracy (200.0 samples, 3.0 genders)\n",
      "Layer 4.0, Head 2.0: 99.5% accuracy (200.0 samples, 3.0 genders)\n",
      "Layer 4.0, Head 5.0: 99.5% accuracy (200.0 samples, 3.0 genders)\n",
      "Layer 3.0, Head 4.0: 99.5% accuracy (200.0 samples, 3.0 genders)\n",
      "Layer 5.0, Head 6.0: 99.0% accuracy (200.0 samples, 3.0 genders)\n",
      "Layer 3.0, Head 6.0: 99.0% accuracy (200.0 samples, 3.0 genders)\n",
      "Layer 5.0, Head 3.0: 99.0% accuracy (200.0 samples, 3.0 genders)\n",
      "\n",
      "=== SUMMARY ===\n",
      "Total heads analyzed: 144\n",
      "Heads with >60% accuracy: 144\n",
      "Best accuracy achieved: 100.0%\n"
     ]
    }
   ],
   "source": [
    "top_gender_predictive_heads = analyze_results(results_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyXaI2sqsL6f"
   },
   "outputs": [],
   "source": [
    "top_gender_predictive_heads = list(zip(top_gender_predictive_heads['layer'], top_gender_predictive_heads['head']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STtJxpWFco-f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "\n",
    "def probe_gender_differences_perturbed(model, tokenizer, sentence_pairs):\n",
    "    \"\"\"\n",
    "    Focus on the DIFFERENCES between original and perturbed sentences\n",
    "    This is perfect for understanding what changes when gender is wrong!\n",
    "    \"\"\"\n",
    "    difference_data = []\n",
    "\n",
    "    for pair_idx, (orig_sentence, pert_sentence, gender) in enumerate(sentence_pairs):\n",
    "\n",
    "        article_pos = find_article_position_from_perturbation(\n",
    "            tokenizer, orig_sentence, pert_sentence\n",
    "        )\n",
    "\n",
    "        if article_pos is None:\n",
    "            continue\n",
    "\n",
    "        # Process both sentences\n",
    "        orig_tokens = tokenizer(orig_sentence, return_tensors=\"pt\")\n",
    "        pert_tokens = tokenizer(pert_sentence, return_tensors=\"pt\")\n",
    "\n",
    "        # Check if sequences have different lengths\n",
    "        orig_length = orig_tokens.input_ids.shape[1]\n",
    "        pert_length = pert_tokens.input_ids.shape[1]\n",
    "\n",
    "        # Skip if article position is out of bounds for either sentence\n",
    "        if article_pos >= orig_length or article_pos >= pert_length:\n",
    "            print(f\"Warning: Article position {article_pos} out of bounds for pair {pair_idx}\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            orig_outputs = model(**orig_tokens, output_attentions=True, output_hidden_states=True)\n",
    "            pert_outputs = model(**pert_tokens, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "        # Extract article info\n",
    "        orig_article = extract_article_from_sentence(orig_sentence, article_pos, tokenizer)\n",
    "        pert_article = extract_article_from_sentence(pert_sentence, article_pos, tokenizer)\n",
    "\n",
    "        #orig_gender = classify_german_article(orig_article)\n",
    "        #pert_gender = classify_german_article(pert_article)\n",
    "\n",
    "        # Compare representations across heads\n",
    "        for layer in range(model.config.num_hidden_layers):\n",
    "            for head in range(model.config.num_attention_heads):\n",
    "\n",
    "                # Getting representations\n",
    "                orig_hidden = orig_outputs.hidden_states[layer][0] # shape: [seq_len, hidden_size]\n",
    "                pert_hidden = pert_outputs.hidden_states[layer][0]\n",
    "\n",
    "                # making sure that the article position is not greater than the number of tokens\n",
    "                if article_pos >= orig_hidden.shape[0] or article_pos >= pert_hidden.shape[0]:\n",
    "                    continue\n",
    "\n",
    "                head_dim = orig_hidden.size(-1) // model.config.num_attention_heads   # 768//12 = 64\n",
    "                start_idx = head * head_dim\n",
    "                end_idx = (head + 1) * head_dim\n",
    "\n",
    "                orig_head_repr = orig_hidden[article_pos, start_idx:end_idx] # each head's representation at the article position\n",
    "                pert_head_repr = pert_hidden[article_pos, start_idx:end_idx]\n",
    "\n",
    "                # Calculate differences\n",
    "                repr_diff = pert_head_repr - orig_head_repr  # vector difference\n",
    "                repr_distance = torch.norm(repr_diff).item()  # magnitude difference\n",
    "                cosine_sim = torch.cosine_similarity(\n",
    "                    orig_head_repr.unsqueeze(0),\n",
    "                    pert_head_repr.unsqueeze(0)\n",
    "                ).item()\n",
    "\n",
    "                # Attention differences - handle different sequence lengths\n",
    "                orig_attention = orig_outputs.attentions[layer][0, head]\n",
    "                pert_attention = pert_outputs.attentions[layer][0, head]\n",
    "\n",
    "                # Only compare attention if both sequences are long enough\n",
    "                attention_diff = None\n",
    "                if (article_pos < orig_attention.shape[0] and # again, checking to see that attention position is within the attention bounds\n",
    "                    article_pos < pert_attention.shape[0]):   # attention matrix of perturbed and original tokens might be of different size\n",
    "\n",
    "                    # Compare attention patterns up to the minimum length\n",
    "                    min_seq_len = min(orig_attention.shape[0], pert_attention.shape[0])\n",
    "\n",
    "                    orig_attn_slice = orig_attention[article_pos, :min_seq_len]\n",
    "                    pert_attn_slice = pert_attention[article_pos, :min_seq_len]\n",
    "\n",
    "                    attention_diff = torch.norm(pert_attn_slice - orig_attn_slice).item()\n",
    "\n",
    "                difference_data.append({\n",
    "                    'pair_id': pair_idx,\n",
    "                    'layer': layer,\n",
    "                    'head': head,\n",
    "                    'orig_sentence': orig_sentence,\n",
    "                    'pert_sentence': pert_sentence,\n",
    "                    'orig_article': orig_article,\n",
    "                    'pert_article': pert_article,\n",
    "                    #'orig_gender': orig_gender['gender'],\n",
    "                    #'pert_gender': pert_gender['gender'],\n",
    "                    #'gender_changed': orig_gender['gender'] != pert_gender['gender'],\n",
    "                    'orig_length': orig_length,\n",
    "                    'pert_length': pert_length,\n",
    "                    'length_diff': abs(orig_length - pert_length),\n",
    "\n",
    "                    # Representation differences\n",
    "                    'representation_distance': repr_distance,\n",
    "                    'cosine_similarity': cosine_sim,\n",
    "                    'difference_vector': repr_diff.numpy(),\n",
    "\n",
    "                    # Attention differences (can be None if sequences too different)\n",
    "                    'attention_change': attention_diff,\n",
    "\n",
    "                    # Original representations for probing\n",
    "                    'orig_representation': orig_head_repr.numpy(),\n",
    "                    'pert_representation': pert_head_repr.numpy(),\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(difference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQjQossSf_wH"
   },
   "outputs": [],
   "source": [
    "def calculate_error_sensitivity_score_from_raw(difference_df):\n",
    "    \"\"\"\n",
    "    To calculate sensitivity scores from raw difference data (before aggregation)\n",
    "\n",
    "    Args:\n",
    "        difference_df: DataFrame with columns:\n",
    "            - layer, head, representation_distance, cosine_similarity, attention_change\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Aggregated head sensitivity with composite scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Aggregating the raw data by head (same as analyze_gender_sensitivity_by_head)\n",
    "    head_sensitivity = difference_df.groupby(['layer', 'head']).agg({\n",
    "        'representation_distance': ['mean', 'std', 'max'],\n",
    "        'cosine_similarity': ['mean', 'std', 'min'],\n",
    "        'attention_change': ['mean', 'std', 'max'],\n",
    "        #'gender_changed': 'sum'  # Count of gender changes\n",
    "    }).round(4)\n",
    "\n",
    "    # Flatten column names\n",
    "    head_sensitivity.columns = ['_'.join(col).strip() for col in head_sensitivity.columns]\n",
    "\n",
    "\n",
    "    repr_dist = head_sensitivity['representation_distance_mean'].fillna(0)\n",
    "    cosine_sim = head_sensitivity['cosine_similarity_mean'].fillna(head_sensitivity['cosine_similarity_mean'].median())\n",
    "    attention_change = head_sensitivity['attention_change_mean'].fillna(0)\n",
    "\n",
    "    # Normalize each metric to 0-1 scale\n",
    "    if repr_dist.max() > repr_dist.min():\n",
    "        repr_dist_norm = (repr_dist - repr_dist.min()) / (repr_dist.max() - repr_dist.min())\n",
    "    else:\n",
    "        repr_dist_norm = repr_dist * 0  # All zeros if no variation\n",
    "\n",
    "    # For cosine similarity, we want LOW values (more different = more sensitive)\n",
    "    if cosine_sim.max() > cosine_sim.min():\n",
    "        cosine_dissim_norm = 1 - ((cosine_sim - cosine_sim.min()) / (cosine_sim.max() - cosine_sim.min()))\n",
    "    else:\n",
    "        cosine_dissim_norm = cosine_sim * 0  # All zeros if no variation\n",
    "\n",
    "    if attention_change.max() > attention_change.min():\n",
    "        attention_change_norm = (attention_change - attention_change.min()) / (attention_change.max() - attention_change.min())\n",
    "    else:\n",
    "        attention_change_norm = attention_change * 0  # All zeros if no variation\n",
    "\n",
    "    # Weighted combination\n",
    "    sensitivity_score = (0.5 * repr_dist_norm +      # 50% weight to representation distance\n",
    "                        0.3 * cosine_dissim_norm +    # 30% weight to cosine dissimilarity\n",
    "                        0.2 * attention_change_norm)  # 20% weight to attention change\n",
    "\n",
    "    # Add sensitivity score to the dataframe\n",
    "    head_sensitivity['sensitivity_score'] = sensitivity_score\n",
    "\n",
    "    # Sort by sensitivity score (highest first)\n",
    "    head_sensitivity = head_sensitivity.sort_values('sensitivity_score', ascending=False)\n",
    "\n",
    "    return head_sensitivity\n",
    "\n",
    "def analyze_error_sensitivity_complete(difference_df):\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline from raw difference data to ranked sensitive heads\n",
    "    \"\"\"\n",
    "    print(\" COMPLETE ERROR SENSITIVITY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Calculate sensitivity scores\n",
    "    head_sensitivity = calculate_error_sensitivity_score_from_raw(difference_df)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\n Analyzed {len(head_sensitivity)} attention heads\")\n",
    "    print(f\" Data from {len(difference_df)} sentence pairs\")\n",
    "\n",
    "    print(\"\\n TOP 10 MOST ERROR-SENSITIVE HEADS:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    top_10 = head_sensitivity.head(10)\n",
    "    for (layer, head), row in top_10.iterrows():\n",
    "        print(f\"Layer {layer:2d}, Head {head:2d}: \"\n",
    "              f\"Score={row['sensitivity_score']:.3f} \"\n",
    "              f\"(RepDist={row['representation_distance_mean']:.3f}, \"\n",
    "              f\"CosSim={row['cosine_similarity_mean']:.3f}, \"\n",
    "              f\"AttnChg={row['attention_change_mean']:.3f})\")\n",
    "\n",
    "    print(f\"\\n SENSITIVITY DISTRIBUTION:\")\n",
    "    print(f\"Mean sensitivity score: {head_sensitivity['sensitivity_score'].mean():.3f}\")\n",
    "    print(f\"Max sensitivity score:  {head_sensitivity['sensitivity_score'].max():.3f}\")\n",
    "    print(f\"Heads with score > 0.7: {len(head_sensitivity[head_sensitivity['sensitivity_score'] > 0.7])}\")\n",
    "    print(f\"Heads with score > 0.5: {len(head_sensitivity[head_sensitivity['sensitivity_score'] > 0.5])}\")\n",
    "\n",
    "    return head_sensitivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sqw29POjgFFe",
    "outputId": "ce73e013-b7c9-44c5-97a6-15be5d4da026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " COMPLETE ERROR SENSITIVITY ANALYSIS\n",
      "============================================================\n",
      "\n",
      " Analyzed 144 attention heads\n",
      " Data from 28800 sentence pairs\n",
      "\n",
      " TOP 10 MOST ERROR-SENSITIVE HEADS:\n",
      "--------------------------------------------------\n",
      "Layer  0, Head  8: Score=0.837 (RepDist=6.196, CosSim=0.414, AttnChg=0.116)\n",
      "Layer  0, Head  0: Score=0.828 (RepDist=5.952, CosSim=0.430, AttnChg=0.228)\n",
      "Layer  0, Head  3: Score=0.811 (RepDist=6.104, CosSim=0.431, AttnChg=0.105)\n",
      "Layer  0, Head  4: Score=0.780 (RepDist=5.968, CosSim=0.479, AttnChg=0.149)\n",
      "Layer  0, Head 11: Score=0.778 (RepDist=5.928, CosSim=0.497, AttnChg=0.189)\n",
      "Layer  0, Head  1: Score=0.717 (RepDist=5.623, CosSim=0.521, AttnChg=0.181)\n",
      "Layer  0, Head  7: Score=0.708 (RepDist=5.628, CosSim=0.540, AttnChg=0.183)\n",
      "Layer  1, Head  0: Score=0.702 (RepDist=5.631, CosSim=0.497, AttnChg=0.092)\n",
      "Layer  0, Head  2: Score=0.679 (RepDist=5.551, CosSim=0.568, AttnChg=0.173)\n",
      "Layer  4, Head  4: Score=0.673 (RepDist=4.840, CosSim=0.651, AttnChg=0.621)\n",
      "\n",
      " SENSITIVITY DISTRIBUTION:\n",
      "Mean sensitivity score: 0.359\n",
      "Max sensitivity score:  0.837\n",
      "Heads with score > 0.7: 8\n",
      "Heads with score > 0.5: 31\n"
     ]
    }
   ],
   "source": [
    "diff_df = probe_gender_differences_perturbed(model,tokenizer, pairs[\"case\"][:200])\n",
    "\n",
    "head_sensitive_heads = analyze_error_sensitivity_complete(diff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mOC2iYSpgKXT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def find_article_position_simple(tokenizer, orig_sentence, pert_sentence):\n",
    "    \"\"\"Helper function to find article position\"\"\"\n",
    "    orig_tokens = tokenizer(orig_sentence, return_tensors=\"pt\")\n",
    "    pert_tokens = tokenizer(pert_sentence, return_tensors=\"pt\")\n",
    "\n",
    "    orig_ids = orig_tokens['input_ids'][0]\n",
    "    pert_ids = pert_tokens['input_ids'][0]\n",
    "\n",
    "    min_len = min(len(orig_ids), len(pert_ids))\n",
    "\n",
    "    for i in range(min_len):\n",
    "        if orig_ids[i] != pert_ids[i]:\n",
    "            return i\n",
    "\n",
    "    return 0  # Fallback to first position\n",
    "\n",
    "def patch_single_head(model, tokenizer, clean_sent, corrupt_sent, layer, head):\n",
    "    \"\"\"\n",
    "    Patch a single head using the existing intervention method\n",
    "    \"\"\"\n",
    "    article_pos = find_article_position_simple(tokenizer, clean_sent, corrupt_sent)\n",
    "\n",
    "    # Using existing intervention function\n",
    "    effect = patch_attention_head_activations(\n",
    "        model, tokenizer, clean_sent, corrupt_sent, layer, head,\n",
    "        article_pos\n",
    "    )\n",
    "\n",
    "    return effect\n",
    "\n",
    "def patch_multiple_heads(model, tokenizer, clean_sent, corrupt_sent, head_list):\n",
    "    \"\"\"\n",
    "    Patch multiple heads simultaneously\n",
    "    \"\"\"\n",
    "    # Tokenizing the sentences\n",
    "    clean_tokens = tokenizer(clean_sent, return_tensors=\"pt\")\n",
    "    corrupt_tokens = tokenizer(corrupt_sent, return_tensors=\"pt\")\n",
    "\n",
    "    # For the article position, we look at hidden state\n",
    "    article_pos = find_article_position_simple(tokenizer, clean_sent, corrupt_sent)\n",
    "\n",
    "    def get_hidden_states(outputs):\n",
    "        if hasattr(outputs, 'last_hidden_state'):\n",
    "            return outputs.last_hidden_state\n",
    "        elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "            return outputs.hidden_states[-1]\n",
    "        else:\n",
    "            raise AttributeError(f\"Cannot find hidden states in {type(outputs)}\")\n",
    "\n",
    "    # Getting baseline hidden states for clean input\n",
    "    with torch.no_grad():\n",
    "        orig_outputs = model(**clean_tokens, output_attentions=True)\n",
    "        baseline_hidden = get_hidden_states(orig_outputs)[0, article_pos]\n",
    "\n",
    "    # Getting intervened attention for all heads\n",
    "    with torch.no_grad():\n",
    "        pert_outputs = model(**corrupt_tokens, output_attentions=True)\n",
    "        intervention_attentions = {}\n",
    "        for layer, head in head_list:\n",
    "            intervention_attentions[(layer, head)] = pert_outputs.attentions[layer][0, head]\n",
    "\n",
    "    # Applying multi-head intervention\n",
    "    original_dropouts = {}\n",
    "    intervention_dropouts = {}\n",
    "\n",
    "    try:\n",
    "        if hasattr(model, 'encoder'):\n",
    "            encoder_layers = model.encoder.layer\n",
    "        elif hasattr(model, 'bert'):\n",
    "            encoder_layers = model.bert.encoder.layer\n",
    "        else:\n",
    "            raise AttributeError(\"Cannot find encoder layers\")\n",
    "\n",
    "        # Intervention dropouts created for each layer that has heads to patch\n",
    "        layers_to_patch = set(layer for layer, head in head_list)\n",
    "\n",
    "        for layer_idx in layers_to_patch:\n",
    "            attention_layer = encoder_layers[layer_idx].attention\n",
    "            original_dropouts[layer_idx] = attention_layer.self.dropout\n",
    "\n",
    "            # Get all heads to patch in this layer\n",
    "            heads_in_layer = [head for layer, head in head_list if layer == layer_idx]\n",
    "\n",
    "            # Create multi-head intervention dropout class\n",
    "            class MultiHeadInterventionDropout(torch.nn.Module):\n",
    "                def __init__(self, original_dropout, intervention_attns, target_heads):\n",
    "                    super().__init__()\n",
    "                    self.original_dropout = original_dropout\n",
    "                    self.intervention_attns = intervention_attns\n",
    "                    self.target_heads = target_heads\n",
    "                    self.training = original_dropout.training\n",
    "\n",
    "                def forward(self, attention_probs):\n",
    "                    if attention_probs.dim() == 4:  # [batch, heads, seq, seq]\n",
    "                        modified_probs = attention_probs.clone()\n",
    "                        for head_idx in self.target_heads:\n",
    "                            if head_idx < modified_probs.shape[1]:\n",
    "                                modified_probs[0, head_idx] = self.intervention_attns[head_idx]\n",
    "                        return self.original_dropout(modified_probs)\n",
    "                    else:\n",
    "                        return self.original_dropout(attention_probs)\n",
    "\n",
    "            # Preparing intervention attentions for this layer and replacing dropout\n",
    "            layer_interventions = {}\n",
    "            for head in heads_in_layer:\n",
    "                layer_interventions[head] = intervention_attentions[(layer_idx, head)]\n",
    "\n",
    "            intervention_dropouts[layer_idx] = MultiHeadInterventionDropout(\n",
    "                original_dropouts[layer_idx], layer_interventions, heads_in_layer\n",
    "            )\n",
    "            attention_layer.self.dropout = intervention_dropouts[layer_idx]\n",
    "\n",
    "        # Doing a Forward pass with multi-head intervention\n",
    "        intervened_outputs = model(**clean_tokens, output_attentions=True)\n",
    "        intervened_hidden = get_hidden_states(intervened_outputs)[0, article_pos]\n",
    "\n",
    "        # finally, calculating effect\n",
    "        effect = torch.norm(intervened_hidden - baseline_hidden).item()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with multi-head patching: {e}\")\n",
    "        effect = 0.0\n",
    "\n",
    "    finally:\n",
    "        # Restoring all original dropouts\n",
    "        for layer_idx, original_dropout in original_dropouts.items():\n",
    "            encoder_layers[layer_idx].attention.self.dropout = original_dropout\n",
    "\n",
    "    return effect\n",
    "\n",
    "def get_head_representation(model, tokenizer, sentence, layer, head):\n",
    "    \"\"\"\n",
    "    Helper function to get representation from a specific head\n",
    "    \"\"\"\n",
    "    # Tokenizing sentence\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    article_pos = 0\n",
    "    if len(tokens['input_ids'][0]) > 1:\n",
    "        for i, token_id in enumerate(tokens['input_ids'][0]):\n",
    "            token = tokenizer.decode([token_id]).strip().lower()\n",
    "            if token in ['der', 'die', 'das', 'den', 'dem', 'des']:\n",
    "                article_pos = i\n",
    "                break\n",
    "\n",
    "    # With the model outputs, we extract head representations\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "    # Extracting head representation od size [seq_len, hidden_size]\n",
    "    hidden_states = outputs.hidden_states[layer][0]\n",
    "\n",
    "    # Getting the head dimensions\n",
    "    head_dim = hidden_states.size(-1) // model.config.num_attention_heads\n",
    "    start_idx = head * head_dim\n",
    "    end_idx = (head + 1) * head_dim\n",
    "\n",
    "    # Finding head representation at article position\n",
    "    head_repr = hidden_states[article_pos, start_idx:end_idx]\n",
    "\n",
    "    return head_repr\n",
    "\n",
    "def get_head_representation_with_patch(model, tokenizer, clean_sent, corrupt_sent, source_head, target_head):\n",
    "    \"\"\"\n",
    "    Helper function to get target head representation when source head is patched\n",
    "    \"\"\"\n",
    "    clean_tokens = tokenizer(clean_sent, return_tensors=\"pt\")\n",
    "    corrupt_tokens = tokenizer(corrupt_sent, return_tensors=\"pt\")\n",
    "    article_pos = find_article_position_simple(tokenizer, clean_sent, corrupt_sent)\n",
    "\n",
    "    source_layer, source_head_idx = source_head\n",
    "    target_layer, target_head_idx = target_head\n",
    "\n",
    "    # Patching commences\n",
    "    with torch.no_grad():\n",
    "        pert_outputs = model(**corrupt_tokens, output_attentions=True)\n",
    "        intervention_attention = pert_outputs.attentions[source_layer][0, source_head_idx]\n",
    "\n",
    "    original_dropout = None\n",
    "\n",
    "    try:\n",
    "        # Access encoder layers\n",
    "        if hasattr(model, 'encoder'):\n",
    "            encoder_layers = model.encoder.layer\n",
    "        elif hasattr(model, 'bert'):\n",
    "            encoder_layers = model.bert.encoder.layer\n",
    "        else:\n",
    "            raise AttributeError(\"Cannot find encoder layers\")\n",
    "\n",
    "        # Patch source head\n",
    "        source_attention_layer = encoder_layers[source_layer].attention\n",
    "        original_dropout = source_attention_layer.self.dropout\n",
    "\n",
    "        class SourceInterventionDropout(torch.nn.Module):\n",
    "            def __init__(self, original_dropout, intervention_attn, target_head):\n",
    "                super().__init__()\n",
    "                self.original_dropout = original_dropout\n",
    "                self.intervention_attn = intervention_attn\n",
    "                self.target_head = target_head\n",
    "                self.training = original_dropout.training\n",
    "\n",
    "            def forward(self, attention_probs):\n",
    "                if attention_probs.dim() == 4:  # [batch, heads, seq, seq]\n",
    "                    modified_probs = attention_probs.clone()\n",
    "                    modified_probs[0, self.target_head] = self.intervention_attn\n",
    "                    return self.original_dropout(modified_probs)\n",
    "                else:\n",
    "                    return self.original_dropout(attention_probs)\n",
    "\n",
    "        # Replacing source head dropout with intervened attention\n",
    "        source_attention_layer.self.dropout = SourceInterventionDropout(\n",
    "            original_dropout, intervention_attention, source_head_idx\n",
    "        )\n",
    "\n",
    "        # Forward pass with source head patched\n",
    "        outputs = model(**clean_tokens, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "        # Extracting target head representation\n",
    "        target_hidden_states = outputs.hidden_states[target_layer][0]\n",
    "        head_dim = target_hidden_states.size(-1) // model.config.num_attention_heads\n",
    "        start_idx = target_head_idx * head_dim\n",
    "        end_idx = (target_head_idx + 1) * head_dim\n",
    "\n",
    "        target_head_repr = target_hidden_states[article_pos, start_idx:end_idx]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in causal influence measurement: {e}\")\n",
    "        # Return zero representation as fallback\n",
    "        hidden_size = model.config.hidden_size\n",
    "        head_dim = hidden_size // model.config.num_attention_heads\n",
    "        target_head_repr = torch.zeros(head_dim)\n",
    "\n",
    "    finally:\n",
    "        # Restoring original dropout\n",
    "        if original_dropout is not None:\n",
    "            source_attention_layer.self.dropout = original_dropout\n",
    "\n",
    "    return target_head_repr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZmAhpDKEngAn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def test_head_synergy_intervention_comprehensive(model, tokenizer, test_pairs,\n",
    "                                               error_sensitive_heads, gender_predictive_heads,\n",
    "                                               config):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        test_pairs: List of (correct_sentence, incorrect_sentence, gender) tuples\n",
    "        error_sensitive_heads: List of (layer, head) tuples from error sensitivity analysis\n",
    "        gender_predictive_heads: List of (layer, head) tuples from classification analysis\n",
    "        config: Dict with testing configuration\n",
    "\n",
    "    Returns:\n",
    "        Dict with detailed synergy analysis results\n",
    "    \"\"\"\n",
    "\n",
    "    results = {\n",
    "        'individual_effects': {},\n",
    "        'pairwise_synergy': {},\n",
    "        'group_effects': {},\n",
    "        'causal_chains': {},\n",
    "        'config': config\n",
    "    }\n",
    "\n",
    "    print(\" COMPREHENSIVE HEAD SYNERGY ANALYSIS\")\n",
    "    print(f\"   Configuration:\")\n",
    "    print(f\"   - Individual heads: {config['num_heads_individual']}\")\n",
    "    print(f\"   - Synergy pairs: {config['num_heads_synergy']} x {config['num_heads_synergy']}\")\n",
    "    print(f\"   - Test sentences: {len(test_pairs)} total\")\n",
    "    print(f\"   - Cross-validation: {config['cross_validation']} folds\")\n",
    "\n",
    "    # 1. Comprehensive individual head effects\n",
    "    print(\"\\n Testing individual head effects \")\n",
    "    #results['individual_effects'] = test_individual_head_effects(\n",
    "    #    model, tokenizer, test_pairs,\n",
    "    #    error_sensitive_heads + gender_predictive_heads,\n",
    "    #    config\n",
    "    #)\n",
    "\n",
    "    # 2. Pairwise synergy testing\n",
    "    #print(\"\\n Testing pairwise synergy\")\n",
    "    #results['pairwise_synergy'] = test_pairwise_synergy(\n",
    "    #    model, tokenizer, test_pairs,\n",
    "    #    error_sensitive_heads, gender_predictive_heads,\n",
    "    #    config\n",
    "    #)\n",
    "\n",
    "    # 3. Multi-size group effects\n",
    "    #print(\"\\n Testing group effects\")\n",
    "    #results['group_effects'] = test_group_effects(\n",
    "    #    model, tokenizer, test_pairs,\n",
    "    #    error_sensitive_heads, gender_predictive_heads,\n",
    "    #    config\n",
    "    #)\n",
    "\n",
    "    # 4. Comprehensive causal chains\n",
    "    print(\"\\n Testing causal chains\")\n",
    "    results['causal_chains'] = test_causal_chains(\n",
    "        model, tokenizer, test_pairs,\n",
    "        error_sensitive_heads, gender_predictive_heads,\n",
    "        config\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def test_individual_head_effects(model, tokenizer, test_pairs, all_heads, config):\n",
    "    \"\"\"\n",
    "    Test individual head effects with cross-validation and larger sample sizes\n",
    "    \"\"\"\n",
    "    individual_effects = {}\n",
    "\n",
    "    heads_to_test = all_heads[:config['num_heads_individual']]\n",
    "    sentences_per_head = config['num_sentences_individual']\n",
    "\n",
    "    print(f\"   Testing {len(heads_to_test)} heads on {sentences_per_head} sentences each...\")\n",
    "\n",
    "    for layer, head in tqdm(heads_to_test, desc=\"Individual heads\"):\n",
    "\n",
    "        cv_effects = []\n",
    "\n",
    "        for cv_fold in range(config['cross_validation']):\n",
    "            # Sampling different sentences for each fold of cross-validation\n",
    "            fold_pairs = random.sample(test_pairs, min(sentences_per_head, len(test_pairs)))\n",
    "\n",
    "            total_effect = 0\n",
    "            valid_pairs = 0\n",
    "\n",
    "            for correct_sent, incorrect_sent, gender in fold_pairs:\n",
    "                try:\n",
    "                    effect = patch_single_head(\n",
    "                        model, tokenizer, correct_sent, incorrect_sent, layer, head\n",
    "                    )\n",
    "                    total_effect += effect\n",
    "                    valid_pairs += 1\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "            if valid_pairs >= config['min_valid_pairs']:\n",
    "                cv_effects.append(total_effect / valid_pairs)\n",
    "\n",
    "        # Storing results\n",
    "        if len(cv_effects) > 0:\n",
    "            individual_effects[(layer, head)] = {\n",
    "                'mean_effect': np.mean(cv_effects),\n",
    "                'std_effect': np.std(cv_effects),\n",
    "                'cv_effects': cv_effects,\n",
    "                'valid_folds': len(cv_effects)\n",
    "            }\n",
    "\n",
    "    return individual_effects\n",
    "\n",
    "def test_pairwise_synergy(model, tokenizer, test_pairs,\n",
    "                                      error_sensitive_heads, gender_predictive_heads, config):\n",
    "    \"\"\"\n",
    "    Comprehensive pairwise synergy testing with more combinations\n",
    "    \"\"\"\n",
    "    synergy_scores = {}\n",
    "\n",
    "    # Test more head combinations\n",
    "    error_heads_to_test = error_sensitive_heads[:config['num_heads_synergy']]\n",
    "    pred_heads_to_test = gender_predictive_heads[:config['num_heads_synergy']]\n",
    "\n",
    "    # Also test within-category synergy\n",
    "    combinations_to_test = []\n",
    "\n",
    "    # Cross-category combinations (error + predictive)\n",
    "    for error_head in error_heads_to_test:\n",
    "        for pred_head in pred_heads_to_test:\n",
    "            if error_head != pred_head:  # Don't test head with itself\n",
    "                combinations_to_test.append(('cross', error_head, pred_head))\n",
    "\n",
    "    # Within-category combinations (error + error, predictive + predictive)\n",
    "    for head1, head2 in combinations(error_heads_to_test, 2):\n",
    "        combinations_to_test.append(('error_error', head1, head2))\n",
    "\n",
    "    for head1, head2 in combinations(pred_heads_to_test, 2):\n",
    "        combinations_to_test.append(('pred_pred', head1, head2))\n",
    "\n",
    "    print(f\"   Testing {len(combinations_to_test)} head pair combinations...\")\n",
    "\n",
    "    for combination_type, head1, head2 in tqdm(combinations_to_test, desc=\"Synergy pairs\"):\n",
    "\n",
    "        synergy_score = calculate_synergy_score(\n",
    "            model, tokenizer, test_pairs[:config['num_sentences_synergy']],\n",
    "            head1, head2, config\n",
    "        )\n",
    "\n",
    "        synergy_scores[((head1, head2), combination_type)] = synergy_score\n",
    "\n",
    "    return synergy_scores\n",
    "\n",
    "def calculate_synergy_score(model, tokenizer, test_pairs, head1, head2, config):\n",
    "    \"\"\"\n",
    "    Calculate synergy score to see if effect of patching 2 heads is greater than patching the heads individually. Typically, one head\n",
    "    from the error-sensitive heads which are in earlier layers and one from later layers is used. Or both from same category but at different layers.\n",
    "    \"\"\"\n",
    "\n",
    "    synergy_scores = []\n",
    "\n",
    "    for correct_sent, incorrect_sent, gender in test_pairs:\n",
    "        try:\n",
    "            # Effect of head1 alone\n",
    "            effect1 = patch_single_head(model, tokenizer, correct_sent, incorrect_sent, *head1)\n",
    "\n",
    "            # Effect of head2 alone\n",
    "            effect2 = patch_single_head(model, tokenizer, correct_sent, incorrect_sent, *head2)\n",
    "\n",
    "            # Effect of both heads together\n",
    "            effect_both = patch_multiple_heads(model, tokenizer, correct_sent, incorrect_sent, [head1, head2])\n",
    "\n",
    "            # Synergy = joint effect - sum of individual effects\n",
    "            synergy = effect_both - (effect1 + effect2)\n",
    "            synergy_scores.append(synergy)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if len(synergy_scores) >= config['min_valid_pairs']:\n",
    "        return {\n",
    "            'mean_synergy': np.mean(synergy_scores),\n",
    "            'std_synergy': np.std(synergy_scores),\n",
    "            'median_synergy': np.median(synergy_scores),\n",
    "            'valid_pairs': len(synergy_scores),\n",
    "            'raw_scores': synergy_scores\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def test_group_effects(model, tokenizer, test_pairs,\n",
    "                                   error_sensitive_heads, gender_predictive_heads, config):\n",
    "    \"\"\"\n",
    "    Test group effects with multiple group sizes and compositions\n",
    "    \"\"\"\n",
    "    group_effects = {}\n",
    "\n",
    "    sentences_to_use = test_pairs[:config['num_sentences_group']]\n",
    "\n",
    "    for group_size in config['group_sizes']:\n",
    "        print(f\"   Testing groups of size {group_size}...\")\n",
    "\n",
    "        # Test different group compositions\n",
    "        group_compositions = {\n",
    "            f'top_{group_size}_error': error_sensitive_heads[:group_size],\n",
    "            f'top_{group_size}_predictive': gender_predictive_heads[:group_size],\n",
    "            #f'mixed_{group_size}': (error_sensitive_heads[:group_size//2] +\n",
    "            #                       gender_predictive_heads[:group_size//2]),\n",
    "            f'combined_{group_size}': (error_sensitive_heads[:group_size//2] +\n",
    "                                      gender_predictive_heads[:group_size//2])\n",
    "        }\n",
    "\n",
    "        for group_name, head_group in group_compositions.items():\n",
    "            if len(head_group) == group_size:  # Ensure we have enough heads\n",
    "\n",
    "                group_effect = test_head_group(\n",
    "                    model, tokenizer, sentences_to_use, head_group, config\n",
    "                )\n",
    "\n",
    "                group_effects[group_name] = group_effect\n",
    "\n",
    "    return group_effects\n",
    "\n",
    "def test_head_group(model, tokenizer, test_pairs, head_group, config):\n",
    "\n",
    "    group_effects = []\n",
    "\n",
    "    for correct_sent, incorrect_sent, gender in test_pairs:\n",
    "        try:\n",
    "            effect = patch_multiple_heads(model, tokenizer, correct_sent, incorrect_sent, head_group)\n",
    "            group_effects.append(effect)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if len(group_effects) >= config['min_valid_pairs']:\n",
    "        return {\n",
    "            'mean_effect': np.mean(group_effects),\n",
    "            'std_effect': np.std(group_effects),\n",
    "            'median_effect': np.median(group_effects),\n",
    "            'valid_pairs': len(group_effects),\n",
    "            'head_count': len(head_group),\n",
    "            'heads': head_group\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def test_causal_chains(model, tokenizer, test_pairs,\n",
    "                                   error_sensitive_heads, gender_predictive_heads, config):\n",
    "    \"\"\"\n",
    "    Comprehensive causal chain testing\n",
    "    \"\"\"\n",
    "    causal_effects = {}\n",
    "\n",
    "    # Test more heads for causal relationships\n",
    "    error_heads_to_test = error_sensitive_heads[:config['num_heads_causal']]\n",
    "    pred_heads_to_test = gender_predictive_heads[:config['num_heads_causal']]\n",
    "\n",
    "    sentences_to_use = test_pairs[:config['num_sentences_causal']]\n",
    "\n",
    "    causal_pairs = []\n",
    "\n",
    "    # Error → Predictive chains\n",
    "    for error_head in error_heads_to_test:\n",
    "        for pred_head in pred_heads_to_test:\n",
    "            if error_head != pred_head and error_head[0] <= pred_head[0]:\n",
    "                causal_pairs.append(('error_to_pred', error_head, pred_head))\n",
    "\n",
    "    # Within-category chains (earlier → later layers)\n",
    "    for head1 in error_heads_to_test:\n",
    "        for head2 in error_heads_to_test:\n",
    "            if head1[0] < head2[0]:  # Earlier layer to later layer\n",
    "                causal_pairs.append(('error_to_error', head1, head2))\n",
    "\n",
    "    for head1 in pred_heads_to_test:\n",
    "        for head2 in pred_heads_to_test:\n",
    "            if head1[0] < head2[0]:  # Earlier layer to later layer\n",
    "                causal_pairs.append(('pred_to_pred', head1, head2))\n",
    "\n",
    "    print(f\"   Testing {len(causal_pairs)} causal relationships...\")\n",
    "\n",
    "    for causal_type, source_head, target_head in tqdm(causal_pairs, desc=\"Causal chains\"):\n",
    "\n",
    "        causal_effect = measure_causal_influence(\n",
    "            model, tokenizer, sentences_to_use, source_head, target_head, config\n",
    "        )\n",
    "\n",
    "        if causal_effect is not None:\n",
    "            causal_effects[((source_head, target_head), causal_type)] = causal_effect\n",
    "\n",
    "    return causal_effects\n",
    "\n",
    "def measure_causal_influence(model, tokenizer, test_pairs, source_head, target_head, config):\n",
    "    \"\"\"\n",
    "    Measure causal influence where the source head is patched and the effect on the target head is noted\n",
    "    \"\"\"\n",
    "    influences = []\n",
    "\n",
    "    for correct_sent, incorrect_sent, gender in test_pairs:\n",
    "        try:\n",
    "            # Getting baseline target head representation\n",
    "            baseline_repr = get_head_representation(\n",
    "                model, tokenizer, correct_sent, *target_head\n",
    "            )\n",
    "\n",
    "            # Patching source head and see how target head changes\n",
    "            patched_repr = get_head_representation_with_patch(\n",
    "                model, tokenizer, correct_sent, incorrect_sent, source_head, target_head\n",
    "            )\n",
    "\n",
    "            baseline_norm = torch.norm(baseline_repr).item()\n",
    "            if baseline_norm > 1e-8:\n",
    "                influence = torch.norm(patched_repr - baseline_repr).item() / baseline_norm\n",
    "            else:\n",
    "                influence = torch.norm(patched_repr - baseline_repr).item()\n",
    "\n",
    "            influences.append(influence)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if len(influences) >= config['min_valid_pairs']:\n",
    "        return {\n",
    "            'mean_influence': np.mean(influences),\n",
    "            'std_influence': np.std(influences),\n",
    "            'median_influence': np.median(influences),\n",
    "            'valid_pairs': len(influences),\n",
    "            'raw_influences': influences\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def analyze_synergy_results_comprehensive(synergy_results, config):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis and interpretation of synergy test results for individual, pairwise, group effects as well as causal chains\n",
    "    \"\"\"\n",
    "    print(\"\\n COMPREHENSIVE SYNERGY ANALYSIS RESULTS\") # with confidence intervals\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "    #individual = synergy_results['individual_effects']\n",
    "    #if individual:\n",
    "    #    print(\"\\n TOP INDIVIDUAL HEAD EFFECTS :\")\n",
    "    #    sorted_individual = sorted(\n",
    "    #        [(k, v) for k, v in individual.items() if v.get('valid_folds', 0) > 0],\n",
    "    #        key=lambda x: x[1]['mean_effect'], reverse=True\n",
    "    #    )\n",
    "\n",
    "    #    for (layer, head), data in sorted_individual[:10]:\n",
    "    #        mean_eff = data['mean_effect']\n",
    "    #        std_eff = data['std_effect']\n",
    "    #        ci_lower = mean_eff - 1.96 * std_eff / np.sqrt(data['valid_folds'])\n",
    "    #        ci_upper = mean_eff + 1.96 * std_eff / np.sqrt(data['valid_folds'])\n",
    "\n",
    "    #        print(f\"  Layer {layer:2d}, Head {head:2d}: {mean_eff:.4f} ± {std_eff:.4f} \"\n",
    "    #              f\"[{ci_lower:.4f}, {ci_upper:.4f}] (n={data['valid_folds']})\")\n",
    "\n",
    "    pairwise = synergy_results['pairwise_synergy']\n",
    "    if pairwise:\n",
    "        print(\"\\nTOP SYNERGISTIC PAIRS:\") # (with significance)\n",
    "\n",
    "        valid_pairs = [(k, v) for k, v in pairwise.items() if v is not None]\n",
    "        sorted_synergy = sorted(valid_pairs, key=lambda x: x[1]['mean_synergy'], reverse=True)\n",
    "\n",
    "        for ((head1, head2), pair_type), data in sorted_synergy[:10]:\n",
    "            mean_syn = data['mean_synergy']\n",
    "            std_syn = data['std_synergy']\n",
    "\n",
    "            # Simple significance test checking if the mean is significantly different from 0?\n",
    "            t_stat = mean_syn / (std_syn / np.sqrt(data['valid_pairs'])) if std_syn > 0 else 0\n",
    "            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), data['valid_pairs'] - 1))\n",
    "\n",
    "            synergy_type = \" Synergistic\" if mean_syn > 0.05 else \" Interfering\" if mean_syn < -0.05 else \" Independent\"\n",
    "            significance = \"*\" if p_value < 0.05 else \"\"\n",
    "\n",
    "            print(f\"  {head1} + {head2} ({pair_type}): {mean_syn:.4f} ± {std_syn:.4f} \"\n",
    "                  f\"({synergy_type}){significance} p={p_value:.3f}\")\n",
    "\n",
    "    group = synergy_results['group_effects']\n",
    "    if group:\n",
    "        print(\"\\nGROUP EFFECTS:\")\n",
    "        for group_name, data in group.items():\n",
    "            if data is not None:\n",
    "                print(f\"  {group_name}: {data['mean_effect']:.4f} ± {data['std_effect']:.4f} \"\n",
    "                      f\"(n={data['valid_pairs']}, heads={data['head_count']})\")\n",
    "\n",
    "    chains = synergy_results['causal_chains']\n",
    "    if chains:\n",
    "        print(\"\\n STRONGEST CAUSAL INFLUENCES:\")\n",
    "\n",
    "        valid_chains = [(k, v) for k, v in chains.items() if v is not None]\n",
    "        sorted_chains = sorted(valid_chains, key=lambda x: x[1]['mean_influence'], reverse=True)\n",
    "\n",
    "        for ((source, target), chain_type), data in sorted_chains[:10]:\n",
    "            mean_inf = data['mean_influence']\n",
    "            std_inf = data['std_influence']\n",
    "\n",
    "            print(f\"  {source} → {target} ({chain_type}): {mean_inf:.4f} ± {std_inf:.4f} \"\n",
    "                  f\"(n={data['valid_pairs']})\")\n",
    "\n",
    "\n",
    "def run_efficient_synergy_analysis(model, tokenizer, test_pairs,\n",
    "                                 error_sensitive_heads, gender_predictive_heads):\n",
    "    \"\"\"\n",
    "    Defining config\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    efficient_config = {\n",
    "        'num_heads_individual': 14,      # Testing all heads individually\n",
    "        'num_heads_synergy': 5,          # Top 5 from each (25 combinations)\n",
    "        'num_heads_causal': 5,           # Top 5\n",
    "        'num_sentences_individual': 80,\n",
    "        'num_sentences_synergy': 50,\n",
    "        'num_sentences_group': 70,\n",
    "        'num_sentences_causal': 30,\n",
    "        'group_sizes': [3, 5],           # Just two group sizes\n",
    "        'cross_validation': 3,           # Just 3 folds\n",
    "        'min_valid_pairs': 5\n",
    "    }\n",
    "\n",
    "    print(f\"Starting efficient analysis with {len(test_pairs)} test pairs...\")\n",
    "    print(f\"Testing: {efficient_config['num_heads_synergy']}×{efficient_config['num_heads_synergy']} = {efficient_config['num_heads_synergy']**2} synergy pairs\")\n",
    "\n",
    "    synergy_results = test_head_synergy_intervention_comprehensive(\n",
    "        model, tokenizer, test_pairs,\n",
    "        error_sensitive_heads, gender_predictive_heads,\n",
    "        efficient_config\n",
    "    )\n",
    "\n",
    "    analyze_synergy_results_comprehensive(synergy_results, efficient_config)\n",
    "\n",
    "    return synergy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5x0e37mtlqex",
    "outputId": "af197830-9bc1-4cc5-af09-3d5124c9cc40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting efficient analysis with 100 test pairs...\n",
      "Testing: 5×5 = 25 synergy pairs\n",
      " COMPREHENSIVE HEAD SYNERGY ANALYSIS\n",
      "   Configuration:\n",
      "   - Individual heads: 14\n",
      "   - Synergy pairs: 5 x 5\n",
      "   - Test sentences: 100 total\n",
      "   - Cross-validation: 3 folds\n",
      "\n",
      " Testing individual head effects \n",
      "\n",
      " Testing causal chains\n",
      "   Testing 33 causal relationships...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Causal chains: 100%|██████████| 33/33 [02:13<00:00,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " COMPREHENSIVE SYNERGY ANALYSIS RESULTS\n",
      "============================================================\n",
      "\n",
      " STRONGEST CAUSAL INFLUENCES:\n",
      "  (6, 0) → (8, 5) (pred_to_pred): 0.3239 ± 0.4697 (n=30)\n",
      "  (6, 0) → (8, 0) (pred_to_pred): 0.3234 ± 0.4768 (n=30)\n",
      "  (6, 4) → (8, 5) (pred_to_pred): 0.3218 ± 0.4717 (n=30)\n",
      "  (0, 0) → (8, 5) (error_to_pred): 0.3211 ± 0.4721 (n=30)\n",
      "  (6, 4) → (8, 0) (pred_to_pred): 0.3211 ± 0.4776 (n=30)\n",
      "  (0, 0) → (8, 0) (error_to_pred): 0.3203 ± 0.4788 (n=30)\n",
      "  (0, 11) → (8, 5) (error_to_pred): 0.3188 ± 0.4750 (n=30)\n",
      "  (0, 11) → (8, 0) (error_to_pred): 0.3181 ± 0.4795 (n=30)\n",
      "  (0, 4) → (8, 5) (error_to_pred): 0.3153 ± 0.4762 (n=30)\n",
      "  (0, 0) → (6, 0) (error_to_pred): 0.3149 ± 0.4671 (n=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'individual_effects': {},\n",
       " 'pairwise_synergy': {},\n",
       " 'group_effects': {},\n",
       " 'causal_chains': {(((0, 8), (8, 5)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.3139770758071471), 'std_influence': np.float64(0.4776290972799598), 'median_influence': np.float64(0.008366465743970111), 'valid_pairs': 30, 'raw_influences': [1.0410605731495473,\n",
       "    0.006288949010171902,\n",
       "    1.1017667660747108,\n",
       "    0.005520208381914462,\n",
       "    1.0286425073700398,\n",
       "    0.0089668298107244,\n",
       "    0.008595327363224119,\n",
       "    0.0027632690727929553,\n",
       "    0.006245707105633919,\n",
       "    0.007035099419349268,\n",
       "    0.8264772896250354,\n",
       "    1.3361900693804658,\n",
       "    0.0034144596339652613,\n",
       "    0.003734178427415919,\n",
       "    0.01466724249516617,\n",
       "    0.009849652792046698,\n",
       "    0.007045786568529879,\n",
       "    0.017301256923706747,\n",
       "    1.0411246184855287,\n",
       "    0.010624240582884895,\n",
       "    0.006137104100608376,\n",
       "    1.214927864523783,\n",
       "    0.005870091090076657,\n",
       "    0.008137604124716103,\n",
       "    0.8629840226119876,\n",
       "    0.0024261807697640407,\n",
       "    0.0071440683112495885,\n",
       "    0.0022070271625727056,\n",
       "    0.8191475611687421,\n",
       "    0.0030167186780592245]},\n",
       "  (((0, 8), (6, 0)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.3058737828004516), 'std_influence': np.float64(0.4721987926568791), 'median_influence': np.float64(0.01060933762259331), 'valid_pairs': 30, 'raw_influences': [1.097232801102173,\n",
       "    0.009438658933256985,\n",
       "    1.1504271868104783,\n",
       "    0.008163443934363515,\n",
       "    0.8219884919840819,\n",
       "    0.014093187254260474,\n",
       "    0.013168960844498081,\n",
       "    0.0025129690612514465,\n",
       "    0.009608497715859568,\n",
       "    0.009428941299301009,\n",
       "    0.859239002153998,\n",
       "    1.0634973515154005,\n",
       "    0.005052089271127528,\n",
       "    0.003819534731972585,\n",
       "    0.02535937069448584,\n",
       "    0.010040949732789834,\n",
       "    0.006918574074298687,\n",
       "    0.014782933188262888,\n",
       "    1.4994754065727443,\n",
       "    0.012876252619331516,\n",
       "    0.005956975760946252,\n",
       "    0.9847700579045426,\n",
       "    0.007680377152979438,\n",
       "    0.011177725512396784,\n",
       "    0.5820607389926765,\n",
       "    0.00321071230584845,\n",
       "    0.008991935485624578,\n",
       "    0.0020221861736011647,\n",
       "    0.9296645232807721,\n",
       "    0.0035536479502232647]},\n",
       "  (((0, 8), (6, 4)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.28998535514339385), 'std_influence': np.float64(0.43645449992961116), 'median_influence': np.float64(0.01022121094637362), 'valid_pairs': 30, 'raw_influences': [0.8259944148514192,\n",
       "    0.008197742252506074,\n",
       "    1.0165259497464938,\n",
       "    0.008296671565875012,\n",
       "    1.0529253521765682,\n",
       "    0.010879091773473326,\n",
       "    0.013171827295840426,\n",
       "    0.0025760302477682057,\n",
       "    0.00922875569765961,\n",
       "    0.0070592381695002634,\n",
       "    0.8725191226831924,\n",
       "    1.1540330708533886,\n",
       "    0.005133646928301868,\n",
       "    0.004107724311954761,\n",
       "    0.02551792755886338,\n",
       "    0.008861696606075728,\n",
       "    0.008754839521318343,\n",
       "    0.013985200445833443,\n",
       "    1.0452179853728065,\n",
       "    0.014884329293004114,\n",
       "    0.00793911130426005,\n",
       "    0.9747544503295116,\n",
       "    0.0080531781542806,\n",
       "    0.01426641702817983,\n",
       "    0.6275779842590383,\n",
       "    0.0035778597607555593,\n",
       "    0.009563330119273914,\n",
       "    0.0031581973277923702,\n",
       "    0.9388381164039656,\n",
       "    0.0039613922629132626]},\n",
       "  (((0, 8), (11, 7)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.24743679280775968), 'std_influence': np.float64(0.3814088978584815), 'median_influence': np.float64(0.006216816913079248), 'valid_pairs': 30, 'raw_influences': [0.7973293340398115,\n",
       "    0.004807517955857742,\n",
       "    0.9792863748482021,\n",
       "    0.004486217242624767,\n",
       "    0.739059194253043,\n",
       "    0.006088642631032323,\n",
       "    0.0070729767454061065,\n",
       "    0.0027734328043978605,\n",
       "    0.005076831859746116,\n",
       "    0.00560259413066638,\n",
       "    0.7366726900752787,\n",
       "    0.9753951022687669,\n",
       "    0.0022859802755348037,\n",
       "    0.002498777513027351,\n",
       "    0.009181527892530216,\n",
       "    0.006403390782051503,\n",
       "    0.003852763041931284,\n",
       "    0.019048217638082832,\n",
       "    0.9485559109295177,\n",
       "    0.009371565201045104,\n",
       "    0.0058859407275226065,\n",
       "    0.9966245223506528,\n",
       "    0.0063449911951261725,\n",
       "    0.004894793692573803,\n",
       "    0.44929802237333716,\n",
       "    0.0020219435009705775,\n",
       "    0.004053912046782986,\n",
       "    0.0014312828365978902,\n",
       "    0.6856341273016766,\n",
       "    0.002065206078993638]},\n",
       "  (((0, 8), (8, 0)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.31349431361339747), 'std_influence': np.float64(0.4825131976569271), 'median_influence': np.float64(0.007372477465010461), 'valid_pairs': 30, 'raw_influences': [1.0413226563701348,\n",
       "    0.007015215266676917,\n",
       "    1.2633227114235657,\n",
       "    0.005063983484460522,\n",
       "    0.9213748057891799,\n",
       "    0.00958998526093278,\n",
       "    0.009177002211094246,\n",
       "    0.0022415896472026144,\n",
       "    0.006365267786598981,\n",
       "    0.007199539708877972,\n",
       "    0.9394864267167757,\n",
       "    1.1996402899212515,\n",
       "    0.0031673036432592764,\n",
       "    0.002637245860433407,\n",
       "    0.01625511222297316,\n",
       "    0.007155600996917157,\n",
       "    0.005467267149054171,\n",
       "    0.01662768898206631,\n",
       "    1.3585872373444332,\n",
       "    0.010303763354635984,\n",
       "    0.005727026907423954,\n",
       "    1.0561502036160622,\n",
       "    0.005263228027378832,\n",
       "    0.00754541522114295,\n",
       "    0.6508555737588557,\n",
       "    0.0027980722626699847,\n",
       "    0.005908298765677446,\n",
       "    0.002058304999560208,\n",
       "    0.8341468264207025,\n",
       "    0.0023757652819248747]},\n",
       "  (((0, 0), (8, 5)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.3211009656532912), 'std_influence': np.float64(0.4720931678889761), 'median_influence': np.float64(0.020748883561529124), 'valid_pairs': 30, 'raw_influences': [1.036384053639313,\n",
       "    0.008380378881679447,\n",
       "    1.1016385782891065,\n",
       "    0.005781379824813618,\n",
       "    1.0315768580457343,\n",
       "    0.005908471650261566,\n",
       "    0.01694036833468401,\n",
       "    0.02052054501877876,\n",
       "    0.007912774369183835,\n",
       "    0.016694138749382414,\n",
       "    0.8229585605809865,\n",
       "    1.3351422336765233,\n",
       "    0.01678928660093196,\n",
       "    0.029523700975549478,\n",
       "    0.005200247334042706,\n",
       "    0.015425802497193728,\n",
       "    0.021827925466346723,\n",
       "    0.01773793236381686,\n",
       "    1.0363910669468759,\n",
       "    0.011324982739533038,\n",
       "    0.03904306725813659,\n",
       "    1.2142616860108886,\n",
       "    0.0097981184471227,\n",
       "    0.020250966383780888,\n",
       "    0.8601472523241642,\n",
       "    0.02665680870239536,\n",
       "    0.01904064791552684,\n",
       "    0.039914163547083346,\n",
       "    0.8188797509206225,\n",
       "    0.020977222104279485]},\n",
       "  (((0, 0), (6, 0)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.3149436879427308), 'std_influence': np.float64(0.46707392801831055), 'median_influence': np.float64(0.02461375718113552), 'valid_pairs': 30, 'raw_influences': [1.1047326379737046,\n",
       "    0.012688426514050248,\n",
       "    1.1486686422922763,\n",
       "    0.009164172186069165,\n",
       "    0.8225601959691572,\n",
       "    0.008491120304268346,\n",
       "    0.018075633299887442,\n",
       "    0.025412909137212267,\n",
       "    0.013035839209368403,\n",
       "    0.018916774504726622,\n",
       "    0.8625849633750535,\n",
       "    1.06099092434691,\n",
       "    0.022985582885172957,\n",
       "    0.02892232272043451,\n",
       "    0.00880785465917516,\n",
       "    0.021831056109631018,\n",
       "    0.026682727081142363,\n",
       "    0.01925283291888414,\n",
       "    1.4989263971659164,\n",
       "    0.015592848985742012,\n",
       "    0.04330392035501491,\n",
       "    0.9878177553584957,\n",
       "    0.017623283215778014,\n",
       "    0.02177691111497963,\n",
       "    0.579467392123559,\n",
       "    0.03143264588250028,\n",
       "    0.023814605225058773,\n",
       "    0.04484885872419374,\n",
       "    0.9295944528110542,\n",
       "    0.020306951832508045]},\n",
       "  (((0, 0), (6, 4)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.29998099200044237), 'std_influence': np.float64(0.4301294686833241), 'median_influence': np.float64(0.026826700136017213), 'valid_pairs': 30, 'raw_influences': [0.8258041456140955,\n",
       "    0.008207031046679289,\n",
       "    1.0193332641043398,\n",
       "    0.011216458556242074,\n",
       "    1.0539320676231108,\n",
       "    0.0077317460772769854,\n",
       "    0.025785485238399946,\n",
       "    0.030160031211076247,\n",
       "    0.013202626221346294,\n",
       "    0.01609876394533036,\n",
       "    0.867087143339349,\n",
       "    1.148998197045481,\n",
       "    0.023265852766947873,\n",
       "    0.03866453404933113,\n",
       "    0.009775371547689798,\n",
       "    0.020879127825020792,\n",
       "    0.02786791503363448,\n",
       "    0.015709122966611942,\n",
       "    1.044544431218032,\n",
       "    0.018508821697644937,\n",
       "    0.04825225144084895,\n",
       "    0.9755156669802774,\n",
       "    0.011942717573847923,\n",
       "    0.023586400129390984,\n",
       "    0.6312471415991461,\n",
       "    0.02846212983608878,\n",
       "    0.02135426625793425,\n",
       "    0.06334604710076087,\n",
       "    0.9433248947400985,\n",
       "    0.025626107227235802]},\n",
       "  (((0, 0), (11, 7)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.2514089698292537), 'std_influence': np.float64(0.37900819124245466), 'median_influence': np.float64(0.012010398362395742), 'valid_pairs': 30, 'raw_influences': [0.7945925640916869,\n",
       "    0.005780355794132521,\n",
       "    0.979143343068912,\n",
       "    0.006005694725630218,\n",
       "    0.7407137417459011,\n",
       "    0.005034975486764952,\n",
       "    0.01036813723842138,\n",
       "    0.012032673594115166,\n",
       "    0.005499217672959203,\n",
       "    0.01132063226496494,\n",
       "    0.7346858016551453,\n",
       "    0.9741731588050281,\n",
       "    0.009654562353761453,\n",
       "    0.019031073486311684,\n",
       "    0.004395075624492698,\n",
       "    0.011547012420466149,\n",
       "    0.011864682303657151,\n",
       "    0.01415300973046072,\n",
       "    0.9514546182669171,\n",
       "    0.009883362944173139,\n",
       "    0.019077799001555013,\n",
       "    0.9980947748985972,\n",
       "    0.007841951704110794,\n",
       "    0.011243941993703358,\n",
       "    0.44874688841728355,\n",
       "    0.012392050107857079,\n",
       "    0.011988123130676319,\n",
       "    0.024650272440025,\n",
       "    0.6871247524853692,\n",
       "    0.00977484742453351]},\n",
       "  (((0, 0), (8, 0)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.3202684910747193), 'std_influence': np.float64(0.47876095221979925), 'median_influence': np.float64(0.01821122381139565), 'valid_pairs': 30, 'raw_influences': [1.041974367110829,\n",
       "    0.008071978914359193,\n",
       "    1.2642727673600003,\n",
       "    0.0067801321922842635,\n",
       "    0.9227143910987297,\n",
       "    0.005695400441239676,\n",
       "    0.01339966374353108,\n",
       "    0.01977513447645847,\n",
       "    0.010755299808598523,\n",
       "    0.014595970479592265,\n",
       "    0.9414345150066006,\n",
       "    1.2001531475269873,\n",
       "    0.015441197161187328,\n",
       "    0.022662607883281758,\n",
       "    0.006444578175647957,\n",
       "    0.01449701121051085,\n",
       "    0.019155333621082876,\n",
       "    0.013306037201993194,\n",
       "    1.3595860597173297,\n",
       "    0.01410489240040606,\n",
       "    0.035301250416832894,\n",
       "    1.0578904422802713,\n",
       "    0.011587614969951707,\n",
       "    0.017267114001708425,\n",
       "    0.6477997933264655,\n",
       "    0.025632121565555343,\n",
       "    0.01658885533729431,\n",
       "    0.03226453404752217,\n",
       "    0.8351482253985071,\n",
       "    0.013754295366819963]},\n",
       "  (((0, 3), (8, 5)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.31230380813095854), 'std_influence': np.float64(0.47837209312729023), 'median_influence': np.float64(0.005343263535640555), 'valid_pairs': 30, 'raw_influences': [1.0371333701404937,\n",
       "    0.003492460109200636,\n",
       "    1.1020561470835653,\n",
       "    0.003132711664678142,\n",
       "    1.0289913110190745,\n",
       "    0.005069445088826378,\n",
       "    0.005617081982454733,\n",
       "    0.008274966909780534,\n",
       "    0.0028598719550241417,\n",
       "    0.00202235590010457,\n",
       "    0.8266209053450567,\n",
       "    1.3345662103332634,\n",
       "    0.006465054605876093,\n",
       "    0.006438872363995319,\n",
       "    0.003116369837027468,\n",
       "    0.007968595722119054,\n",
       "    0.002657897892782903,\n",
       "    0.0038341997215451573,\n",
       "    1.0425307531699297,\n",
       "    0.003198121684714838,\n",
       "    0.013009751281576028,\n",
       "    1.213463243853525,\n",
       "    0.0038217049178806057,\n",
       "    0.004471209103404491,\n",
       "    0.8629385330363916,\n",
       "    0.003545881038977646,\n",
       "    0.003917646568104304,\n",
       "    0.0037482907628689807,\n",
       "    0.8204914654796651,\n",
       "    0.003659815356851838]},\n",
       "  (((0, 3), (6, 0)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.30396726985603045), 'std_influence': np.float64(0.4738695615973428), 'median_influence': np.float64(0.006524097576764426), 'valid_pairs': 30, 'raw_influences': [1.0998192173569752,\n",
       "    0.0052229109046753675,\n",
       "    1.150010072062164,\n",
       "    0.004229290197705922,\n",
       "    0.8225759898558123,\n",
       "    0.006778831836632287,\n",
       "    0.006822994290209388,\n",
       "    0.011692731234670843,\n",
       "    0.0048934173380352705,\n",
       "    0.0033574627582387043,\n",
       "    0.8564889352914922,\n",
       "    1.061099842340329,\n",
       "    0.00531131632238697,\n",
       "    0.007704758919280613,\n",
       "    0.004583972012944819,\n",
       "    0.010404532712495244,\n",
       "    0.002767343510762094,\n",
       "    0.00474263844794107,\n",
       "    1.5027918715609299,\n",
       "    0.0032107164945210914,\n",
       "    0.015044432146818545,\n",
       "    0.9889975590913045,\n",
       "    0.005854860070197272,\n",
       "    0.006269363316896564,\n",
       "    0.5809717731500569,\n",
       "    0.004477221017235454,\n",
       "    0.005390013693440883,\n",
       "    0.0036268688160992035,\n",
       "    0.9301722311132605,\n",
       "    0.0037049278174005624]},\n",
       "  (((0, 3), (6, 4)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.2882946143154183), 'std_influence': np.float64(0.43754267760364546), 'median_influence': np.float64(0.006687605455150103), 'valid_pairs': 30, 'raw_influences': [0.8256581878268664,\n",
       "    0.004080850986741529,\n",
       "    1.0189248018039099,\n",
       "    0.004926316260164047,\n",
       "    1.052402182434587,\n",
       "    0.005787447733078026,\n",
       "    0.008907321683326284,\n",
       "    0.012192014106152747,\n",
       "    0.005095399601737417,\n",
       "    0.0024585503113455447,\n",
       "    0.8694143181489714,\n",
       "    1.1510920427055067,\n",
       "    0.006238783263805624,\n",
       "    0.008791045484617465,\n",
       "    0.005043222458225428,\n",
       "    0.011583775562016159,\n",
       "    0.004352166047496788,\n",
       "    0.004555543075128171,\n",
       "    1.0464004802993707,\n",
       "    0.0037414830511360683,\n",
       "    0.018618755956945718,\n",
       "    0.976779219035206,\n",
       "    0.006028874519061829,\n",
       "    0.0069279711126570675,\n",
       "    0.6279195092028885,\n",
       "    0.004629014601126635,\n",
       "    0.004965185304458863,\n",
       "    0.006447239797643139,\n",
       "    0.9401159111802609,\n",
       "    0.004760815908116571]},\n",
       "  (((0, 3), (11, 7)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.24611591936224977), 'std_influence': np.float64(0.3822438550903403), 'median_influence': np.float64(0.004128029124322306), 'valid_pairs': 30, 'raw_influences': [0.7940508167145683,\n",
       "    0.0017960618522788717,\n",
       "    0.9784922583734134,\n",
       "    0.0021191637989401383,\n",
       "    0.7399991964730968,\n",
       "    0.0038607897847711963,\n",
       "    0.004395268463873416,\n",
       "    0.00716187964341837,\n",
       "    0.002934679286838058,\n",
       "    0.0012930517494359952,\n",
       "    0.7358565110624519,\n",
       "    0.9735628003899252,\n",
       "    0.0061581244382491055,\n",
       "    0.004877787693855431,\n",
       "    0.002515769895280507,\n",
       "    0.0075584071826273105,\n",
       "    0.0017463619001493078,\n",
       "    0.0033267313076406273,\n",
       "    0.9527097278202722,\n",
       "    0.0026500816900160637,\n",
       "    0.008804681790352873,\n",
       "    0.9978404866254528,\n",
       "    0.003309890530345062,\n",
       "    0.0030479544368432108,\n",
       "    0.4479286532964735,\n",
       "    0.0022317346360072117,\n",
       "    0.0023569871269554423,\n",
       "    0.0023379315160063485,\n",
       "    0.6864990974039822,\n",
       "    0.002054693983972191]},\n",
       "  (((0, 3), (8, 0)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.31208159268119806), 'std_influence': np.float64(0.48382499009612234), 'median_influence': np.float64(0.0047477014417219385), 'valid_pairs': 30, 'raw_influences': [1.0421940039411968,\n",
       "    0.0031520807146334273,\n",
       "    1.2636332129228125,\n",
       "    0.0031439170828166754,\n",
       "    0.9227064132473263,\n",
       "    0.00506933998518997,\n",
       "    0.00449671880518557,\n",
       "    0.009071696444393132,\n",
       "    0.0036962592021804064,\n",
       "    0.001995943774729643,\n",
       "    0.9376535081050235,\n",
       "    1.1993658780068779,\n",
       "    0.0064863961219622755,\n",
       "    0.004998684078258307,\n",
       "    0.003339354396282027,\n",
       "    0.00713004235409446,\n",
       "    0.0021901532943717846,\n",
       "    0.0030282413063639217,\n",
       "    1.3614652775962848,\n",
       "    0.0030227784552920647,\n",
       "    0.011121584293360997,\n",
       "    1.0576368955012427,\n",
       "    0.003937419994268057,\n",
       "    0.004279781210389864,\n",
       "    0.6497640152089212,\n",
       "    0.0036145467971485687,\n",
       "    0.003538517541522663,\n",
       "    0.002895172354944248,\n",
       "    0.8351983642668039,\n",
       "    0.00262158343206312]},\n",
       "  (((0, 4), (8, 5)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.3153282059573402), 'std_influence': np.float64(0.4761773504124825), 'median_influence': np.float64(0.010955886811379191), 'valid_pairs': 30, 'raw_influences': [1.0392571979368475,\n",
       "    0.011614648544215714,\n",
       "    1.0994077941007447,\n",
       "    0.005688857535980222,\n",
       "    1.0304920680363856,\n",
       "    0.008695381284145782,\n",
       "    0.014471674300059063,\n",
       "    0.006688098362624585,\n",
       "    0.0021596616300613176,\n",
       "    0.00862453261333602,\n",
       "    0.8252665812839073,\n",
       "    1.3345022713604362,\n",
       "    0.00812131912777653,\n",
       "    0.011759717900501763,\n",
       "    0.004329286372450402,\n",
       "    0.024024648237175927,\n",
       "    0.009249618597342007,\n",
       "    0.010297125078542666,\n",
       "    1.041297171298943,\n",
       "    0.01208401411714643,\n",
       "    0.01229277849613307,\n",
       "    1.2125911005689727,\n",
       "    0.008164428650673072,\n",
       "    0.004557914967250473,\n",
       "    0.8627006298930527,\n",
       "    0.008376901674748364,\n",
       "    0.005312144999361535,\n",
       "    0.008266970072699983,\n",
       "    0.8194656848806358,\n",
       "    0.01008595679805476]},\n",
       "  (((0, 4), (6, 0)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.3074015792871528), 'std_influence': np.float64(0.47053024802621557), 'median_influence': np.float64(0.013054279028906737), 'valid_pairs': 30, 'raw_influences': [1.0976313038086005,\n",
       "    0.012277581028543854,\n",
       "    1.1495393126349327,\n",
       "    0.008026500541090314,\n",
       "    0.8232041005789414,\n",
       "    0.01170966690002442,\n",
       "    0.017325563028967393,\n",
       "    0.008680200347664382,\n",
       "    0.002353297466963612,\n",
       "    0.011616100578660629,\n",
       "    0.8533493425223286,\n",
       "    1.0624140377432367,\n",
       "    0.010248605431365196,\n",
       "    0.015063759897053509,\n",
       "    0.005905421184272514,\n",
       "    0.030377545617036938,\n",
       "    0.012541967802457359,\n",
       "    0.012881171138836916,\n",
       "    1.4939078162786705,\n",
       "    0.013227386918976557,\n",
       "    0.01594890211168205,\n",
       "    0.9888111929087748,\n",
       "    0.008763183824337556,\n",
       "    0.005485075588983226,\n",
       "    0.5827262813540087,\n",
       "    0.007846043081971274,\n",
       "    0.006320312927420578,\n",
       "    0.010013420929637126,\n",
       "    0.9292437771846348,\n",
       "    0.014608507254509857]},\n",
       "  (((0, 4), (6, 4)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.2921135047350416), 'std_influence': np.float64(0.434982708303046), 'median_influence': np.float64(0.014344804987372945), 'valid_pairs': 30, 'raw_influences': [0.8259486739997087,\n",
       "    0.014198744265440353,\n",
       "    1.0194339046298688,\n",
       "    0.009311867392513109,\n",
       "    1.0536576794419787,\n",
       "    0.009670263436572455,\n",
       "    0.01893257440010771,\n",
       "    0.008961846478082638,\n",
       "    0.0033686518994540493,\n",
       "    0.01018935183098149,\n",
       "    0.868222079557124,\n",
       "    1.1507532065203787,\n",
       "    0.01414766179485844,\n",
       "    0.01661266891599052,\n",
       "    0.005802116647035049,\n",
       "    0.028995126173031887,\n",
       "    0.01384624109620922,\n",
       "    0.010635832907833183,\n",
       "    1.0437816008535337,\n",
       "    0.016902571596540922,\n",
       "    0.01571559468666872,\n",
       "    0.9770469157612618,\n",
       "    0.008953556498347998,\n",
       "    0.006128291511217498,\n",
       "    0.6262168431035117,\n",
       "    0.010464930079519635,\n",
       "    0.006365312616211922,\n",
       "    0.013102230220160304,\n",
       "    0.9415479380278002,\n",
       "    0.014490865709305537]},\n",
       "  (((0, 4), (11, 7)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.2476640016288994), 'std_influence': np.float64(0.38123867347505325), 'median_influence': np.float64(0.0066767645196616696), 'valid_pairs': 30, 'raw_influences': [0.7956072258562378,\n",
       "    0.006644646675904796,\n",
       "    0.979192870239989,\n",
       "    0.004400833252292892,\n",
       "    0.7389820170126803,\n",
       "    0.003837844825900639,\n",
       "    0.009534935831241912,\n",
       "    0.003743646954057206,\n",
       "    0.0017329922399970978,\n",
       "    0.005661274349451838,\n",
       "    0.7353108688236838,\n",
       "    0.972948978538964,\n",
       "    0.006255095833297799,\n",
       "    0.008112764943440558,\n",
       "    0.0032503953386936882,\n",
       "    0.01468276377963869,\n",
       "    0.0056230199357222834,\n",
       "    0.007971819547013106,\n",
       "    0.9526323887535795,\n",
       "    0.00842276653225247,\n",
       "    0.006075383098719909,\n",
       "    0.9976937171938708,\n",
       "    0.006243165831806483,\n",
       "    0.0023927954459595662,\n",
       "    0.44812444504533044,\n",
       "    0.004368308445774005,\n",
       "    0.0034719576992042903,\n",
       "    0.004298541211310265,\n",
       "    0.685993703267548,\n",
       "    0.006708882363418544]},\n",
       "  (((0, 4), (8, 0)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.3143869253756708), 'std_influence': np.float64(0.4812855224979459), 'median_influence': np.float64(0.009058239637597474), 'valid_pairs': 30, 'raw_influences': [1.0425491916280236,\n",
       "    0.009129434950872324,\n",
       "    1.2618457775301213,\n",
       "    0.005026664488041275,\n",
       "    0.922591341412411,\n",
       "    0.008987044324322624,\n",
       "    0.014004911137970933,\n",
       "    0.006638178765439076,\n",
       "    0.0019356635898924246,\n",
       "    0.007657782006415939,\n",
       "    0.9365834182411563,\n",
       "    1.199413646892873,\n",
       "    0.007808706466960066,\n",
       "    0.008945812523578905,\n",
       "    0.004729035224704695,\n",
       "    0.019720786827030067,\n",
       "    0.008572533427975303,\n",
       "    0.009660722336609523,\n",
       "    1.3523932448211102,\n",
       "    0.01016936781031137,\n",
       "    0.01318331074515518,\n",
       "    1.057357289747703,\n",
       "    0.008011780787030392,\n",
       "    0.0037585232461853505,\n",
       "    0.6510420563377219,\n",
       "    0.006894341239567314,\n",
       "    0.003956205836999391,\n",
       "    0.0068457808099253595,\n",
       "    0.8339995973383669,\n",
       "    0.008195610775648553]},\n",
       "  (((0, 11), (8, 5)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.31884225134835437), 'std_influence': np.float64(0.47499750411080116), 'median_influence': np.float64(0.016845747973268613), 'valid_pairs': 30, 'raw_influences': [1.037890102718519,\n",
       "    0.011128729741170391,\n",
       "    1.1022927694004254,\n",
       "    0.009202216261679282,\n",
       "    1.0306057514707612,\n",
       "    0.009127179805318315,\n",
       "    0.016869492410066116,\n",
       "    0.011299215420092521,\n",
       "    0.008470345769931336,\n",
       "    0.015915647649815564,\n",
       "    0.8267544478181826,\n",
       "    1.3374113991927656,\n",
       "    0.016822003536471106,\n",
       "    0.015319183738693239,\n",
       "    0.008044547148228439,\n",
       "    0.02153781114267859,\n",
       "    0.008325346360808318,\n",
       "    0.021556410369660755,\n",
       "    1.0405679900268894,\n",
       "    0.009484582735354501,\n",
       "    0.02050450166432797,\n",
       "    1.216694301269492,\n",
       "    0.011736089018701948,\n",
       "    0.004901367460265068,\n",
       "    0.8639100472990702,\n",
       "    0.01861502037668652,\n",
       "    0.01758986462633279,\n",
       "    0.01538397294324156,\n",
       "    0.8234384676737481,\n",
       "    0.013868735401253992]},\n",
       "  (((0, 11), (6, 0)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.3126143639116712), 'std_influence': np.float64(0.46808631052381294), 'median_influence': np.float64(0.021099383625611107), 'valid_pairs': 30, 'raw_influences': [1.0993076535627655,\n",
       "    0.016555253677290244,\n",
       "    1.149425689138626,\n",
       "    0.013766653395587854,\n",
       "    0.8229596424541756,\n",
       "    0.013617836810906161,\n",
       "    0.020852732932126927,\n",
       "    0.017735982630502578,\n",
       "    0.017386469652092954,\n",
       "    0.017906503038590028,\n",
       "    0.854337031531616,\n",
       "    1.0624396037031067,\n",
       "    0.026385541962437954,\n",
       "    0.01992730735051464,\n",
       "    0.016527002382108618,\n",
       "    0.02478129794780705,\n",
       "    0.01581550683921643,\n",
       "    0.02047260775686801,\n",
       "    1.502266857973414,\n",
       "    0.011769824023783506,\n",
       "    0.02503704201814544,\n",
       "    0.9863682496479999,\n",
       "    0.01822326693757061,\n",
       "    0.006684623451386301,\n",
       "    0.5811669398295558,\n",
       "    0.021346034319095288,\n",
       "    0.022630889417943903,\n",
       "    0.02047525978000297,\n",
       "    0.9296186986483269,\n",
       "    0.022642914536572755]},\n",
       "  (((0, 11), (6, 4)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.2970688672661233), 'std_influence': np.float64(0.43166956727502714), 'median_influence': np.float64(0.02396071615317171), 'valid_pairs': 30, 'raw_influences': [0.82634747705056,\n",
       "    0.014733084557113956,\n",
       "    1.0195540158098342,\n",
       "    0.015034148035733193,\n",
       "    1.0528288858570551,\n",
       "    0.011126108820530955,\n",
       "    0.029353063872662115,\n",
       "    0.018944607203327867,\n",
       "    0.02030063719389864,\n",
       "    0.01696086159556,\n",
       "    0.8677972324540646,\n",
       "    1.1500551966170125,\n",
       "    0.028845701575169202,\n",
       "    0.022719638520600244,\n",
       "    0.013722648865684604,\n",
       "    0.024909757162308234,\n",
       "    0.013140347201418169,\n",
       "    0.014976942406805303,\n",
       "    1.0462688061870737,\n",
       "    0.012921513273786262,\n",
       "    0.028333523596417536,\n",
       "    0.9763749558399576,\n",
       "    0.018364635447910855,\n",
       "    0.006556253724018137,\n",
       "    0.6243983578113538,\n",
       "    0.022060530392449224,\n",
       "    0.023223684844910074,\n",
       "    0.02700987902247554,\n",
       "    0.9405057755825723,\n",
       "    0.024697747461433345]},\n",
       "  (((0, 11), (11, 7)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.2508030726971389), 'std_influence': np.float64(0.37929654004067426), 'median_influence': np.float64(0.011964081053311664), 'valid_pairs': 30, 'raw_influences': [0.7949855673635168,\n",
       "    0.0062736817927991975,\n",
       "    0.9799280418133772,\n",
       "    0.0066738282703317576,\n",
       "    0.7406325606773262,\n",
       "    0.0073276723355204295,\n",
       "    0.013372289725720446,\n",
       "    0.010371684828581914,\n",
       "    0.007544204298296998,\n",
       "    0.008692125605992885,\n",
       "    0.7335891753239254,\n",
       "    0.9732476998817381,\n",
       "    0.011846890238308707,\n",
       "    0.014275426924807447,\n",
       "    0.008413942050416429,\n",
       "    0.01562723736863456,\n",
       "    0.0058980982749291255,\n",
       "    0.021436533740451395,\n",
       "    0.9504519812410793,\n",
       "    0.006942152062664879,\n",
       "    0.014506262190248167,\n",
       "    0.9983044768404342,\n",
       "    0.010957164155095678,\n",
       "    0.0034150983323468047,\n",
       "    0.44818135940795367,\n",
       "    0.012081271868314621,\n",
       "    0.01014247463085129,\n",
       "    0.009633410173025258,\n",
       "    0.6880944362203817,\n",
       "    0.011245433277096597]},\n",
       "  (((0, 11), (8, 0)),\n",
       "   'error_to_pred'): {'mean_influence': np.float64(0.3180777949873187), 'std_influence': np.float64(0.47948427563458423), 'median_influence': np.float64(0.014943436912519535), 'valid_pairs': 30, 'raw_influences': [1.0411449736409137,\n",
       "    0.01135470897759321,\n",
       "    1.2621541417668458,\n",
       "    0.009649377923494058,\n",
       "    0.9239207809761543,\n",
       "    0.009516127392895849,\n",
       "    0.014704098242475615,\n",
       "    0.01225237223718978,\n",
       "    0.01261811638020341,\n",
       "    0.012011924033415902,\n",
       "    0.9382052492328907,\n",
       "    1.2002767564974257,\n",
       "    0.01747585988107755,\n",
       "    0.015182775582563455,\n",
       "    0.011544902029973651,\n",
       "    0.01702267245590503,\n",
       "    0.011389468345735962,\n",
       "    0.022938429112894787,\n",
       "    1.3563067654345011,\n",
       "    0.007524786507100942,\n",
       "    0.019864974729550924,\n",
       "    1.0583821195681222,\n",
       "    0.012231297562322194,\n",
       "    0.004745074914340316,\n",
       "    0.6487210955999384,\n",
       "    0.016494831422310074,\n",
       "    0.013051877284924554,\n",
       "    0.013635560563746962,\n",
       "    0.8343897673289449,\n",
       "    0.01362296399411006]},\n",
       "  (((8, 5), (11, 7)),\n",
       "   'pred_to_pred'): {'mean_influence': np.float64(0.2585606839149097), 'std_influence': np.float64(0.37405668145920684), 'median_influence': np.float64(0.02663571803577037), 'valid_pairs': 30, 'raw_influences': [0.7944951576430828,\n",
       "    0.026389299081855253,\n",
       "    0.9795808470945714,\n",
       "    0.03379520168755668,\n",
       "    0.7420358334341194,\n",
       "    0.020302455936436675,\n",
       "    0.03849512190346072,\n",
       "    0.009922528896902908,\n",
       "    0.03046350747972343,\n",
       "    0.013692611886004,\n",
       "    0.7374398786668224,\n",
       "    0.9743950351646972,\n",
       "    0.015076183274434754,\n",
       "    0.024308399826545042,\n",
       "    0.017117825006326665,\n",
       "    0.01579541683620711,\n",
       "    0.007130154852028245,\n",
       "    0.04867038028654083,\n",
       "    0.9491520492570804,\n",
       "    0.007589715472760212,\n",
       "    0.015421771746139915,\n",
       "    0.9944416925176023,\n",
       "    0.020194732534353187,\n",
       "    0.0264749835453376,\n",
       "    0.4407949531831014,\n",
       "    0.011627406757272607,\n",
       "    0.030181270999989063,\n",
       "    0.017901053967560497,\n",
       "    0.6871385959825745,\n",
       "    0.02679645252620314]},\n",
       "  (((6, 0), (8, 5)),\n",
       "   'pred_to_pred'): {'mean_influence': np.float64(0.3239450414578772), 'std_influence': np.float64(0.4697331324796843), 'median_influence': np.float64(0.02860071596279354), 'valid_pairs': 30, 'raw_influences': [1.0338843478428876,\n",
       "    0.02916293581700499,\n",
       "    1.0997222625760927,\n",
       "    0.004017608206682801,\n",
       "    1.027802044673616,\n",
       "    0.020833004550223758,\n",
       "    0.017412330458405446,\n",
       "    0.03182959077172321,\n",
       "    0.02552360369093192,\n",
       "    0.015568955572184408,\n",
       "    0.8255868145994925,\n",
       "    1.3338719476059244,\n",
       "    0.016951695934376263,\n",
       "    0.012986326534417962,\n",
       "    0.016748796648174767,\n",
       "    0.04745317183291054,\n",
       "    0.02834769322752415,\n",
       "    0.01984095744162633,\n",
       "    1.0407979934859806,\n",
       "    0.028853738698062934,\n",
       "    0.041838328285994014,\n",
       "    1.2138360520417601,\n",
       "    0.007916305830330506,\n",
       "    0.008916790342600674,\n",
       "    0.8603113266200135,\n",
       "    0.014462932095233923,\n",
       "    0.04696316024355207,\n",
       "    0.015771079583321326,\n",
       "    0.8136412576741651,\n",
       "    0.017498190851100628]},\n",
       "  (((6, 0), (11, 7)),\n",
       "   'pred_to_pred'): {'mean_influence': np.float64(0.25444031276330964), 'std_influence': np.float64(0.37767698228629415), 'median_influence': np.float64(0.017641551405009094), 'valid_pairs': 30, 'raw_influences': [0.7972032865827413,\n",
       "    0.016233286876913108,\n",
       "    0.9818600219197673,\n",
       "    0.003982687021999533,\n",
       "    0.7413573916467442,\n",
       "    0.011372812841810858,\n",
       "    0.010872448822196549,\n",
       "    0.018744751367712767,\n",
       "    0.020748781989909055,\n",
       "    0.007978890944721585,\n",
       "    0.7343051945801834,\n",
       "    0.9734254895891282,\n",
       "    0.013169304994640427,\n",
       "    0.016538351442305425,\n",
       "    0.013144966882328357,\n",
       "    0.03383224883971989,\n",
       "    0.013226883643756366,\n",
       "    0.012649412007130515,\n",
       "    0.9546373368239413,\n",
       "    0.0235659817990709,\n",
       "    0.022381039362458246,\n",
       "    0.997686554143923,\n",
       "    0.01364828523312903,\n",
       "    0.005988681035658086,\n",
       "    0.44813230842245055,\n",
       "    0.00812339103324747,\n",
       "    0.027404453894344594,\n",
       "    0.010250957712659544,\n",
       "    0.6870559609531027,\n",
       "    0.013688220491595049]},\n",
       "  (((6, 0), (8, 0)),\n",
       "   'pred_to_pred'): {'mean_influence': np.float64(0.32343297646034375), 'std_influence': np.float64(0.47684048469645934), 'median_influence': np.float64(0.02341418927713133), 'valid_pairs': 30, 'raw_influences': [1.044738804610329,\n",
       "    0.023962511706666374,\n",
       "    1.2670292375803247,\n",
       "    0.0031376372016415034,\n",
       "    0.9288005378459299,\n",
       "    0.019669908094755197,\n",
       "    0.013587003098120203,\n",
       "    0.0259528156343262,\n",
       "    0.026665542939913536,\n",
       "    0.015582990646041413,\n",
       "    0.9355669947213614,\n",
       "    1.198130665527444,\n",
       "    0.01700466437783425,\n",
       "    0.013259847590580597,\n",
       "    0.020163143765755263,\n",
       "    0.049407515570680086,\n",
       "    0.022865866847596287,\n",
       "    0.016095300430761555,\n",
       "    1.3565766544850013,\n",
       "    0.022806378281856034,\n",
       "    0.03820779922631578,\n",
       "    1.05731917947567,\n",
       "    0.00723368948192368,\n",
       "    0.008656007765701933,\n",
       "    0.6476667456400005,\n",
       "    0.010073991977245768,\n",
       "    0.04764280590941453,\n",
       "    0.013054486509069234,\n",
       "    0.836534970008428,\n",
       "    0.01559559685962198]},\n",
       "  (((6, 4), (8, 5)),\n",
       "   'pred_to_pred'): {'mean_influence': np.float64(0.3217650962070133), 'std_influence': np.float64(0.4716986540393009), 'median_influence': np.float64(0.022157951839091306), 'valid_pairs': 30, 'raw_influences': [1.0357298775976518,\n",
       "    0.012057010199576413,\n",
       "    1.1031760815927758,\n",
       "    0.022037909977564007,\n",
       "    1.0297853505568744,\n",
       "    0.01216731312248877,\n",
       "    0.04004996669259292,\n",
       "    0.01980037577625082,\n",
       "    0.014013501665584408,\n",
       "    0.005838122247845045,\n",
       "    0.824245729662834,\n",
       "    1.3341099532749257,\n",
       "    0.010612439203369494,\n",
       "    0.01775400860603668,\n",
       "    0.03592591974727852,\n",
       "    0.018189657183094836,\n",
       "    0.02235738027345745,\n",
       "    0.020533786365656084,\n",
       "    1.0387667314961715,\n",
       "    0.022791286838101894,\n",
       "    0.015972238926771826,\n",
       "    1.2135255511848353,\n",
       "    0.006964085186445266,\n",
       "    0.02300575394351296,\n",
       "    0.8606163635724982,\n",
       "    0.017556928275825612,\n",
       "    0.0222779937006186,\n",
       "    0.013287822614039973,\n",
       "    0.8188151753726292,\n",
       "    0.02098857135309383]},\n",
       "  (((6, 4), (11, 7)),\n",
       "   'pred_to_pred'): {'mean_influence': np.float64(0.2529976809317989), 'std_influence': np.float64(0.3787608912226863), 'median_influence': np.float64(0.015602536023760301), 'valid_pairs': 30, 'raw_influences': [0.7944642874995238,\n",
       "    0.009069465041357003,\n",
       "    0.9792497970563202,\n",
       "    0.018310359058884722,\n",
       "    0.7424773590417763,\n",
       "    0.007600513855144245,\n",
       "    0.022634604855153395,\n",
       "    0.015429648224952883,\n",
       "    0.013101282427914862,\n",
       "    0.0037953760598220137,\n",
       "    0.7362657251696407,\n",
       "    0.9714667723014604,\n",
       "    0.009047801524088907,\n",
       "    0.012602743592910994,\n",
       "    0.020349295602260387,\n",
       "    0.014703644102709069,\n",
       "    0.010974747384548957,\n",
       "    0.01577542382256772,\n",
       "    0.9587600224992303,\n",
       "    0.015839795034640926,\n",
       "    0.009235361061467036,\n",
       "    1.001075334600316,\n",
       "    0.008943693377458614,\n",
       "    0.021048033377362642,\n",
       "    0.4472886758618293,\n",
       "    0.009280225186044901,\n",
       "    0.011640704881976303,\n",
       "    0.010038437845466248,\n",
       "    0.6865700187050494,\n",
       "    0.012891278902089671]},\n",
       "  (((6, 4), (8, 0)),\n",
       "   'pred_to_pred'): {'mean_influence': np.float64(0.3210839569737288), 'std_influence': np.float64(0.4775704091046059), 'median_influence': np.float64(0.019078220375491495), 'valid_pairs': 30, 'raw_influences': [1.0396950683487416,\n",
       "    0.012781534012134095,\n",
       "    1.26533994528266,\n",
       "    0.020555532452971145,\n",
       "    0.9248220180374017,\n",
       "    0.012585797483037288,\n",
       "    0.03368414892121173,\n",
       "    0.01912500928585289,\n",
       "    0.016641396418948812,\n",
       "    0.005065800741038509,\n",
       "    0.9351243751597745,\n",
       "    1.195495880511754,\n",
       "    0.012988215651626209,\n",
       "    0.0169327038862422,\n",
       "    0.03536612480803017,\n",
       "    0.017040342898771803,\n",
       "    0.016996387826845292,\n",
       "    0.019031431465130103,\n",
       "    1.360081158199232,\n",
       "    0.02359443138254978,\n",
       "    0.015472456674174715,\n",
       "    1.0581669178266628,\n",
       "    0.006735787374760448,\n",
       "    0.022401876344225524,\n",
       "    0.6461622157039913,\n",
       "    0.016732459596409212,\n",
       "    0.016255997513299644,\n",
       "    0.012895747078183294,\n",
       "    0.8362920291001856,\n",
       "    0.01845591922601793]},\n",
       "  (((8, 0), (11, 7)),\n",
       "   'pred_to_pred'): {'mean_influence': np.float64(0.2565357436130988), 'std_influence': np.float64(0.376682550096619), 'median_influence': np.float64(0.02221989212959207), 'valid_pairs': 30, 'raw_influences': [0.7919630628262136,\n",
       "    0.012723043726770258,\n",
       "    0.9780552588690214,\n",
       "    0.027668774546965515,\n",
       "    0.7426073453956251,\n",
       "    0.0172174640322759,\n",
       "    0.016139905700355155,\n",
       "    0.0076163289210300685,\n",
       "    0.02223386501634337,\n",
       "    0.01649300070442181,\n",
       "    0.7357725174420309,\n",
       "    0.9757269427169355,\n",
       "    0.009284935421113218,\n",
       "    0.013243179057501348,\n",
       "    0.008225085203055128,\n",
       "    0.028400425217368606,\n",
       "    0.013969301677432901,\n",
       "    0.03225035006318333,\n",
       "    0.9558072837590731,\n",
       "    0.007626047956909557,\n",
       "    0.007568350330168918,\n",
       "    1.0014517601843038,\n",
       "    0.02772336683662108,\n",
       "    0.022205919242840776,\n",
       "    0.45042292920183047,\n",
       "    0.010078271586699033,\n",
       "    0.020421295407136646,\n",
       "    0.010538808167890526,\n",
       "    0.6887493401266332,\n",
       "    0.04388814905521301]}},\n",
       " 'config': {'num_heads_individual': 14,\n",
       "  'num_heads_synergy': 5,\n",
       "  'num_heads_causal': 5,\n",
       "  'num_sentences_individual': 80,\n",
       "  'num_sentences_synergy': 50,\n",
       "  'num_sentences_group': 70,\n",
       "  'num_sentences_causal': 30,\n",
       "  'group_sizes': [3, 5],\n",
       "  'cross_validation': 3,\n",
       "  'min_valid_pairs': 5}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pairs = pairs[\"case\"][200:300]\n",
    "top_gender_predictive_heads = [(8,5),(6,0),(6,4),(11,7),(8,0),(6,9),(6,1),(6,6),(6,10),(10,7)] ## from the heads collected in Gender_Encoding.ipynb\n",
    "top_error_sensitive_heads = [(0,8),(0,0),(0,3),(0,4),(0,11),(0,1),(0,7),(1,0),(0,2),(4,4)]\n",
    "\n",
    "run_efficient_synergy_analysis(\n",
    "     model, tokenizer, test_pairs,\n",
    "     top_error_sensitive_heads[:7], top_gender_predictive_heads[:7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5pC9gP5WgwR",
    "outputId": "ef3bfb47-5eb9-48bb-9514-45a547b5d71b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 753 sentences\n"
     ]
    }
   ],
   "source": [
    "def extract_ud_test_sentences(ud_file_path):\n",
    "    \"\"\"\n",
    "    Extracting German test sentences from UD dataset to create a dataset to just test logit distribution after ablation\n",
    "\n",
    "    Args:\n",
    "        ud_file_path: Path to UD German file (e.g., 'de_gsd-ud-test.conllu')\n",
    "\n",
    "    Returns:\n",
    "        List of (sentence, article, gender) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    # German articles and their genders\n",
    "    article_genders = {\n",
    "        'das': 'Neut', 'der': 'Masc', 'die': 'Fem',\n",
    "        'den': 'Masc', 'dem': 'Masc', 'des': 'Masc'\n",
    "    }\n",
    "\n",
    "    test_sentences = []\n",
    "    current_sentence = []\n",
    "    current_words = []\n",
    "\n",
    "    try:\n",
    "        with open(ud_file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "\n",
    "                # End of sentence\n",
    "                if not line or line.startswith('#'):\n",
    "                    if current_sentence and current_words:\n",
    "\n",
    "                        sentence_text = ' '.join(current_words)\n",
    "\n",
    "                        # Finding articles in this sentence\n",
    "                        for token in current_sentence:\n",
    "                            word_original = token['form']  # Keeping original capitalization\n",
    "                            word_lower = word_original.lower()\n",
    "                            upos = token['upos']\n",
    "\n",
    "                            if upos == 'DET' and word_lower in article_genders:\n",
    "                                gender = article_genders[word_lower]\n",
    "                                test_sentences.append((sentence_text, word_original, gender))\n",
    "                                break  # Only taking the first article per sentence\n",
    "\n",
    "                    current_sentence = []\n",
    "                    current_words = []\n",
    "                    continue\n",
    "\n",
    "                # Parsing token line\n",
    "                if '\\t' in line:\n",
    "                    fields = line.split('\\t')\n",
    "                    if len(fields) >= 4 and '-' not in fields[0]:  # Skip multiword tokens\n",
    "                        token_info = {\n",
    "                            'form': fields[1],\n",
    "                            'upos': fields[3]\n",
    "                        }\n",
    "                        current_sentence.append(token_info)\n",
    "                        current_words.append(fields[1])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {ud_file_path}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Extracted {len(test_sentences)} sentences\")\n",
    "    return test_sentences\n",
    "\n",
    "test_sentences = extract_ud_test_sentences('UD_German-GSD/de_gsd-ud-test.conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIxx6GXiWgwS",
    "outputId": "64458917-105c-415c-e146-96bdf6775ab1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Der Hauptgang war in Ordnung , aber alles andere als umwerfend .',\n",
       "  'Der',\n",
       "  'Masc'),\n",
       " ('Ich habe dort 2007 meinen OWD gemacht und weil mir das Tauchen so gefiel hab ich dort noch in dem selben Jahr den AOWD und den Deep drangehängt .',\n",
       "  'das',\n",
       "  'Neut')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hbSCkLPWgwS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def compute_kl_divergence(orig_probs, ablated_probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Computing KL divergence: KL(orig || ablated)\n",
    "    \"\"\"\n",
    "    ablated_probs_safe = ablated_probs + epsilon #adding epsilon to prevent log 0\n",
    "    kl_div = torch.sum(orig_probs * torch.log(orig_probs / ablated_probs_safe))\n",
    "    return kl_div.item()\n",
    "\n",
    "def compute_js_divergence(orig_probs, ablated_probs, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Computing Jensen-Shannon divergence (symmetric version of KL)\n",
    "    \"\"\"\n",
    "    orig_probs_safe = orig_probs + epsilon\n",
    "    ablated_probs_safe = ablated_probs + epsilon\n",
    "\n",
    "    # Average distribution\n",
    "    m = 0.5 * (orig_probs_safe + ablated_probs_safe)\n",
    "\n",
    "    # JS divergence = 0.5 * KL(P || M) + 0.5 * KL(Q || M)\n",
    "    kl1 = torch.sum(orig_probs_safe * torch.log(orig_probs_safe / m))\n",
    "    kl2 = torch.sum(ablated_probs_safe * torch.log(ablated_probs_safe / m))\n",
    "\n",
    "    js_div = 0.5 * kl1 + 0.5 * kl2\n",
    "    return js_div.item()\n",
    "\n",
    "def compute_distribution_distances(orig_logits, ablated_logits):\n",
    "    \"\"\"\n",
    "    Computing the various distance metrics between original and ablated model outputs\n",
    "    \"\"\"\n",
    "    # Convert to probabilities\n",
    "    orig_probs = F.softmax(orig_logits, dim=0)\n",
    "    ablated_probs = F.softmax(ablated_logits, dim=0)\n",
    "\n",
    "    # Distance metrics\n",
    "    results = {}\n",
    "\n",
    "    # 1. KL divergence (asymmetric)\n",
    "    results['kl_orig_to_ablated'] = compute_kl_divergence(orig_probs, ablated_probs)\n",
    "    results['kl_ablated_to_orig'] = compute_kl_divergence(ablated_probs, orig_probs)\n",
    "\n",
    "    # 2. Jensen-Shannon divergence (symmetric)\n",
    "    results['js_divergence'] = compute_js_divergence(orig_probs, ablated_probs)\n",
    "\n",
    "    # 3. L2 distance on logits (unnormalized scores)\n",
    "    results['l2_logits'] = torch.norm(orig_logits - ablated_logits, p=2).item()\n",
    "\n",
    "    # 4. L2 distance on probabilities\n",
    "    results['l2_probs'] = torch.norm(orig_probs - ablated_probs, p=2).item()\n",
    "\n",
    "    # 5. Cosine similarity on logits\n",
    "    cos_sim = F.cosine_similarity(orig_logits.unsqueeze(0), ablated_logits.unsqueeze(0))\n",
    "    results['cosine_similarity'] = cos_sim.item()\n",
    "    results['cosine_distance'] = 1 - cos_sim.item()\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_masked_logits_full(model, tokenizer, masked_sentence):\n",
    "    \"\"\"\n",
    "    Get full logits for the masked token (entire vocabulary)\n",
    "    \"\"\"\n",
    "    # Tokenizing masked sentence\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Finding [MASK] position\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    mask_positions = (inputs.input_ids == mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "    if len(mask_positions) == 0:\n",
    "        raise ValueError(f\"No [MASK] token found in: {masked_sentence}\")\n",
    "\n",
    "    mask_pos = mask_positions[0].item()\n",
    "\n",
    "    # Getting logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, mask_pos]  # Full vocabulary logits\n",
    "\n",
    "    return logits\n",
    "\n",
    "def create_masked_sentence(tokenizer, sentence, article_position):\n",
    "    \"\"\"Replacing article with [MASK] token\"\"\"\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Simple approach: replace the article word with [MASK] by finding which word contains the article\n",
    "    tokens_with_positions = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "\n",
    "    if hasattr(tokens_with_positions, 'offset_mapping'):\n",
    "        offset_map = tokens_with_positions.offset_mapping[0]\n",
    "        if article_position < len(offset_map):\n",
    "            start_char, end_char = offset_map[article_position]\n",
    "\n",
    "            # Replacing the character span with [MASK]\n",
    "            masked_sentence = sentence[:start_char] + \"[MASK]\" + sentence[end_char:]\n",
    "            return masked_sentence\n",
    "\n",
    "    words = sentence.split()\n",
    "    article_word = get_article_at_position(tokenizer, sentence, article_position)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if article_word in word:\n",
    "            words[i] = \"[MASK]\"\n",
    "            break\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def find_article_in_sentence(tokenizer, sentence, target_article):\n",
    "    \"\"\"Find position of specific article in sentence\"\"\"\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    token_ids = tokens.input_ids[0]\n",
    "\n",
    "    target_id = tokenizer.convert_tokens_to_ids(target_article)\n",
    "\n",
    "    for i, token_id in enumerate(token_ids):\n",
    "        if token_id == target_id:\n",
    "            return i\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_article_at_position(tokenizer, sentence, position):\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    token_ids = tokens.input_ids[0]\n",
    "\n",
    "    if position < len(token_ids):\n",
    "        token_id = token_ids[position]\n",
    "        return tokenizer.decode([token_id])\n",
    "\n",
    "    return None\n",
    "\n",
    "def masked_article_test_with_distances(model, tokenizer, test_sentences):\n",
    "    \"\"\"\n",
    "    Test using KL-divergence and L2 distances instead of accuracy\n",
    "\n",
    "    Args:\n",
    "        test_sentences: List of (sentence, correct_article, gender) tuples\n",
    "\n",
    "    Returns:\n",
    "        List of results with distance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"MASKED ARTICLE TEST - Distance-based Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = []\n",
    "    total_tests = 0\n",
    "\n",
    "    for i, (sentence, correct_article, gender) in enumerate(test_sentences):\n",
    "\n",
    "        article_pos = find_article_in_sentence(tokenizer, sentence, correct_article)\n",
    "\n",
    "        if article_pos is None:\n",
    "            print(f\"Sentence {i+1}: Could not find article '{correct_article}'\")\n",
    "            continue\n",
    "\n",
    "        masked_sentence = create_masked_sentence(tokenizer, sentence, article_pos)\n",
    "\n",
    "        try:\n",
    "            # Getting original model logits\n",
    "            orig_logits = get_masked_logits_full(model, tokenizer, masked_sentence)\n",
    "\n",
    "            # Storing baseline result\n",
    "            results.append({\n",
    "                'sentence_id': i + 1,\n",
    "                'gender': gender,\n",
    "                'original_sentence': sentence,\n",
    "                'masked_sentence': masked_sentence,\n",
    "                'correct_article': correct_article,\n",
    "                'orig_logits': orig_logits.clone(),\n",
    "                'is_baseline': True\n",
    "            })\n",
    "\n",
    "            total_tests += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence {i+1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully processed {total_tests} sentences for baseline\")\n",
    "    return results\n",
    "\n",
    "def simple_ablation_test_distances(model, tokenizer, test_sentences, heads_to_ablate):\n",
    "    \"\"\"\n",
    "    Ablation test using KL-divergence and L2 distances\n",
    "\n",
    "    Args:\n",
    "        model: mBERT model\n",
    "        tokenizer: mBERT tokenizer\n",
    "        test_sentences: List of (sentence, correct_article, gender) tuples\n",
    "        heads_to_ablate: List of (layer, head) tuples to ablate\n",
    "\n",
    "    Returns:\n",
    "        Dict with baseline and ablated results plus distance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"ABLATION TEST WITH DISTANCE METRICS\")\n",
    "    print(f\"Ablating heads: {heads_to_ablate}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"Getting baseline logits (no ablation)...\")\n",
    "    baseline_results = masked_article_test_with_distances(model, tokenizer, test_sentences)\n",
    "\n",
    "    if not baseline_results:\n",
    "        print(\"No baseline results obtained!\")\n",
    "        return None\n",
    "\n",
    "    # Applying ablation\n",
    "    print(f\"\\nApplying ablation to heads: {heads_to_ablate}\")\n",
    "    original_weights = ablate_heads(model, heads_to_ablate)\n",
    "\n",
    "    try:\n",
    "        # Getting ablated logits\n",
    "        print(\"Getting ablated logits...\")\n",
    "        distance_results = []\n",
    "\n",
    "        for baseline_result in baseline_results:\n",
    "            sentence_id = baseline_result['sentence_id']\n",
    "            masked_sentence = baseline_result['masked_sentence']\n",
    "            orig_logits = baseline_result['orig_logits']\n",
    "\n",
    "            try:\n",
    "                ablated_logits = get_masked_logits_full(model, tokenizer, masked_sentence)\n",
    "\n",
    "                distances = compute_distribution_distances(orig_logits, ablated_logits)\n",
    "\n",
    "                # Storing result\n",
    "                result = {\n",
    "                    **baseline_result,  # Include all baseline info\n",
    "                    'ablated_logits': ablated_logits.clone(),\n",
    "                    **distances,  # Include all distance metrics\n",
    "                    'is_baseline': False\n",
    "                }\n",
    "                distance_results.append(result)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing ablated sentence {sentence_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if distance_results:\n",
    "            analyze_distance_results(distance_results, heads_to_ablate)\n",
    "\n",
    "        results = {\n",
    "            'heads_ablated': heads_to_ablate,\n",
    "            'baseline_results': baseline_results,\n",
    "            'distance_results': distance_results,\n",
    "            'n_sentences': len(distance_results)\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        restore_heads(model, original_weights)\n",
    "        print(f\"\\nRestored original model weights\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def analyze_distance_results(results, heads_ablated):\n",
    "    \"\"\"\n",
    "    Analyze and summarize the distance-based ablation results\n",
    "    \"\"\"\n",
    "    print(f\"\\nDISTANCE-BASED ABLATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Extract distance metrics\n",
    "    kl_orig_to_ablated = [r['kl_orig_to_ablated'] for r in results]\n",
    "    kl_ablated_to_orig = [r['kl_ablated_to_orig'] for r in results]\n",
    "    js_divergences = [r['js_divergence'] for r in results]\n",
    "    l2_logits = [r['l2_logits'] for r in results]\n",
    "    l2_probs = [r['l2_probs'] for r in results]\n",
    "    cosine_distances = [r['cosine_distance'] for r in results]\n",
    "\n",
    "    metrics = {\n",
    "        'KL(orig||ablated)': kl_orig_to_ablated,\n",
    "        'KL(ablated||orig)': kl_ablated_to_orig,\n",
    "        'JS Divergence': js_divergences,\n",
    "        'L2 Logits': l2_logits,\n",
    "        'L2 Probs': l2_probs,\n",
    "        'Cosine Distance': cosine_distances\n",
    "    }\n",
    "\n",
    "    print(f\"Ablated heads: {heads_ablated}\")\n",
    "    print(f\"Number of sentences: {len(results)}\")\n",
    "    print()\n",
    "\n",
    "    for metric_name, values in metrics.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        max_val = np.max(values)\n",
    "        min_val = np.min(values)\n",
    "\n",
    "        print(f\"{metric_name}:\")\n",
    "        print(f\"  Mean: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "        print(f\"  Range: [{min_val:.4f}, {max_val:.4f}]\")\n",
    "\n",
    "    mean_js = np.mean(js_divergences)\n",
    "    mean_l2_logits = np.mean(l2_logits)\n",
    "\n",
    "    print(f\"\\nOVERALL IMPACT ASSESSMENT:\")\n",
    "\n",
    "    if mean_js > 0.5:\n",
    "        js_impact = \"MASSIVE\"\n",
    "    elif mean_js > 0.2:\n",
    "        js_impact = \"STRONG\"\n",
    "    elif mean_js > 0.05:\n",
    "        js_impact = \"MODERATE\"\n",
    "    else:\n",
    "        js_impact = \"MINIMAL\"\n",
    "\n",
    "    if mean_l2_logits > 50:\n",
    "        l2_impact = \"MASSIVE\"\n",
    "    elif mean_l2_logits > 20:\n",
    "        l2_impact = \"STRONG\"\n",
    "    elif mean_l2_logits > 5:\n",
    "        l2_impact = \"MODERATE\"\n",
    "    else:\n",
    "        l2_impact = \"MINIMAL\"\n",
    "\n",
    "    print(f\"  JS Divergence Impact: {js_impact} (mean = {mean_js:.4f})\")\n",
    "    print(f\"  L2 Logits Impact: {l2_impact} (mean = {mean_l2_logits:.4f})\")\n",
    "\n",
    "    print(f\"\\nMOST AFFECTED SENTENCES (by JS Divergence):\")\n",
    "    sorted_results = sorted(results, key=lambda x: x['js_divergence'], reverse=True)[:3]\n",
    "\n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        print(f\"{i}. Sentence {result['sentence_id']} (JS = {result['js_divergence']:.4f}):\")\n",
    "        print(f\"   Original: {result['original_sentence']}\")\n",
    "        print(f\"   Gender: {result['gender']}, Target: {result['correct_article']}\")\n",
    "\n",
    "def ablate_heads(model, heads_to_ablate):\n",
    "    \"\"\"Zero out heads with validation and confirmation\"\"\"\n",
    "\n",
    "    original_weights = {}\n",
    "\n",
    "    for layer_idx, head_idx in heads_to_ablate:\n",
    "        try:\n",
    "            if hasattr(model, 'bert'):\n",
    "                attention_layer = model.bert.encoder.layer[layer_idx].attention\n",
    "            else:\n",
    "                attention_layer = model.encoder.layer[layer_idx].attention\n",
    "\n",
    "            self_attn = attention_layer.self\n",
    "            output_attn = attention_layer.output\n",
    "\n",
    "            head_dim = self_attn.attention_head_size\n",
    "            start_idx = head_idx * head_dim\n",
    "            end_idx = (head_idx + 1) * head_dim\n",
    "\n",
    "            if end_idx > self_attn.query.weight.data.shape[1]:\n",
    "                print(f\"ERROR: Head ({layer_idx}, {head_idx}) out of range!\")\n",
    "                continue\n",
    "\n",
    "            key = (layer_idx, head_idx)\n",
    "            original_weights[key] = {\n",
    "                'query': self_attn.query.weight.data[:, start_idx:end_idx].clone(),\n",
    "                'key': self_attn.key.weight.data[:, start_idx:end_idx].clone(),\n",
    "                'value': self_attn.value.weight.data[:, start_idx:end_idx].clone(),\n",
    "                'output_dense': output_attn.dense.weight.data[:, start_idx:end_idx].clone()\n",
    "            }\n",
    "\n",
    "            # Verify we have non-zero weights before ablation\n",
    "            orig_norm = torch.norm(original_weights[key]['query']).item()\n",
    "            print(f\"   Head {layer_idx}.{head_idx} original norm: {orig_norm:.4f}\")\n",
    "\n",
    "            # Zeroing out this head's weights\n",
    "            self_attn.query.weight.data[:, start_idx:end_idx] = 0\n",
    "            self_attn.key.weight.data[:, start_idx:end_idx] = 0\n",
    "            self_attn.value.weight.data[:, start_idx:end_idx] = 0\n",
    "            output_attn.dense.weight.data[:, start_idx:end_idx] = 0\n",
    "\n",
    "            new_norm = torch.norm(self_attn.query.weight.data[:, start_idx:end_idx]).item()\n",
    "            print(f\"   Head {layer_idx}.{head_idx} after ablation...: {new_norm:.4f}\")\n",
    "\n",
    "            if new_norm > 1e-6:\n",
    "                print(f\"   WARNING: Ablation may have failed for head ({layer_idx}, {head_idx})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR ablating head ({layer_idx}, {head_idx}): {e}\")\n",
    "            continue\n",
    "\n",
    "    return original_weights\n",
    "def restore_heads(model, original_weights):\n",
    "    \"\"\"To restore original attention head weights (query/key/value/output)\"\"\"\n",
    "\n",
    "    for (layer_idx, head_idx), weights in original_weights.items():\n",
    "        if hasattr(model, 'bert'):\n",
    "            attention_layer = model.bert.encoder.layer[layer_idx].attention\n",
    "        else:\n",
    "            attention_layer = model.encoder.layer[layer_idx].attention\n",
    "\n",
    "        self_attn = attention_layer.self\n",
    "        output_attn = attention_layer.output\n",
    "\n",
    "        head_dim = self_attn.attention_head_size\n",
    "        start_idx = head_idx * head_dim\n",
    "        end_idx = (head_idx + 1) * head_dim\n",
    "\n",
    "        self_attn.query.weight.data[:, start_idx:end_idx] = weights['query']\n",
    "        self_attn.key.weight.data[:, start_idx:end_idx] = weights['key']\n",
    "        self_attn.value.weight.data[:, start_idx:end_idx] = weights['value']\n",
    "\n",
    "        output_attn.dense.weight.data[:, start_idx:end_idx] = weights['output_dense']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tVMertYWgwS",
    "outputId": "dadd0b20-5cd5-4acb-a651-fb908d333605"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABLATION TEST WITH DISTANCE METRICS\n",
      "Ablating heads: [(8, 5), (6, 0), (6, 4), (11, 7), (8, 0), (6, 9), (6, 1), (6, 6), (6, 10), (10, 7)]\n",
      "============================================================\n",
      "Getting baseline logits (no ablation)...\n",
      "MASKED ARTICLE TEST - Distance-based Analysis\n",
      "============================================================\n",
      "Sentence 609: Could not find article 'Einem'\n",
      "Successfully processed 752 sentences for baseline\n",
      "\n",
      "Applying ablation to heads: [(8, 5), (6, 0), (6, 4), (11, 7), (8, 0), (6, 9), (6, 1), (6, 6), (6, 10), (10, 7)]\n",
      "   Head 8.5 original norm: 9.9065\n",
      "   Head 8.5 after ablation...: 0.0000\n",
      "   Head 6.0 original norm: 10.5033\n",
      "   Head 6.0 after ablation...: 0.0000\n",
      "   Head 6.4 original norm: 10.5122\n",
      "   Head 6.4 after ablation...: 0.0000\n",
      "   Head 11.7 original norm: 10.1050\n",
      "   Head 11.7 after ablation...: 0.0000\n",
      "   Head 8.0 original norm: 9.8228\n",
      "   Head 8.0 after ablation...: 0.0000\n",
      "   Head 6.9 original norm: 10.5352\n",
      "   Head 6.9 after ablation...: 0.0000\n",
      "   Head 6.1 original norm: 10.5665\n",
      "   Head 6.1 after ablation...: 0.0000\n",
      "   Head 6.6 original norm: 10.7673\n",
      "   Head 6.6 after ablation...: 0.0000\n",
      "   Head 6.10 original norm: 10.4291\n",
      "   Head 6.10 after ablation...: 0.0000\n",
      "   Head 10.7 original norm: 10.1807\n",
      "   Head 10.7 after ablation...: 0.0000\n",
      "Getting ablated logits...\n",
      "\n",
      "DISTANCE-BASED ABLATION ANALYSIS\n",
      "==================================================\n",
      "Ablated heads: [(8, 5), (6, 0), (6, 4), (11, 7), (8, 0), (6, 9), (6, 1), (6, 6), (6, 10), (10, 7)]\n",
      "Number of sentences: 752\n",
      "\n",
      "KL(orig||ablated):\n",
      "  Mean: 0.2400 ± 0.2750\n",
      "  Range: [0.0016, 2.8389]\n",
      "KL(ablated||orig):\n",
      "  Mean: 0.3037 ± 0.3327\n",
      "  Range: [0.0016, 3.4655]\n",
      "JS Divergence:\n",
      "  Mean: 0.0580 ± 0.0584\n",
      "  Range: [0.0004, 0.5266]\n",
      "L2 Logits:\n",
      "  Mean: 356.5845 ± 63.7670\n",
      "  Range: [230.3590, 800.0422]\n",
      "L2 Probs:\n",
      "  Mean: 0.1884 ± 0.1462\n",
      "  Range: [0.0030, 0.8438]\n",
      "Cosine Distance:\n",
      "  Mean: 0.0103 ± 0.0036\n",
      "  Range: [0.0041, 0.0450]\n",
      "\n",
      "OVERALL IMPACT ASSESSMENT:\n",
      "  JS Divergence Impact: MODERATE (mean = 0.0580)\n",
      "  L2 Logits Impact: MASSIVE (mean = 356.5845)\n",
      "\n",
      "MOST AFFECTED SENTENCES (by JS Divergence):\n",
      "1. Sentence 717 (JS = 0.5266):\n",
      "   Original: Freilich können Symbole sich auch verschleißen , ebenso wie die bloße Konfrontation auf Dauer nicht ausreicht ;\n",
      "   Gender: Fem, Target: die\n",
      "2. Sentence 503 (JS = 0.4219):\n",
      "   Original: `` Wie kann man nur jahrelang so pennen und immer die Wohnungen vergessen '' , ärgert sich Pahlke .\n",
      "   Gender: Fem, Target: die\n",
      "3. Sentence 147 (JS = 0.3329):\n",
      "   Original: In dem Anschluss gibt es Kaffee und Kuchen und die Möglichkeit zu einem Gespräch .\n",
      "   Gender: Masc, Target: dem\n",
      "\n",
      "Restored original model weights\n"
     ]
    }
   ],
   "source": [
    "## For Gender-Encoding heads\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "heads_to_test = [(8,5),(6,0),(6,4),(11,7),(8,0),(6,9),(6,1),(6,6),(6,10),(10,7)]\n",
    "\n",
    "results = simple_ablation_test_distances(model, tokenizer, test_sentences, heads_to_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6wBUaXyWgwS",
    "outputId": "3d7a8277-2c00-4bf7-f729-eeb53680d5ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABLATION TEST WITH DISTANCE METRICS\n",
      "Ablating heads: [(0, 8), (0, 0), (0, 3), (0, 4), (0, 11), (0, 1), (0, 7), (1, 0), (0, 2), (4, 4)]\n",
      "============================================================\n",
      "Getting baseline logits (no ablation)...\n",
      "MASKED ARTICLE TEST - Distance-based Analysis\n",
      "============================================================\n",
      "Sentence 609: Could not find article 'Einem'\n",
      "Successfully processed 752 sentences for baseline\n",
      "\n",
      "Applying ablation to heads: [(0, 8), (0, 0), (0, 3), (0, 4), (0, 11), (0, 1), (0, 7), (1, 0), (0, 2), (4, 4)]\n",
      "   Head 0.8 original norm: 9.2777\n",
      "   Head 0.8 after ablation...: 0.0000\n",
      "   Head 0.0 original norm: 9.1679\n",
      "   Head 0.0 after ablation...: 0.0000\n",
      "   Head 0.3 original norm: 8.9877\n",
      "   Head 0.3 after ablation...: 0.0000\n",
      "   Head 0.4 original norm: 9.0872\n",
      "   Head 0.4 after ablation...: 0.0000\n",
      "   Head 0.11 original norm: 9.0908\n",
      "   Head 0.11 after ablation...: 0.0000\n",
      "   Head 0.1 original norm: 9.1184\n",
      "   Head 0.1 after ablation...: 0.0000\n",
      "   Head 0.7 original norm: 9.1842\n",
      "   Head 0.7 after ablation...: 0.0000\n",
      "   Head 1.0 original norm: 9.4222\n",
      "   Head 1.0 after ablation...: 0.0000\n",
      "   Head 0.2 original norm: 8.6442\n",
      "   Head 0.2 after ablation...: 0.0000\n",
      "   Head 4.4 original norm: 9.6944\n",
      "   Head 4.4 after ablation...: 0.0000\n",
      "Getting ablated logits...\n",
      "\n",
      "DISTANCE-BASED ABLATION ANALYSIS\n",
      "==================================================\n",
      "Ablated heads: [(0, 8), (0, 0), (0, 3), (0, 4), (0, 11), (0, 1), (0, 7), (1, 0), (0, 2), (4, 4)]\n",
      "Number of sentences: 752\n",
      "\n",
      "KL(orig||ablated):\n",
      "  Mean: 0.1652 ± 0.2754\n",
      "  Range: [0.0005, 2.2450]\n",
      "KL(ablated||orig):\n",
      "  Mean: 0.1860 ± 0.3202\n",
      "  Range: [0.0005, 2.7303]\n",
      "JS Divergence:\n",
      "  Mean: 0.0379 ± 0.0567\n",
      "  Range: [0.0001, 0.3653]\n",
      "L2 Logits:\n",
      "  Mean: 251.3336 ± 83.4371\n",
      "  Range: [113.1024, 769.1024]\n",
      "L2 Probs:\n",
      "  Mean: 0.1436 ± 0.1462\n",
      "  Range: [0.0009, 0.8916]\n",
      "Cosine Distance:\n",
      "  Mean: 0.0052 ± 0.0040\n",
      "  Range: [0.0013, 0.0300]\n",
      "\n",
      "OVERALL IMPACT ASSESSMENT:\n",
      "  JS Divergence Impact: MINIMAL (mean = 0.0379)\n",
      "  L2 Logits Impact: MASSIVE (mean = 251.3336)\n",
      "\n",
      "MOST AFFECTED SENTENCES (by JS Divergence):\n",
      "1. Sentence 366 (JS = 0.3653):\n",
      "   Original: Nein zu der Produktion in Sichtweite\n",
      "   Gender: Masc, Target: der\n",
      "2. Sentence 203 (JS = 0.3631):\n",
      "   Original: Zimmer stank , dank der darunterliegenden Kücke eines Restaurants , permanent nach Friteusenfett\n",
      "   Gender: Masc, Target: der\n",
      "3. Sentence 420 (JS = 0.3547):\n",
      "   Original: Leicht verletzt wurde eine Korrespondentin des deutschen ARD - Fernsehens .\n",
      "   Gender: Fem, Target: eine\n",
      "\n",
      "Restored original model weights\n"
     ]
    }
   ],
   "source": [
    "# For error-sensitive heads\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "heads_to_test = [(0,8),(0,0),(0,3),(0,4),(0,11),(0,1),(0,7),(1,0),(0,2),(4,4)]\n",
    "\n",
    "results = simple_ablation_test_distances(model, tokenizer, test_sentences, heads_to_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui0_wvktWgwS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eB2-TrF1WgwS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02ce8086ce0c4aed9868b4076dd0ca92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05f501d4cbd8407e9c56bb97206783fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0702d2ea0594f44b32726ea6bf7b756",
      "placeholder": "​",
      "style": "IPY_MODEL_76014b9da26e4b3c8236b4c77b3121a3",
      "value": " 1.96M/1.96M [00:00&lt;00:00, 20.7MB/s]"
     }
    },
    "0673588584f7487f9274e7aeee3bec7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "074298d534a040efb9f3fee7f5af616a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e551e6fc294432f830a4fb818d7ef48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30e31501bb324843a806060d02764812": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d862992bb384669a74f14e785227b02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e597a8544064b63bb035e1dcd293b0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44a8705097f84ea4b74769347301328b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4cda7b8a7aa741b2a7132427dd86503b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "561c8ce40e9a4a75856317ba57f90f59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "586f032e1ec84c64bb7ad7ccd8403647": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae2dd07167304b9aaf531e3506ae8841",
      "max": 1961828,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c21e45a4ebdb400cbbcb4d6ade3ec324",
      "value": 1961828
     }
    },
    "59a65b0ef781442f8f93e06ae9e2ed66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02ce8086ce0c4aed9868b4076dd0ca92",
      "placeholder": "​",
      "style": "IPY_MODEL_30e31501bb324843a806060d02764812",
      "value": " 49.0/49.0 [00:00&lt;00:00, 5.50kB/s]"
     }
    },
    "634ca89b087949e78c2659258bbcaecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95e9d60420214e1eb36185b1d4f858cf",
      "max": 625,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_561c8ce40e9a4a75856317ba57f90f59",
      "value": 625
     }
    },
    "6358a96fa7cd489192ed96ab0e004916": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7006b599412f4050892d748cd6cb7f7b",
      "placeholder": "​",
      "style": "IPY_MODEL_b75df36b062546259f505165c159ca2c",
      "value": " 996k/996k [00:00&lt;00:00, 16.4MB/s]"
     }
    },
    "641395dc25e44feb9827cbf4ea7e2489": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72e1ee1e260d4f79a517c6b5f33f20f5",
      "placeholder": "​",
      "style": "IPY_MODEL_c8557c58b83d4eb2a26c8e9f410e1e0a",
      "value": " 714M/714M [00:02&lt;00:00, 559MB/s]"
     }
    },
    "662f62f068444844b138a3b64370be51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c8e788dd3a447fba760f320966c3f3d",
      "placeholder": "​",
      "style": "IPY_MODEL_3e597a8544064b63bb035e1dcd293b0a",
      "value": "config.json: 100%"
     }
    },
    "6729c88515574dad87d1378e2c0acc51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_662f62f068444844b138a3b64370be51",
       "IPY_MODEL_634ca89b087949e78c2659258bbcaecb",
       "IPY_MODEL_f78347e9c3d4496ebdf670577b2f5ff1"
      ],
      "layout": "IPY_MODEL_c4e8402252cd48daa5a8c9407b184f38"
     }
    },
    "7006b599412f4050892d748cd6cb7f7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72e1ee1e260d4f79a517c6b5f33f20f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "735869ec37204e41bd5bbd1c57333bf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76014b9da26e4b3c8236b4c77b3121a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78c05bfa29784c5dad221c638d03adec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7bffcac8e4904b868b1963ea358d3fef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c8e788dd3a447fba760f320966c3f3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b2372e4391c4cda9c22c52dbcf74bd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5b70b61f09d415faf91aab2d3425af2",
       "IPY_MODEL_945e044d736f41bf85b73e0d0642b87f",
       "IPY_MODEL_641395dc25e44feb9827cbf4ea7e2489"
      ],
      "layout": "IPY_MODEL_d242e5ef29044edfbdcd10d2b55e4b4e"
     }
    },
    "8c36ab9bb6cc480e845ce11e08b96307": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be90b8bbf40c4685866304f64581bd09",
       "IPY_MODEL_586f032e1ec84c64bb7ad7ccd8403647",
       "IPY_MODEL_05f501d4cbd8407e9c56bb97206783fe"
      ],
      "layout": "IPY_MODEL_f7b22adac63141589796039a6c79aca0"
     }
    },
    "91cfa857920641938badb3c4859ff6de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "945e044d736f41bf85b73e0d0642b87f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4d96c9bd7b94357becc678da3fd7311",
      "max": 714290682,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_78c05bfa29784c5dad221c638d03adec",
      "value": 714290682
     }
    },
    "9526d45d31014131a2224e772d5a2952": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95e9d60420214e1eb36185b1d4f858cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a41e4a295a6f4d11811f432cf5456356": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bffcac8e4904b868b1963ea358d3fef",
      "max": 995526,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_44a8705097f84ea4b74769347301328b",
      "value": 995526
     }
    },
    "a93224c63388419fbbe028d57f90efa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae2dd07167304b9aaf531e3506ae8841": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2b6b1c8b70240a684f269e923cd10cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4d96c9bd7b94357becc678da3fd7311": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b75df36b062546259f505165c159ca2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be90b8bbf40c4685866304f64581bd09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdabbe55e04c4ec1921d74d6f2d76aeb",
      "placeholder": "​",
      "style": "IPY_MODEL_ef16083930bd4773a64d4fa59060e8fd",
      "value": "tokenizer.json: 100%"
     }
    },
    "c21e45a4ebdb400cbbcb4d6ade3ec324": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c4e8402252cd48daa5a8c9407b184f38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5b70b61f09d415faf91aab2d3425af2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9526d45d31014131a2224e772d5a2952",
      "placeholder": "​",
      "style": "IPY_MODEL_dc7c32d20f0c4289a130d0ff1fee58a4",
      "value": "model.safetensors: 100%"
     }
    },
    "c8557c58b83d4eb2a26c8e9f410e1e0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cdabbe55e04c4ec1921d74d6f2d76aeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d242e5ef29044edfbdcd10d2b55e4b4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d616f8577eaa4f0cb3200ebc05c50be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2b6b1c8b70240a684f269e923cd10cc",
      "placeholder": "​",
      "style": "IPY_MODEL_a93224c63388419fbbe028d57f90efa8",
      "value": "vocab.txt: 100%"
     }
    },
    "dc7c32d20f0c4289a130d0ff1fee58a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df70851135d94f51b3a31b45514c0f1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d616f8577eaa4f0cb3200ebc05c50be9",
       "IPY_MODEL_a41e4a295a6f4d11811f432cf5456356",
       "IPY_MODEL_6358a96fa7cd489192ed96ab0e004916"
      ],
      "layout": "IPY_MODEL_074298d534a040efb9f3fee7f5af616a"
     }
    },
    "e495526ff6f64c5682f822d324d452a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe44266a6b994bb1bb23180f5a46348e",
       "IPY_MODEL_ee0445ef2f1c455784f9b9f03518149c",
       "IPY_MODEL_59a65b0ef781442f8f93e06ae9e2ed66"
      ],
      "layout": "IPY_MODEL_91cfa857920641938badb3c4859ff6de"
     }
    },
    "ee0445ef2f1c455784f9b9f03518149c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e551e6fc294432f830a4fb818d7ef48",
      "max": 49,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ff11389886e7416cb14c7ef508d14f4b",
      "value": 49
     }
    },
    "ef16083930bd4773a64d4fa59060e8fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0702d2ea0594f44b32726ea6bf7b756": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f78347e9c3d4496ebdf670577b2f5ff1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_735869ec37204e41bd5bbd1c57333bf9",
      "placeholder": "​",
      "style": "IPY_MODEL_3d862992bb384669a74f14e785227b02",
      "value": " 625/625 [00:00&lt;00:00, 73.5kB/s]"
     }
    },
    "f7b22adac63141589796039a6c79aca0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe44266a6b994bb1bb23180f5a46348e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0673588584f7487f9274e7aeee3bec7f",
      "placeholder": "​",
      "style": "IPY_MODEL_4cda7b8a7aa741b2a7132427dd86503b",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "ff11389886e7416cb14c7ef508d14f4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
